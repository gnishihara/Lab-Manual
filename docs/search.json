[{"path":"index.html","id":"このマニュアルについて","chapter":"このマニュアルについて","heading":"このマニュアルについて","text":"【重要】このマニュアルは水圏植物生態学研究室用に準備しましたが、\n一部は水産学部の講義につかいます。解析の背景については講義1の資料やレクチャーを参考にしてください。\n解析は章ごとに紹介していて、コードはお互いに独立させたつもりです。\nそれにしても、そのままコピペすると動かないコードもあるので、コードをよく読んでから使ってください。\nコードに使用したデータは研究室のサーバにあるが、公開するつもりはありません。このマニュアルは統計学や解析についての詳細に説明するつもりはないです。\nつまり、このマニュアルだけだと、コードをくめるようになるが、統計学の理解には不十分です！\n解析の背景は十分理解してから、コードを使ってください。\nさらに、コードは研究室のサーバ環境で動きますが、その他の環境で動くは確認していません。最後に、マニュアルは随時更新するので、タイトルにあるバージョン番号を時々確認してね。\nわからないことまたは間違いがあれば、メールで連絡ください。研究室の皆さん：研究室用のデータは RStudio の ~/Lab_Data/ に入っています。マニュアルの更新日：05/04/2022 06:58:09 JST","code":""},{"path":"index.html","id":"サーバ環境","chapter":"このマニュアルについて","heading":"サーバ環境","text":"","code":"\nSys.info()[c(1:3,5)]\n#>                                sysname \n#>                                \"Linux\" \n#>                                release \n#>                      \"5.10.0-10-amd64\" \n#>                                version \n#> \"#1 SMP Debian 5.10.84-1 (2021-12-08)\" \n#>                                machine \n#>                               \"x86_64\""},{"path":"index.html","id":"マニュアルの作り方","chapter":"このマニュアルについて","heading":"マニュアルの作り方","text":"このマニュアルは bookdown パッケージで作成しました。HTMLバージョンのマニュアルは RStudio の Build パネルで組み立てます。Build パネルに移動するBuild Book のアイコンをクリックして、bookdown::bs4_book を選択するコンソールから組み立てる場合はつぎのコードを実行しましょう。作業しながら、マニュアルのプレビュー版はみれます。\n複数人でのプレビューはまだしたことないので、うまくいくかはわからない。","code":"\nbookdown::render_book()\nbookdown::serve_book()"},{"path":"data-input.html","id":"data-input","chapter":"1 データの読み込み","heading":"1 データの読み込み","text":"","code":""},{"path":"data-input.html","id":"必要なパッケージ","chapter":"1 データの読み込み","heading":"1.1 必要なパッケージ","text":"","code":"\nlibrary(tidyverse)\nlibrary(readxl)"},{"path":"data-input.html","id":"データの確認","chapter":"1 データの読み込み","heading":"1.2 データの確認","text":"データは環境省の「瀬戸内海における藻場・干潟分布状況調査（概要）」からまとめました。\nもとのファイルは環境省平成３０年９月スライドデッキ からダウンロードできます。\nXLSXファイルは readxl パッケージの read_xlsx() 関数で読み込みます。\nでは、XLSXファイルに存在するシートの確認をしましょう2。excel_sheets() を実行したら、ファイルから 2つのシート名が返ってきました。\n読み込む前に、それぞれのシートの構造を確認しましょう (Fig. 1.1 1.2)。\n確認はスプレッドシートソフト（MS Office、 Google Sheets、 Open Office、 Apple Numbers、 など）で行います。\nFigure 1.1: 瀬戸内海藻場データ.xlsx の FY1990 シートに入力されているデータは縦長の形式です。\nFY1990 のデータの構造は縦長なので、読み込みは比較的に楽です。\nそれぞれの変数は一つの列3に入力されているから、読み込みが簡単です。\nFigure 1.2: 瀬戸内海藻場データ.xlsx の FY2018 シートに入力されているデータは横長の形式です。\nFY2018 のデータの構造は横長です。\nデータは海藻と海草にわけられ、それぞれの変数じゃなくて、それぞれの場所の値を列に入力されています。\nこの用なデータの読み込みは手間がかかります4。","code":"\nrootdatafolder = rprojroot::find_rstudio_root_file(\"Data/\")\nfilename = '瀬戸内海藻場データ.xlsx'\npath = str_c(rootdatafolder, filename)\nexcel_sheets(path) # シート名を確認する\n#> [1] \"FY1990\" \"FY2018\""},{"path":"data-input.html","id":"データを読み込む","chapter":"1 データの読み込み","heading":"1.3 データを読み込む","text":"では、FY1990 シートのデータを読み込みます。\nここでシートから読み込むセルの範囲を指定します。データは tibble として読み込まれました。\nデータに大きな問題がなければ、各列の型・タイプ (type)5 は自動的に設定されます。調査海域 の列は <chr> : character, 文字列海藻 の列は <dbl>: double, ダブル・数値・実数海草 の列は <dbl>: double, ダブル・数値・実数変数名が日本語の場合、コードが書きづらくなったり、バグの原因になります。\n最初から英語表記にするのが合理的ですが、R環境内で名前を変換することは難しくないです。\nとりあえず d19 の内容をみましょう。FY2018 シートの読み込みは、海藻と海草ごとにする必要があります。\n読み込んだ後に、データを縦長に変換し、2 つの tibble を縦に結合します。最初のセル範囲を読み込んで ファイルのコンテンツを seaweed に書き込んだら、RNG を次のセル範囲に書き換えます。\nデータは同じシートにあるので、SHEET を変更したり、新たに定義する必要はありません。seaweed の内容は次のとおりです。seagrass の内容は次のとおりです。NA は Available の諸略です。\nRの場合、存在しないデータ (欠損値) は NA になります。","code":"\nRNG = \"A4:C27\"   # セルの範囲\nSHEET = \"FY1990\" # シート名\nd19 = read_xlsx(path, sheet = SHEET, range = RNG)\nd19 # FY1990 データの内容\n#> # A tibble: 23 × 3\n#>    調査海域  海藻  海草\n#>    <chr>    <dbl> <dbl>\n#>  1 東部        55    14\n#>  2 東部       128    62\n#>  3 東部        86     0\n#>  4 東部        87     8\n#>  5 東部       214    54\n#>  6 東部       140    57\n#>  7 中部        30    45\n#>  8 中部         5   623\n#>  9 中部       460   886\n#> 10 中部        51   180\n#> # … with 13 more rows\nRNG = \"A6:C15\"   # 海藻データのセル範囲\nSHEET = \"FY2018\" # シート名\nseaweed = read_xlsx(path, sheet = SHEET, range = RNG)\nRNG = \"E6:G15\"   # 海草データのセル範囲\nseagrass = read_xlsx(path, sheet = SHEET, range = RNG)\nseaweed\n#> # A tibble: 9 × 3\n#>    東部  中部  西部\n#>   <dbl> <dbl> <dbl>\n#> 1    14    62   108\n#> 2    93   838     0\n#> 3    12   933     0\n#> 4     8   193     0\n#> 5   444   235     0\n#> 6    85   150   126\n#> 7    NA   283     0\n#> 8    NA     3     0\n#> 9    NA    12    NA\nseagrass\n#> # A tibble: 9 × 3\n#>    東部  中部  西部\n#>   <dbl> <dbl> <dbl>\n#> 1    71    63   430\n#> 2   145     5   231\n#> 3    94   674   404\n#> 4    82    69  2005\n#> 5    49    21  1094\n#> 6   100   141    54\n#> 7    NA   635   221\n#> 8    NA    62   182\n#> 9    NA   440    NA"},{"path":"data-wrangling.html","id":"data-wrangling","chapter":"2 データの処理","heading":"2 データの処理","text":"","code":""},{"path":"data-wrangling.html","id":"必要なパッケージ-1","chapter":"2 データの処理","heading":"2.1 必要なパッケージ","text":"","code":"\nlibrary(tidyverse)\nlibrary(readxl)"},{"path":"data-wrangling.html","id":"データの読み込み","chapter":"2 データの処理","heading":"2.2 データの読み込み","text":"データの読み込みを参考に：1 。","code":"\nrootdatafolder = rprojroot::find_rstudio_root_file(\"Data/\")\nfilename = '瀬戸内海藻場データ.xlsx'\npath = str_c(rootdatafolder, filename)\n\n# fy1990 の処理\nRNG = \"A4:C27\"   # セルの範囲\nSHEET = \"FY1990\" # シート名\nd19 = read_xlsx(path, sheet = SHEET, range = RNG)\n\n# fy2018の処理\nRNG = \"A6:C15\"   # 海藻データのセル範囲\nSHEET = \"FY2018\" # シート名\nseaweed = read_xlsx(path, sheet = SHEET, range = RNG)\nRNG = \"E6:G15\"   # 海草データのセル範囲\nseagrass = read_xlsx(path, sheet = SHEET, range = RNG)"},{"path":"data-wrangling.html","id":"データの処理","chapter":"2 データの処理","heading":"2.3 データの処理","text":"データの形（横長から縦長）を変えたいとき、tidyverse の pivot_wider() と pivot_longer() を使うと楽です。では、FY2018シートの構造をFY1990シートと同じようにします。\n横長のデータを縦長に変換するには、pivot_longer() を使います。\nこれは MS Excel の ピボットテーブル (pivot table) の機能とにています。ここでの重要なポイントは、必ずピボットしたい列を指定することです。\nこのとき、すべての列をピボットしたいので、pivot_longer() には cols = everything() をわたします。\nピボットされた seaweed は次のとおりです。\n|> print(n = Inf) をすると、tibble 内容をすべて表示できます6。seagrass も同じように処理しました。では、次は seaweed と seagrass を縦に結合することです。\n複数の tibble を縦に結合するための関数は bind_rows() です。seaweed に seaweed、seagrass に seagrass を渡します。\nさらに、seaweed と seagrass を type 変数に書き込みます。実は、次のように bind_rows() を実行できますが、データの構造は不都合になります。\nどちらも 2つの tibble を縦に結合してくれますが、結果は全く違います。\nコードと結果の違いをよく確認して、その違いを理解しましょう。では、d20 の type ごとの value 変数を横にならべたら、d19 と全く同じ構造になります。このように処理したら、Warning message がでます。\nWarning (ウォーニング) は Error (エラー) ほどの問題ではないので、コードは実行されています。\nError の場合はコードは実行されません。\nこの Warning で values uniquely identified と返ってきました。\nつまり、各サンプルの値は、区別することができないと意味します。\nこのデータの場合は、区別しなくても問題ないので、このまま解析を続きます。\nそれにしても、seaweed と seagrass の変数 type は <list> です。\nそれぞれの変数の要素に <dbl [9]> と記述されています。\n各要素に 9つの値が入力されていると意味します。\n研究室では、seaweed と seagrass 変数は nested (ネスト) または、「たたまれている」といいます。\nでは、この２つの変数を unnest (アンネスト) します。さらに、name を site (調査海域) に変更します。最後に、d20 の NA データを外します。これで、d20 と d19 はほぼ同じ構造です。\n次のコードブロックで、d19 の変数名をrename() を用いて英語に変えます。\n日本語の変数名は使いづらくて、バグの原因になることが多いので名前を変更します。解析をするまえに、site を要因 (因子) として設定します。\nlevels = c('東部', '中部', '西部') は因子の順序を指定するためです。\n指定しなかった場合、アルファベット順やあいうえお順になります。","code":"\n# %>% と |> はパイプ演算子とよびます。\n# |> はR 4.1.0 から追加された、ネーティブのパイプ演算子です。\n# RStudio の設定を変えなければ、CTRL+SHIFT+M をしたら、%>% が入力されるとおもいます。\n# ネーティブパイプを使いたいなら、Tools -> Global Options -> Code に\n#   いって、Use native pipe operator のボックスにチェックを入れてください。\n# seaweed = seaweed %>% pivot_longer(cols = everything())\nseaweed = seaweed |> pivot_longer(cols = everything())\nseaweed |> print(n = Inf)\n#> # A tibble: 27 × 2\n#>    name  value\n#>    <chr> <dbl>\n#>  1 東部     14\n#>  2 中部     62\n#>  3 西部    108\n#>  4 東部     93\n#>  5 中部    838\n#>  6 西部      0\n#>  7 東部     12\n#>  8 中部    933\n#>  9 西部      0\n#> 10 東部      8\n#> 11 中部    193\n#> 12 西部      0\n#> 13 東部    444\n#> 14 中部    235\n#> 15 西部      0\n#> 16 東部     85\n#> 17 中部    150\n#> 18 西部    126\n#> 19 東部     NA\n#> 20 中部    283\n#> 21 西部      0\n#> 22 東部     NA\n#> 23 中部      3\n#> 24 西部      0\n#> 25 東部     NA\n#> 26 中部     12\n#> 27 西部     NA\nseagrass = seagrass |> pivot_longer(cols = everything())\nd20 = bind_rows(seaweed = seaweed, seagrass = seagrass, .id = \"type\")\nd20　# FY2018 データ\n#> # A tibble: 54 × 3\n#>    type    name  value\n#>    <chr>   <chr> <dbl>\n#>  1 seaweed 東部     14\n#>  2 seaweed 中部     62\n#>  3 seaweed 西部    108\n#>  4 seaweed 東部     93\n#>  5 seaweed 中部    838\n#>  6 seaweed 西部      0\n#>  7 seaweed 東部     12\n#>  8 seaweed 中部    933\n#>  9 seaweed 西部      0\n#> 10 seaweed 東部      8\n#> # … with 44 more rows\nbind_rows(seaweed, seagrass)\n#> # A tibble: 54 × 2\n#>    name  value\n#>    <chr> <dbl>\n#>  1 東部     14\n#>  2 中部     62\n#>  3 西部    108\n#>  4 東部     93\n#>  5 中部    838\n#>  6 西部      0\n#>  7 東部     12\n#>  8 中部    933\n#>  9 西部      0\n#> 10 東部      8\n#> # … with 44 more rows\nd20 = d20 |> pivot_wider(id_cols = name,\n                   names_from = type,\n                   values_from = value)\nd20 = d20 |> unnest(c(seaweed, seagrass))\nd20\n#> # A tibble: 27 × 3\n#>    name  seaweed seagrass\n#>    <chr>   <dbl>    <dbl>\n#>  1 東部       14       71\n#>  2 東部       93      145\n#>  3 東部       12       94\n#>  4 東部        8       82\n#>  5 東部      444       49\n#>  6 東部       85      100\n#>  7 東部       NA       NA\n#>  8 東部       NA       NA\n#>  9 東部       NA       NA\n#> 10 中部       62       63\n#> # … with 17 more rows\nd20 = d20 |> rename(site = name)\nd20\n#> # A tibble: 27 × 3\n#>    site  seaweed seagrass\n#>    <chr>   <dbl>    <dbl>\n#>  1 東部       14       71\n#>  2 東部       93      145\n#>  3 東部       12       94\n#>  4 東部        8       82\n#>  5 東部      444       49\n#>  6 東部       85      100\n#>  7 東部       NA       NA\n#>  8 東部       NA       NA\n#>  9 東部       NA       NA\n#> 10 中部       62       63\n#> # … with 17 more rows\nd20 = d20 |> drop_na() # NAを外す\nd20\n#> # A tibble: 23 × 3\n#>    site  seaweed seagrass\n#>    <chr>   <dbl>    <dbl>\n#>  1 東部       14       71\n#>  2 東部       93      145\n#>  3 東部       12       94\n#>  4 東部        8       82\n#>  5 東部      444       49\n#>  6 東部       85      100\n#>  7 中部       62       63\n#>  8 中部      838        5\n#>  9 中部      933      674\n#> 10 中部      193       69\n#> # … with 13 more rows\nd19 = d19 |> \n  rename(site = 調査海域, seaweed = 海藻, seagrass = 海草) |> \n  mutate(site = factor(site, levels = c('東部', '中部', '西部')))\nd20 = d20 |> \n  mutate(site = factor(site, levels = c('東部', '中部', '西部')))"},{"path":"data-logger-input.html","id":"data-logger-input","chapter":"3 ロガーからの読み込み","heading":"3 ロガーからの読み込み","text":"研究室のデータロガーは主に Onset Computer Corporation (リンクは国内代理店に飛びます) と Dataflow Systems Inc. (リンクはメーカーに飛びます) の商品を使っています。ティドビットV2 (TidBit V2, UTBI-001)ウォーターテンププロ V2 (U22-001)U20 ウォーターレベルロガー(標準型・9m) (U20-001-01)U26 溶存酸素ロガー (U26-001)USBマイクロステーションロガー (H21-USB)\n風速センサー (S-WSB-M003)\nPAR 光量子センサー (S-LIA-M003)\n気圧センサー (S-BPB-CM50)\n風速センサー (S-WSB-M003)PAR 光量子センサー (S-LIA-M003)気圧センサー (S-BPB-CM50)Odyssey PAR Light logger (旧バージョン)","code":""},{"path":"data-logger-input.html","id":"必要なパッケージ-2","chapter":"3 ロガーからの読み込み","heading":"3.1 必要なパッケージ","text":"gnnlab は研究室用のパッケージです。\nここにデータロガー読み取り専用関数が入っています。\nパッケージのURLは (https://github.com/gnishihara/gnnlab) です。\nもちろん、研究室サーバの場合はインストール済みですが、\n個人のパソコンにインストールしたいなら、remote のパッケージでできます。Onsetのロガー用関数は read_onset() です。\nこれで、上で紹介したすべてのロガーのCSVファイルからデータの読み込みができます。\nMicrostation の場合は、風速、PAR、気圧センサーだけに対応しています。\n他のセンサーを追加したい場合、データファイル送ってくれると追加できるとおもいます。\nDataflow systems のロガーは read_odyssey() です。この関数は read_csv() のラッパーなので、tidyverse パッケージも必要です。","code":"\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(gnnlab)\nremotes::install_github(\"https://github.com/gnishihara/gnnlab\")\nread_onset()"},{"path":"data-logger-input.html","id":"使い方","chapter":"3 ロガーからの読み込み","heading":"3.2 使い方","text":"read_onset() は読み込んだデータが負の値または、水温と溶存酸素濃度の値が 40 を超えた場合、NA とします。\n不都合の場合は、次のように読み込んでくださし。使いにくい列名なので、select() をつかって作り直します。次は、datetime の修正です。\nchr から dttm の変換します。\nこのとき、lubridate の parse_date_time() をつかいます。localeはパソコンの環境によって異なりますが、研究室のサーバの場合は\nlocale = Sys.setlocale(\"LC_TIME\", \"ja_JP.UTF-8\") にしてください。研究室のサーバで解析をするなら、read_onset() を使うこと！","code":"\nlabdata = \"~/Lab_Data/kawatea/Temperature/Temp_03_ecser_surface_210326.csv\"\necser = read_onset(labdata)\necser\n#> # A tibble: 9,057 × 2\n#>    datetime            temperature\n#>    <dttm>                    <dbl>\n#>  1 2021-03-26 18:00:00        21.2\n#>  2 2021-03-26 18:10:00        20.3\n#>  3 2021-03-26 18:20:00        19.1\n#>  4 2021-03-26 18:30:00        17.5\n#>  5 2021-03-26 18:40:00        16.2\n#>  6 2021-03-26 18:50:00        15.3\n#>  7 2021-03-26 19:00:00        14.9\n#>  8 2021-03-26 19:10:00        14.5\n#>  9 2021-03-26 19:20:00        14.0\n#> 10 2021-03-26 19:30:00        14.0\n#> # … with 9,047 more rows\necser = read_csv(labdata, skip = 1)\necser\n#> # A tibble: 9,057 × 3\n#>      `#` `日付 時間, GMT+09:00`   `温度, °C`\n#>    <dbl> <chr>                         <dbl>\n#>  1     1 03/26/2021 06:00:00 午後       21.2\n#>  2     2 03/26/2021 06:10:00 午後       20.3\n#>  3     3 03/26/2021 06:20:00 午後       19.1\n#>  4     4 03/26/2021 06:30:00 午後       17.5\n#>  5     5 03/26/2021 06:40:00 午後       16.2\n#>  6     6 03/26/2021 06:50:00 午後       15.3\n#>  7     7 03/26/2021 07:00:00 午後       14.9\n#>  8     8 03/26/2021 07:10:00 午後       14.5\n#>  9     9 03/26/2021 07:20:00 午後       14.0\n#> 10    10 03/26/2021 07:30:00 午後       14.0\n#> # … with 9,047 more rows\necser = ecser |> \n  select(datetime = matches(\"GMT\"),\n         temperature = matches(\"温度\"))\necser\n#> # A tibble: 9,057 × 2\n#>    datetime                 temperature\n#>    <chr>                          <dbl>\n#>  1 03/26/2021 06:00:00 午後        21.2\n#>  2 03/26/2021 06:10:00 午後        20.3\n#>  3 03/26/2021 06:20:00 午後        19.1\n#>  4 03/26/2021 06:30:00 午後        17.5\n#>  5 03/26/2021 06:40:00 午後        16.2\n#>  6 03/26/2021 06:50:00 午後        15.3\n#>  7 03/26/2021 07:00:00 午後        14.9\n#>  8 03/26/2021 07:10:00 午後        14.5\n#>  9 03/26/2021 07:20:00 午後        14.0\n#> 10 03/26/2021 07:30:00 午後        14.0\n#> # … with 9,047 more rows\nlocale = Sys.setlocale(\"LC_TIME\", \"ja_JP.UTF-8\")\necser |> mutate(datetime = parse_date_time(datetime, \"mdyT\", locale = locale))\n#> # A tibble: 9,057 × 2\n#>    datetime            temperature\n#>    <dttm>                    <dbl>\n#>  1 2021-03-26 18:00:00        21.2\n#>  2 2021-03-26 18:10:00        20.3\n#>  3 2021-03-26 18:20:00        19.1\n#>  4 2021-03-26 18:30:00        17.5\n#>  5 2021-03-26 18:40:00        16.2\n#>  6 2021-03-26 18:50:00        15.3\n#>  7 2021-03-26 19:00:00        14.9\n#>  8 2021-03-26 19:10:00        14.5\n#>  9 2021-03-26 19:20:00        14.0\n#> 10 2021-03-26 19:30:00        14.0\n#> # … with 9,047 more rows"},{"path":"data-logger-input.html","id":"複数データの読み込み","chapter":"3 ロガーからの読み込み","heading":"3.3 複数データの読み込み","text":"一度に複数の似たようなデータを読み込むなら、map()シリーズの関数でします。データ読んで、一つのtibble として返す場合は map_dfr() をつかう。研究室のデータファイルは、次のように作っているので、ファイル名もtibble に残して読んだほうがいい。Temp_856_arikawaamamo_surface_200819.csv は観測するデータの種類_ロガー番号_調査地点_場所_設置日.csv として定義しています。あとはファイル名から情報を抜き出して、data をアンネストするだけです。","code":"\nlabfolder = \"~/Lab_Data/kawatea/Temperature/\"\nfnames = dir(labfolder, pattern = \"Temp_.*arikawa.*surface.*csv\", full = TRUE)\nmap_dfr(fnames, read_onset)\n#> # A tibble: 186,608 × 2\n#>    datetime            temperature\n#>    <dttm>                    <dbl>\n#>  1 2017-07-19 09:30:00        29.8\n#>  2 2017-07-19 09:40:00        29.7\n#>  3 2017-07-19 09:50:00        29.7\n#>  4 2017-07-19 10:00:00        29.5\n#>  5 2017-07-19 10:10:00        29.9\n#>  6 2017-07-19 10:20:00        32.0\n#>  7 2017-07-19 10:30:00        32.1\n#>  8 2017-07-19 10:40:00        31.7\n#>  9 2017-07-19 10:50:00        31.4\n#> 10 2017-07-19 11:00:00        35.7\n#> # … with 186,598 more rows\ndout = tibble(fnames) |> mutate(data = map(fnames, read_onset))\ndout\n#> # A tibble: 40 × 2\n#>    fnames                                           data    \n#>    <chr>                                            <list>  \n#>  1 /home/gnishihara/Lab_Data/kawatea/Temperature//… <tibble>\n#>  2 /home/gnishihara/Lab_Data/kawatea/Temperature//… <tibble>\n#>  3 /home/gnishihara/Lab_Data/kawatea/Temperature//… <tibble>\n#>  4 /home/gnishihara/Lab_Data/kawatea/Temperature//… <tibble>\n#>  5 /home/gnishihara/Lab_Data/kawatea/Temperature//… <tibble>\n#>  6 /home/gnishihara/Lab_Data/kawatea/Temperature//… <tibble>\n#>  7 /home/gnishihara/Lab_Data/kawatea/Temperature//… <tibble>\n#>  8 /home/gnishihara/Lab_Data/kawatea/Temperature//… <tibble>\n#>  9 /home/gnishihara/Lab_Data/kawatea/Temperature//… <tibble>\n#> 10 /home/gnishihara/Lab_Data/kawatea/Temperature//… <tibble>\n#> # … with 30 more rows\ndout = dout |> \n  mutate(fnames = basename(fnames)) |> \n  mutate(fnames = str_remove(fnames, \".csv\")) |> \n  separate(fnames, \n           c(\"type\", \"id\", \"location\", \"position\", \"surveydate\"),\n           sep = \"_\") |> \n  unnest(data)\ndout\n#> # A tibble: 186,608 × 7\n#>    type  id       location     position surveydate\n#>    <chr> <chr>    <chr>        <chr>    <chr>     \n#>  1 Temp  11000843 arikawaamamo surface  170819    \n#>  2 Temp  11000843 arikawaamamo surface  170819    \n#>  3 Temp  11000843 arikawaamamo surface  170819    \n#>  4 Temp  11000843 arikawaamamo surface  170819    \n#>  5 Temp  11000843 arikawaamamo surface  170819    \n#>  6 Temp  11000843 arikawaamamo surface  170819    \n#>  7 Temp  11000843 arikawaamamo surface  170819    \n#>  8 Temp  11000843 arikawaamamo surface  170819    \n#>  9 Temp  11000843 arikawaamamo surface  170819    \n#> 10 Temp  11000843 arikawaamamo surface  170819    \n#> # … with 186,598 more rows, and 2 more variables:\n#> #   datetime <dttm>, temperature <dbl>"},{"path":"data-summary.html","id":"data-summary","chapter":"4 データの集計","heading":"4 データの集計","text":"","code":""},{"path":"data-summary.html","id":"必要なパッケージ-3","chapter":"4 データの集計","heading":"4.1 必要なパッケージ","text":"","code":"\nlibrary(tidyverse)\nlibrary(readxl)"},{"path":"data-summary.html","id":"データの準備","chapter":"4 データの集計","heading":"4.2 データの準備","text":"データの読み込みは章1 を参考にしてください。データの処理は章2 を参考にしてください。","code":"\nrootdatafolder = rprojroot::find_rstudio_root_file(\"Data/\")\nfilename = '瀬戸内海藻場データ.xlsx'\npath = str_c(rootdatafolder, filename)\n\n# fy1990 の処理\nRNG = \"A4:C27\"   # セルの範囲\nSHEET = \"FY1990\" # シート名\nd19 = read_xlsx(path, sheet = SHEET, range = RNG)\n\n# fy2018の処理\nRNG = \"A6:C15\"   # 海藻データのセル範囲\nSHEET = \"FY2018\" # シート名\nseaweed = read_xlsx(path, sheet = SHEET, range = RNG)\nRNG = \"E6:G15\"   # 海草データのセル範囲\nseagrass = read_xlsx(path, sheet = SHEET, range = RNG)\nseaweed = seaweed |> pivot_longer(cols = everything())\nseagrass = seagrass |> pivot_longer(cols = everything())\n\nd20 = bind_rows(seaweed = seaweed, seagrass = seagrass, .id = \"type\")\nd20 = d20 |> pivot_wider(id_cols = name,\n                   names_from = type, values_from = value, \n                   values_fn = \"list\")\nd20 = d20 |> unnest(c(seaweed, seagrass)) |> rename(site = name) |> drop_na()\n\n\nd19 = d19 |> \n  rename(site = 調査海域, seaweed = 海藻, seagrass = 海草) |> \n  mutate(site = factor(site, levels = c('東部', '中部', '西部')))\nd20 = d20 |> \n  mutate(site = factor(site, levels = c('東部', '中部', '西部')))"},{"path":"data-summary.html","id":"記述統計量","chapter":"4 データの集計","heading":"4.3 記述統計量","text":"一般的には、数値データは2つの値にまとめられます。Measures central tendency: 位置の尺度（平均値、中央値、最頻値）Measures dispersion: ばらつきの尺度（四分位数間範囲、平均絶対偏差、中央絶対偏差、範囲、標準偏差、分散）まず、サイコロの関数を定義してから、位置のの尺度とばらつきの尺度を求めましょう。2つのサイコロを10回投げます。サイコロの結果は次のとおりです。平均値, mean, average中央値, メディアン, median最頻値, モード, mode再頻値を求める。専用の関数がないので、ここで定義して使用します。分散, variance分散と標準偏差はもっとも使われるばらつきの尺度です。\\[\nVar(x) = \\frac{1}{N-1}\\sum_{n = 1}^N(x_n - \\overline{x})^2\n\\]標準偏差, standard deviation標準偏差は分散の平方根です。四分位数間範囲, inter-quantile range, IQR四分位数間範囲は第2四分位数と第3四分位数の距離です。\n箱ひげ図の箱の高さが四分位数間範囲です。範囲, range範囲はデータを最大値と最小値の距離です。平均絶対偏差, mean absolute deviation平均絶対偏差は、データと平均値との距離の平均値です。\\[\n\\text{MAD} = \\frac{1}{N} \\sum_{n = 1}^N |x_n - \\overline{x}|\n\\]\n\\(\\overline{x}\\) は平均値、\\(N\\)はデータ数です。\n専用の関数がないので、これも定義します。中央絶対偏差, median absolute deviation, MAD中央絶対偏差は、データと中央値との距離の中央値です。\\[\n\\text{MAD} = median(|x_i - \\tilde{x}|)\n\\]\n\\(\\tilde{x}\\) は \\(x\\) の中央値です。標準偏差として使う場合は、\\[\n\\hat{\\sigma}\\equiv s = k \\cdot \\text{MAD} = \\frac{1}{\\Phi^{-1}(3/4)}\\cdot \\text{MAD}\n\\]\nこの積は標準偏差のロバスト推定量 (robust estimator) といいます。\nロバスト推定量は外れ値に強く影響されないのが特徴です。tibbleデータの集計は次のようにします。\n全データの集計の場合は、tibble　を summarise 関数に渡します。ここではseaweed と seagrass の平均値を求めています。across() 関数を使えば、コードは諸略できます。\nacross() に渡したそれぞれのベクトル（列）に mean 関数を適応しています。mean() と sd() を同時に適応できます。ところが帰ってくる結果をみて、平均値と標準偏差の区別ができないので、次のようにコードをくみましょう。では、平均値、標準偏差、平均絶対偏差を求めています。site ごとに集計したいとき、group_by() 関数を使って、データのグループ化してから、それぞれの関数を適応します。","code":"\n# n: サイコロの数\n# s: サイコロの面の数\nroll_dice = function(n = 1, s = 6) {\n  face = 1:s\n  sum(sample(x = face, size = n, replace = TRUE))\n}\nset.seed(2022) #疑似乱数を固定することで、再現性のあるシミュレーションができる。\nx = replicate(10, roll_dice(n = 2, s = 6))\nx\n#>  [1]  7  9 10 10  4 11  8  6  4  4\nmean(x)\n#> [1] 7.3\nmedian(x)\n#> [1] 7.5\nmode = function(x) {\n  u = unique(x)\n  matched = tabulate(match(x,u))\n  u[near(matched, max(matched))]\n}\nmode(x)\n#> [1] 4\nvar(x)\n#> [1] 7.344444\nsd(x)\n#> [1] 2.710064\ndiff(quantile(x, c(0.25, 0.75)))\n#>  75% \n#> 5.25\ndiff(range(x))\n#> [1] 7\nmean_absolute_deviation = function(x) {\n  xbar = mean(x)\n  xout = abs(x - xbar)\n  mean(xout)\n}\nmean_absolute_deviation(x)\n#> [1] 2.3\nmad(x, constant = 1)\n#> [1] 2.5\nmad(x)\n#> [1] 3.7065\nd19 |> \n  summarise(seaweed = mean(seaweed),\n            seagrass = mean(seagrass))\n#> # A tibble: 1 × 2\n#>   seaweed seagrass\n#>     <dbl>    <dbl>\n#> 1    312.     127.\nd19 |> summarise(across(c(seaweed, seagrass), mean))\n#> # A tibble: 1 × 2\n#>   seaweed seagrass\n#>     <dbl>    <dbl>\n#> 1    312.     127.\nd19 |> summarise(across(c(seaweed, seagrass), list(mean, sd)))\n#> # A tibble: 1 × 4\n#>   seaweed_1 seaweed_2 seagrass_1 seagrass_2\n#>       <dbl>     <dbl>      <dbl>      <dbl>\n#> 1      312.      439.       127.       227.\nd19 |> summarise(across(c(seaweed, seagrass), list(mean = mean, sd = sd)))\n#> # A tibble: 1 × 4\n#>   seaweed_mean seaweed_sd seagrass_mean seagrass_sd\n#>          <dbl>      <dbl>         <dbl>       <dbl>\n#> 1         312.       439.          127.        227.\nd19 |> \n  summarise(across(c(seaweed, seagrass),\n                   list(mean = mean, sd = sd,\n                        mad = mean_absolute_deviation)))\n#> # A tibble: 1 × 6\n#>   seaweed_mean seaweed_sd seaweed_mad seagrass_mean\n#>          <dbl>      <dbl>       <dbl>         <dbl>\n#> 1         312.       439.        266.          127.\n#> # … with 2 more variables: seagrass_sd <dbl>,\n#> #   seagrass_mad <dbl>\nd19 |>\n  group_by(site) |> \n  summarise(across(c(seaweed, seagrass),\n                   list(mean = mean, sd = sd,\n                        mad = mean_absolute_deviation)))\n#> # A tibble: 3 × 7\n#>   site  seaweed_mean seaweed_sd seaweed_mad seagrass_mean\n#>   <fct>        <dbl>      <dbl>       <dbl>         <dbl>\n#> 1 東部          118.       56.1        42.3          32.5\n#> 2 中部          205.      195.        160.          276. \n#> 3 西部          578.      658.        486.           29.2\n#> # … with 2 more variables: seagrass_sd <dbl>,\n#> #   seagrass_mad <dbl>"},{"path":"map-function.html","id":"map-function","chapter":"5 map 関数ってすごい","heading":"5 map 関数ってすごい","text":"","code":""},{"path":"map-function.html","id":"必要なパッケージ-4","chapter":"5 map 関数ってすごい","heading":"5.1 必要なパッケージ","text":"","code":"\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(broom)"},{"path":"map-function.html","id":"データの準備-1","chapter":"5 map 関数ってすごい","heading":"5.2 データの準備","text":"Rの iris7 データで解析を紹介します。\niris は data.frame として定義されているので、as_tibble() を使って tibble のクラスを追加します。","code":"\niris = iris |> as_tibble()\niris\n#> # A tibble: 150 × 5\n#>    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#>           <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n#>  1          5.1         3.5          1.4         0.2 setosa \n#>  2          4.9         3            1.4         0.2 setosa \n#>  3          4.7         3.2          1.3         0.2 setosa \n#>  4          4.6         3.1          1.5         0.2 setosa \n#>  5          5           3.6          1.4         0.2 setosa \n#>  6          5.4         3.9          1.7         0.4 setosa \n#>  7          4.6         3.4          1.4         0.3 setosa \n#>  8          5           3.4          1.5         0.2 setosa \n#>  9          4.4         2.9          1.4         0.2 setosa \n#> 10          4.9         3.1          1.5         0.1 setosa \n#> # … with 140 more rows"},{"path":"map-function.html","id":"説明","chapter":"5 map 関数ってすごい","heading":"5.3 説明","text":"tidyverse の開発によって、Rでのデータ処理はすこぶる楽になりました。\n個人的には、Rでデータ処理するのはとても楽しいです。\nそこで、もっともデータ処理を楽にしてくれたのは map() 関数です。\n実は、数種類のmap() があります。map()map_lgl(), map_int(), map_dbl(), map_chr()そのほかにもありますが、研究室のコードでは上のものが多いです。\n他によく使う関数は pmap() と map2() です。\npmap() は3変数以上を関数に渡したいときに使います。\nmap2() は2変数のバージョンです。どの map() には .x と .f の引数を渡す必要があります。.x は list または vector のオブジェクトです。.f は list/vector のそれぞれの要素に適応したい関数です。例えば、つぎの list を定義します。それぞれの要素の平均値を出したいなら、次のように map() を使います。map()は必ず list として結果を返します。\nベース　(base) Rlapply() と同じですね。ベースRの sapply() のようにベクトルとして返してほしいなら、map_dbl() を使います。ベースRの sapply() の結果と同じです。ちなみに、-loop でもできますが、研究室では使用を禁じます。","code":"\nz = list(a = rnorm(10),\n         b = rnorm(10),\n         c = rnorm(5))\nz\n#> $a\n#>  [1] -0.8863398 -0.9816108  0.1365766  0.2214021 -1.4657715\n#>  [6] -0.7864034  0.9836139 -0.1147985  0.3445795 -0.9186878\n#> \n#> $b\n#>  [1] -0.2084705  1.4010499 -1.4752405  1.3494695 -0.2854956\n#>  [6]  1.2839244  0.0082045 -1.1255159 -0.4015480  0.2610532\n#> \n#> $c\n#> [1] -0.3354253 -0.6033843 -0.4297671  0.5687520 -0.1878554\nmap(z, mean)\n#> $a\n#> [1] -0.346744\n#> \n#> $b\n#> [1] 0.0807431\n#> \n#> $c\n#> [1] -0.197536\nlapply(z, mean)\n#> $a\n#> [1] -0.346744\n#> \n#> $b\n#> [1] 0.0807431\n#> \n#> $c\n#> [1] -0.197536\nmap_dbl(z, mean)\n#>          a          b          c \n#> -0.3467440  0.0807431 -0.1975360\nsapply(z, mean)\n#>          a          b          c \n#> -0.3467440  0.0807431 -0.1975360\n# 良い子はループ使わない。\nzout = vector(\"numeric\", 3)\nn = length(z)\nfor(i in 1:n) {\n  zout[i] = mean(z[[i]])\n}\nzout\n#> [1] -0.3467440  0.0807431 -0.1975360"},{"path":"map-function.html","id":"map-の魅力","chapter":"5 map 関数ってすごい","heading":"5.4 map() の魅力","text":"map()の魅力は tidyverse のパイプラインに使えること、map() に複雑な関数を渡せること。\n結果は tibble として返せることかな。\n他にあるとおもいますが、使えるようになるとデータ処理は楽しいです。たとえば、次のようことができます。\niris のデータを tibble に変換し、pivot_longer() に渡して縦長に変えます。\npivot_longer() には Sepal と Petal を含む列を cols 引数に渡すようにしています。\n変換したあと、pivot_longer() が作った name の列は separate() によって part と measurement に分けます。ここでは関数を定義していますが、この関数は複数の t 検定を実施し、その結果を一つの tibble にまとめています。\nt.test() に渡すデータは filter() 関数に通しています。\nfilter() は str_detect() を使って、 Species 列から解析したいデータを抽出しています。\nstr_detect() で処理する列は Species、検索する文字列は pattern に渡しています。\nたとえば、pattern = \"set|ver\" は set または ver を意味しています。\nt 検定の結果を t12、t13、t23の入れます。\nこんど、それらを broom パッケージの tidy() に渡し、tibbleかします。\nbind_rows() を使って、縦に結合し、結合した要素の名前を comparison にします。上のコードチャンクで定義した関数は iris_long に適応しますが、\nmeasurement ごとに data の要素ごとに実施されます。つまり、data は map() を通して、 runmultttest() が適応されます。","code":"\niris_long = iris |> \n  as_tibble() |> \n  pivot_longer(cols = matches(\"Sepal|Petal\")) |> \n  separate(name, c(\"part\", \"measurement\"))\nrunmultttest = function(df) {\n  #t12: setosa - versicolor\n  #t13: setosa - virginica\n  #t23: versicolor - virginica\n   \n  t12 = t.test(value ~ part, data = filter(df, str_detect(string = Species, pattern = \"set|ver\")))\n  t13 = t.test(value ~ part, data = filter(df, str_detect(string = Species, pattern = \"set|vir\")))\n  t23 = t.test(value ~ part, data = filter(df, str_detect(string = Species, pattern = \"ver|vir\")))\n  bind_rows(\"setosa vs. versicolor\"    = tidy(t12), \n            \"setosa vs. virginica\"     = tidy(t13), \n            \"versicolor vs. virginica\" = tidy(t23), .id = \"comparison\")\n}\niris_long = iris_long |> group_nest(measurement) \niris_long\n#> # A tibble: 2 × 2\n#>   measurement               data\n#>   <chr>       <list<tibble[,3]>>\n#> 1 Length               [300 × 3]\n#> 2 Width                [300 × 3]\niris_long |> \n  mutate(tout = map(data, runmultttest)) |> \n  unnest(tout)\n#> # A tibble: 6 × 13\n#>   measurement             data comparison estimate estimate1\n#>   <chr>       <list<tibble[,3> <chr>         <dbl>     <dbl>\n#> 1 Length             [300 × 3] setosa vs…    -2.61     2.86 \n#> 2 Length             [300 × 3] setosa vs…    -2.29     3.51 \n#> 3 Length             [300 × 3] versicolo…    -1.36     4.91 \n#> 4 Width              [300 × 3] setosa vs…    -2.31     0.786\n#> 5 Width              [300 × 3] setosa vs…    -2.07     1.14 \n#> 6 Width              [300 × 3] versicolo…    -1.20     1.68 \n#> # … with 8 more variables: estimate2 <dbl>,\n#> #   statistic <dbl>, p.value <dbl>, parameter <dbl>,\n#> #   conf.low <dbl>, conf.high <dbl>, method <chr>,\n#> #   alternative <chr>"},{"path":"map-function.html","id":"map_dbl-の使い方","chapter":"5 map 関数ってすごい","heading":"5.5 map_dbl() の使い方","text":"map_lgl(), map_int(), map_dbl(), map_chr() シリーズの関数が返すものは N = 1 のベクトルです。\nよって、適応する関数はベクトルを返すようにくみましょう。\\(df) {...} は無名関数と呼びます。\n\\(df) {...} は function(df) {...} の諸略です。\nこのとき、関数は summarise() を通して、tibble()を返すので、エラーが発生します。次のコードは pull() を使って、mean だけ返すようにしたが、\nN > 1 のベクトルなので、エラーが発生した。Species, measurement, part ごとに map_dbl() で平均を求めたいので、\n一旦 iris_long の data のネスティングを作り直します。エラーがなくなりましたが、グループごとの平均値をもとめたいなら、summarise() のほうがいいですね。map2() を使えば、 2変数渡せます。\nここでは map2_dbl() を使っています。pmap() の場合、渡す変数は list にまとめてから渡しましょう。","code":"\niris_long |> \n  mutate(out = map_dbl(data, \\(df) {\n    df |> \n      group_by(Species, part) |> \n      summarise(value = mean(value))\n  }))\n#> Error in `mutate()`:\n#> ! Problem while computing `out = map_dbl(...)`.\n#> Caused by error in `stop_bad_type()`:\n#> ! Result 1 must be a single double, not a vector of class `grouped_df/tbl_df/tbl/data.frame` and of length 3\niris_long |> \n  mutate(out = map_dbl(data, \\(df) {\n    df |> \n      group_by(Species, part) |> \n      summarise(value = mean(value)) |> pull(mean)\n  }))\n#> Error in `mutate()`:\n#> ! Problem while computing `out = map_dbl(...)`.\n#> Caused by error:\n#> ! Must extract column with a single valid subscript.\n#> ✖ Subscript `var` has the wrong type `function`.\n#> ℹ It must be numeric or character.\niris_long |> \n  unnest(data) |> \n  group_nest(Species, measurement, part) |> \n  mutate(out = map_dbl(data, \\(df) {\n    # mean(df$value) # でもOK\n    df |> summarise(value = mean(value)) |> pull(value)\n  }))\n#> # A tibble: 12 × 5\n#>    Species    measurement part                data   out\n#>    <fct>      <chr>       <chr> <list<tibble[,1]>> <dbl>\n#>  1 setosa     Length      Petal           [50 × 1] 1.46 \n#>  2 setosa     Length      Sepal           [50 × 1] 5.01 \n#>  3 setosa     Width       Petal           [50 × 1] 0.246\n#>  4 setosa     Width       Sepal           [50 × 1] 3.43 \n#>  5 versicolor Length      Petal           [50 × 1] 4.26 \n#>  6 versicolor Length      Sepal           [50 × 1] 5.94 \n#>  7 versicolor Width       Petal           [50 × 1] 1.33 \n#>  8 versicolor Width       Sepal           [50 × 1] 2.77 \n#>  9 virginica  Length      Petal           [50 × 1] 5.55 \n#> 10 virginica  Length      Sepal           [50 × 1] 6.59 \n#> 11 virginica  Width       Petal           [50 × 1] 2.03 \n#> 12 virginica  Width       Sepal           [50 × 1] 2.97\niris_long |> \n  unnest(data) |> \n  group_by(Species, measurement, part) |> \n  summarise(value = mean(value))\n#> # A tibble: 12 × 4\n#> # Groups:   Species, measurement [6]\n#>    Species    measurement part  value\n#>    <fct>      <chr>       <chr> <dbl>\n#>  1 setosa     Length      Petal 1.46 \n#>  2 setosa     Length      Sepal 5.01 \n#>  3 setosa     Width       Petal 0.246\n#>  4 setosa     Width       Sepal 3.43 \n#>  5 versicolor Length      Petal 4.26 \n#>  6 versicolor Length      Sepal 5.94 \n#>  7 versicolor Width       Petal 1.33 \n#>  8 versicolor Width       Sepal 2.77 \n#>  9 virginica  Length      Petal 5.55 \n#> 10 virginica  Length      Sepal 6.59 \n#> 11 virginica  Width       Petal 2.03 \n#> 12 virginica  Width       Sepal 2.97\niris |> \n  mutate(LW = map2_dbl(Petal.Length, Petal.Width, \\(l,w) {\n    (l * w)\n  })) \n#> # A tibble: 150 × 6\n#>    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#>           <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n#>  1          5.1         3.5          1.4         0.2 setosa \n#>  2          4.9         3            1.4         0.2 setosa \n#>  3          4.7         3.2          1.3         0.2 setosa \n#>  4          4.6         3.1          1.5         0.2 setosa \n#>  5          5           3.6          1.4         0.2 setosa \n#>  6          5.4         3.9          1.7         0.4 setosa \n#>  7          4.6         3.4          1.4         0.3 setosa \n#>  8          5           3.4          1.5         0.2 setosa \n#>  9          4.4         2.9          1.4         0.2 setosa \n#> 10          4.9         3.1          1.5         0.1 setosa \n#> # … with 140 more rows, and 1 more variable: LW <dbl>\niris |> \n  mutate(out = pmap_dbl(list(Petal.Length, Petal.Width, \n                             Sepal.Length, Sepal.Width), \\(pl,pw, sl, sw) {\n                               (pl * pw) / (sl * sw)\n                             })) \n#> # A tibble: 150 × 6\n#>    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#>           <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n#>  1          5.1         3.5          1.4         0.2 setosa \n#>  2          4.9         3            1.4         0.2 setosa \n#>  3          4.7         3.2          1.3         0.2 setosa \n#>  4          4.6         3.1          1.5         0.2 setosa \n#>  5          5           3.6          1.4         0.2 setosa \n#>  6          5.4         3.9          1.7         0.4 setosa \n#>  7          4.6         3.4          1.4         0.3 setosa \n#>  8          5           3.4          1.5         0.2 setosa \n#>  9          4.4         2.9          1.4         0.2 setosa \n#> 10          4.9         3.1          1.5         0.1 setosa \n#> # … with 140 more rows, and 1 more variable: out <dbl>"},{"path":"t-test.html","id":"t-test","chapter":"6 t 検定","heading":"6 t 検定","text":"","code":""},{"path":"t-test.html","id":"必要なパッケージ-5","chapter":"6 t 検定","heading":"6.1 必要なパッケージ","text":"","code":"\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(broom)"},{"path":"t-test.html","id":"データの準備-2","chapter":"6 t 検定","heading":"6.2 データの準備","text":"Rの iris8 データで解析を紹介します。\niris は data.frame として定義されているので、as_tibble() を使って tibble のクラスを追加します。irisには4つの数値データと1つの文字列データが入っています。このデータを可視化してから、解析します。","code":"\niris = iris |> as_tibble()\niris\n#> # A tibble: 150 × 5\n#>    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#>           <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n#>  1          5.1         3.5          1.4         0.2 setosa \n#>  2          4.9         3            1.4         0.2 setosa \n#>  3          4.7         3.2          1.3         0.2 setosa \n#>  4          4.6         3.1          1.5         0.2 setosa \n#>  5          5           3.6          1.4         0.2 setosa \n#>  6          5.4         3.9          1.7         0.4 setosa \n#>  7          4.6         3.4          1.4         0.3 setosa \n#>  8          5           3.4          1.5         0.2 setosa \n#>  9          4.4         2.9          1.4         0.2 setosa \n#> 10          4.9         3.1          1.5         0.1 setosa \n#> # … with 140 more rows\niris |> pivot_longer(cols = matches(\"Sepal|Petal\")) |> \n  separate(name, c(\"part\", \"measurement\")) |> \n  ggplot() + \n  geom_point(aes(x = Species, y = value, color = Species),\n             position = position_jitter(0.2)) +\n  scale_color_viridis_d(end = 0.8) +\n  facet_grid(rows = vars(part),\n             cols = vars(measurement))"},{"path":"t-test.html","id":"petal-と-sepal-の長さの比較","chapter":"6 t 検定","heading":"6.3 Petal と Sepal の長さの比較","text":"種を無視して、アヤメの Petal （花びら）と Sepal (萼片)の長さを比較しています。花びらと萼片の平均値、標準偏差、標準誤差を先にもとめます。\n専用の標準誤差の関数はないので、ここで定義します。H0: 花びらと萼片の長さに違いがないHA: 花びらと萼片の長さに違いがあるt検定は t.test() で実施します。\n関数の出力には、使用したデータ、t値、 自由度、 P値、平均値の差の95%信頼区間、それぞれの平均値の推定量が返ってきました。\n結論から説明すると、P < 0.0001 なので、有意水準が 0.05 のとき、帰無仮説は棄却できます。t.test() の結果をオブジェクトに書き込んだら、t値 (t value)、p値 (p value)、自由度 (degrees freedom) を抽出できます。英語論文に記述する場合は、次の通りです。mean petal length samples 3.76 ± 0.14, whereas mean sepal length 5.84 ± 0.07.\nWelch’S t-test revealed statistical significance differences two sites (t(211.54) = -13.098; P < 0.0001) significance level \\(\\alpha=\\) 0.05.大事なポイントは、この 3 つの情報を報告することです。t(211.54): 検定に使用した自由度（サンプル数の目安）-13.098: t検定の統計量P = 0.0000: 結果のP値","code":"\nse = function(x) {\n  sd(x) / sqrt(length(x))\n}\nirisout = iris |> \n  summarise(across(matches(\"Length\"), list(mean = mean, sd = sd, se = se)))\nirisout\n#> # A tibble: 1 × 6\n#>   Sepal.Length_mean Sepal.Length_sd Sepal.Length_se\n#>               <dbl>           <dbl>           <dbl>\n#> 1              5.84           0.828          0.0676\n#> # … with 3 more variables: Petal.Length_mean <dbl>,\n#> #   Petal.Length_sd <dbl>, Petal.Length_se <dbl>\nt.test(iris$Petal.Length, iris$Sepal.Length)\n#> \n#>  Welch Two Sample t-test\n#> \n#> data:  iris$Petal.Length and iris$Sepal.Length\n#> t = -13.098, df = 211.54, p-value < 2.2e-16\n#> alternative hypothesis: true difference in means is not equal to 0\n#> 95 percent confidence interval:\n#>  -2.399166 -1.771500\n#> sample estimates:\n#> mean of x mean of y \n#>  3.758000  5.843333\ndset_test = t.test(iris$Petal.Length, iris$Sepal.Length)\ndset_test$statistic  # t value\n#>         t \n#> -13.09835\ndset_test$parameter  # degrees of freedom \n#>       df \n#> 211.5427\ndset_test$p.value 　 # p value\n#> [1] 4.262173e-29"},{"path":"t-test.html","id":"petal-と-sepal-の幅の比較","chapter":"6 t 検定","heading":"6.4 Petal と Sepal の幅の比較","text":"先程の説明は横長のでデータ解析でしたが、tidy データ（縦長）の場合、もっと簡易に検定のコードをくめます。\nまず、データを縦長に変換します。記述統計ととり方も変わります。\nグループ化してから平均値、標準偏差、標準誤差を求めます。H0: 花びらと萼片の幅に違いがないHA: 花びらと萼片の幅に違いがある長さと幅のデータが混ざっているので、幅のデータのフィルターを掛けてから解析します。幅の解析をしても、P < 0.0001 なので、帰無仮説を棄却できます。","code":"\niris_long = iris |> pivot_longer(cols = matches(\"Sepal|Petal\")) |> \n  separate(name, c(\"part\", \"measurement\"))\niris_long\n#> # A tibble: 600 × 4\n#>    Species part  measurement value\n#>    <fct>   <chr> <chr>       <dbl>\n#>  1 setosa  Sepal Length        5.1\n#>  2 setosa  Sepal Width         3.5\n#>  3 setosa  Petal Length        1.4\n#>  4 setosa  Petal Width         0.2\n#>  5 setosa  Sepal Length        4.9\n#>  6 setosa  Sepal Width         3  \n#>  7 setosa  Petal Length        1.4\n#>  8 setosa  Petal Width         0.2\n#>  9 setosa  Sepal Length        4.7\n#> 10 setosa  Sepal Width         3.2\n#> # … with 590 more rows\nirisout = iris_long |> \n  group_by(part, measurement) |> \n  summarise(across(value, list(mean = mean, sd = sd, se = se)))\nirisout\n#> # A tibble: 4 × 5\n#> # Groups:   part [2]\n#>   part  measurement value_mean value_sd value_se\n#>   <chr> <chr>            <dbl>    <dbl>    <dbl>\n#> 1 Petal Length            3.76    1.77    0.144 \n#> 2 Petal Width             1.20    0.762   0.0622\n#> 3 Sepal Length            5.84    0.828   0.0676\n#> 4 Sepal Width             3.06    0.436   0.0356\niris_width = iris_long |> filter(str_detect(measurement, \"Width\"))\niris_width\n#> # A tibble: 300 × 4\n#>    Species part  measurement value\n#>    <fct>   <chr> <chr>       <dbl>\n#>  1 setosa  Sepal Width         3.5\n#>  2 setosa  Petal Width         0.2\n#>  3 setosa  Sepal Width         3  \n#>  4 setosa  Petal Width         0.2\n#>  5 setosa  Sepal Width         3.2\n#>  6 setosa  Petal Width         0.2\n#>  7 setosa  Sepal Width         3.1\n#>  8 setosa  Petal Width         0.2\n#>  9 setosa  Sepal Width         3.6\n#> 10 setosa  Petal Width         0.2\n#> # … with 290 more rows\nt.test(value ~ part, data = iris_width)\n#> \n#>  Welch Two Sample t-test\n#> \n#> data:  value by part\n#> t = -25.916, df = 237.03, p-value < 2.2e-16\n#> alternative hypothesis: true difference in means between group Petal and group Sepal is not equal to 0\n#> 95 percent confidence interval:\n#>  -1.999237 -1.716763\n#> sample estimates:\n#> mean in group Petal mean in group Sepal \n#>            1.199333            3.057333"},{"path":"t-test.html","id":"map-関数を用いて解析する","chapter":"6 t 検定","heading":"6.5 map() 関数を用いて解析する","text":"上に紹介した解析は長さと幅に分けてから実施したが、tidyverse の機能を上手につかば最低限のコードでできます。map()関数の詳細は専用の章に説明しています (5)。group_nest() を使って、データのグループ化とネスティングをします。これで、iris_long の tibble は2つの 300 x 3 tibbleに分離しました。\n解析は map() を通して実施します。結果は tout に入っています。","code":"\niris_long = iris |> pivot_longer(cols = matches(\"Sepal|Petal\")) |> \n  separate(name, c(\"part\", \"measurement\"))\niris_long = iris_long |> group_nest(measurement)\niris_long\n#> # A tibble: 2 × 2\n#>   measurement               data\n#>   <chr>       <list<tibble[,3]>>\n#> 1 Length               [300 × 3]\n#> 2 Width                [300 × 3]\nrunttest = function(df) {\n  t.test(value ~ part, data = df)\n}\n\niris_long = iris_long |> mutate(tout = map(data, runttest))\niris_long\n#> # A tibble: 2 × 3\n#>   measurement               data tout   \n#>   <chr>       <list<tibble[,3]>> <list> \n#> 1 Length               [300 × 3] <htest>\n#> 2 Width                [300 × 3] <htest>\nlibrary(broom)\n\niris_long |> mutate(tout = map(tout, tidy)) |> unnest(tout)\n#> # A tibble: 2 × 12\n#>   measurement              data estimate estimate1 estimate2\n#>   <chr>       <list<tibble[,3]>    <dbl>     <dbl>     <dbl>\n#> 1 Length              [300 × 3]    -2.09      3.76      5.84\n#> 2 Width               [300 × 3]    -1.86      1.20      3.06\n#> # … with 7 more variables: statistic <dbl>, p.value <dbl>,\n#> #   parameter <dbl>, conf.low <dbl>, conf.high <dbl>,\n#> #   method <chr>, alternative <chr>"},{"path":"t-test.html","id":"種ごとの比較","chapter":"6 t 検定","heading":"6.6 種ごとの比較","text":"検証する帰無仮説はH0: 種間の花びらの長さに違いがないまたは、萼片の幅に違いがないHA: 種間の花びらの長さに違いがあるまたは、萼片の幅に違いがあるすべての t 検定に対して、P < 0.0001 でした。\nよって、帰無仮説を棄却できます。\nところが、このように複数の組み合わせで検定をすることで、第1種の誤りを起こすことがあります。第1種の誤差とは、帰無仮説が事実であるのに、棄却する誤りです。\n第1種の誤りを起こす確率は有意水準と同じです（\\(\\alpha\\)）。\nつまり、$= $ 0.05 にした場合、第1種の誤りを起こす確率は 0.5 です。第1種の誤りは検定の回数によって上昇します。\\[\n\\text{Type-error rate} = 1 - (1-\\alpha)^n\n\\]アヤメの解析の場合、種間の比較は 3 回実施したので、第1種の誤りは、\\[\n1 - (1-0.05)^3 = 0.142625\n\\]\nです。有意水準を厳しくすることで誤りの確率を抑えることができます。\n例えば、全体の第1種の誤りを 0.05 に抑えたい場合、ここの検定の有意水準を小さくします。\\[\n1 - (1 + 0.05)^{1/3} = 0.01695243\n\\]\nつまり、ここの検定の有意水準を 0.016 にすれば、第1種の誤りは 0.047 に抑えられます。\nそれにしても、このような 多重比較 は専用の解析手法があります。\n複数群の平均値が統計学的に違うかを検証したい場合は分散分析のような線形解析を実施します。\n最初から複数群のペアごとの違いを検証したいなら、多重比較の検定法を使用します。","code":"\niris_long = iris |> pivot_longer(cols = matches(\"Sepal|Petal\")) |> \n  separate(name, c(\"part\", \"measurement\"))\n\nrunmultttest = function(df) {\n  #t12: setosa - versicolor\n  #t13: setosa - virginica\n  #t23: versicolor - virginica\n   \n  t12 = t.test(value ~ part, data = filter(df, str_detect(Species, \"set|ver\"))) |> tidy()\n  t13 = t.test(value ~ part, data = filter(df, str_detect(Species, \"set|vir\"))) |> tidy()\n  t23 = t.test(value ~ part, data = filter(df, str_detect(Species, \"ver|vir\"))) |> tidy()\n  bind_rows(\"setosa vs. versicolor\" = t12, \n            \"setosa vs. virginica\" = t13, \n            \"versicolor vs. virginica\" = t23, .id = \"comparison\")\n}\n\niris_long |> group_nest(measurement) |> \n  mutate(tout = map(data, runmultttest)) |> \n  unnest(tout) \n#> # A tibble: 6 × 13\n#>   measurement             data comparison estimate estimate1\n#>   <chr>       <list<tibble[,3> <chr>         <dbl>     <dbl>\n#> 1 Length             [300 × 3] setosa vs…    -2.61     2.86 \n#> 2 Length             [300 × 3] setosa vs…    -2.29     3.51 \n#> 3 Length             [300 × 3] versicolo…    -1.36     4.91 \n#> 4 Width              [300 × 3] setosa vs…    -2.31     0.786\n#> 5 Width              [300 × 3] setosa vs…    -2.07     1.14 \n#> 6 Width              [300 × 3] versicolo…    -1.20     1.68 \n#> # … with 8 more variables: estimate2 <dbl>,\n#> #   statistic <dbl>, p.value <dbl>, parameter <dbl>,\n#> #   conf.low <dbl>, conf.high <dbl>, method <chr>,\n#> #   alternative <chr>"},{"path":"anova.html","id":"anova","chapter":"7 分散分析","heading":"7 分散分析","text":"","code":""},{"path":"anova.html","id":"必要なパッケージ-6","chapter":"7 分散分析","heading":"7.1 必要なパッケージ","text":"","code":"\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(car)\nlibrary(emmeans)"},{"path":"anova.html","id":"データの読み込み-1","chapter":"7 分散分析","heading":"7.2 データの読み込み","text":"","code":"\nrootdatafolder = rprojroot::find_rstudio_root_file(\"Data/\")\nfilename = '瀬戸内海藻場データ.xlsx'\npath = str_c(rootdatafolder, filename)\n\n# fy1990 の処理\nRNG = \"A4:C27\"   # セルの範囲\nSHEET = \"FY1990\" # シート名\nd19 = read_xlsx(path, sheet = SHEET, range = RNG)\nd19 = d19 |> \n  rename(site = 調査海域, seaweed = 海藻, seagrass = 海草) |> \n  mutate(site = factor(site, levels = c('東部', '中部', '西部')))\n\n# fy2018の処理\nRNG = \"A6:C15\"   # 海藻データのセル範囲\nSHEET = \"FY2018\" # シート名\nseaweed = read_xlsx(path, sheet = SHEET, range = RNG)\nRNG = \"E6:G15\"   # 海草データのセル範囲\n\nseagrass = read_xlsx(path, sheet = SHEET, range = RNG)\nseaweed = seaweed |> pivot_longer(cols = everything())\nseagrass = seagrass |> pivot_longer(cols = everything())\n\nd20 = bind_rows(seaweed = seaweed, seagrass = seagrass, .id = \"type\")\nd20 = d20 |> pivot_wider(id_cols = name,\n                   names_from = type, values_from = value, \n                   values_fn = \"list\")\nd20 = d20 |> unnest(c(seaweed, seagrass)) |> rename(site = name) |> drop_na()\nd20 = d20 |> \n  mutate(site = factor(site, levels = c('東部', '中部', '西部')))"},{"path":"anova.html","id":"一元配置分散分析","chapter":"7 分散分析","heading":"7.3 一元配置分散分析","text":"では、一元配置分散分析を実施します。\nまず、分散分析の平方和を正しく求めるためには、contr.sum を設定することです。\nその処理のあと、lm() 関数でモデルを当てはめます。\nlm() 関数に渡すモデルは、 〜 の右辺に説明変数、左辺に観測値を指定しましょう。FY1990 海藻藻場面積の一元配置分散分析の結果は次のとおりです。FY2019 海藻藻場面積の一元配置分散分析の結果は次のとおりです。FY1990 のP値は P = 0.0944、\nFY2018 のP値は P = 0.0750 でした。\nどちらも有意水準 (α = 0.05) より大きいので、帰無仮説（海域間の藻場面積は同じ）を棄却できません。等分散性と正規性の検定を無視したように、今回だけ分散分析の結果を無視して、多重比較をしてみます。","code":"\ncontrasts(d19$site) = contr.sum\ncontrasts(d20$site) = contr.sum\nm19 = lm(seaweed ~ site, data = d19)\nm20 = lm(seaweed ~ site, data = d20)\na19 = anova(m19)\na20 = anova(m20)\nanova(m19) # FY1990 の処理\n#> Analysis of Variance Table\n#> \n#> Response: seaweed\n#>           Df  Sum Sq Mean Sq F value  Pr(>F)  \n#> site       2  892963  446482  2.6621 0.09439 .\n#> Residuals 20 3354343  167717                  \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nanova(m20) # FY2018 の処理\n#> Analysis of Variance Table\n#> \n#> Response: seaweed\n#>           Df  Sum Sq Mean Sq F value  Pr(>F)  \n#> site       2  330811  165405  2.9569 0.07499 .\n#> Residuals 20 1118771   55939                  \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"anova.html","id":"多重比較","chapter":"7 分散分析","heading":"7.4 多重比較","text":"調査海域の全ペアの比較をしるので、Tukey HSDを用います。FY2019 の場合、全ペアを比較したら、有意な結果はありません。FY2020 も同じですね。この用な結果は予想していました。そもそも分散分析から有意な結果がでなかったので、多重比較しても有意な結果はでません。ちなみに Dunnet Method をつかって、西部と東部を中部と比較したら次の結果になります。Dunnet Method の場合でも有意な結果はありません。","code":"\ne19 = emmeans(m19, specs = pairwise ~ site, adjust = \"tukey\")\ne20 = emmeans(m20, specs = pairwise ~ site, adjust = \"tukey\")\ne19 # FY1990 の処理\n#> $emmeans\n#>  site emmean  SE df lower.CL upper.CL\n#>  東部    118 167 20     -230      467\n#>  中部    205 137 20      -80      490\n#>  西部    578 145 20      276      880\n#> \n#> Confidence level used: 0.95 \n#> \n#> $contrasts\n#>     contrast estimate  SE df t.ratio p.value\n#>  東部 - 中部    -86.4 216 20  -0.400  0.9158\n#>  東部 - 西部   -459.3 221 20  -2.077  0.1202\n#>  中部 - 西部   -372.8 199 20  -1.874  0.1723\n#> \n#> P value adjustment: tukey method for comparing a family of 3 estimates\ne20 # FY2018 の処理\n#> $emmeans\n#>  site emmean   SE df lower.CL upper.CL\n#>  東部  109.3 96.6 20    -92.1      311\n#>  中部  301.0 78.8 20    136.5      465\n#>  西部   29.2 83.6 20   -145.2      204\n#> \n#> Confidence level used: 0.95 \n#> \n#> $contrasts\n#>     contrast estimate  SE df t.ratio p.value\n#>  東部 - 中部   -191.7 125 20  -1.538  0.2953\n#>  東部 - 西部     80.1 128 20   0.627  0.8072\n#>  中部 - 西部    271.8 115 20   2.365  0.0696\n#> \n#> P value adjustment: tukey method for comparing a family of 3 estimates\ne19d = emmeans(m19, specs = trt.vs.ctrl ~ site, ref = 2)\ne20d = emmeans(m20, specs = trt.vs.ctrl ~ site, ref = 2)\ne19d # FY1990 の処理\n#> $emmeans\n#>  site emmean  SE df lower.CL upper.CL\n#>  東部    118 167 20     -230      467\n#>  中部    205 137 20      -80      490\n#>  西部    578 145 20      276      880\n#> \n#> Confidence level used: 0.95 \n#> \n#> $contrasts\n#>     contrast estimate  SE df t.ratio p.value\n#>  東部 - 中部    -86.4 216 20  -0.400  0.8781\n#>  西部 - 中部    372.8 199 20   1.874  0.1373\n#> \n#> P value adjustment: dunnettx method for 2 tests\ne20d # FY2018 の処理\n#> $emmeans\n#>  site emmean   SE df lower.CL upper.CL\n#>  東部  109.3 96.6 20    -92.1      311\n#>  中部  301.0 78.8 20    136.5      465\n#>  西部   29.2 83.6 20   -145.2      204\n#> \n#> Confidence level used: 0.95 \n#> \n#> $contrasts\n#>     contrast estimate  SE df t.ratio p.value\n#>  東部 - 中部     -192 125 20  -1.538  0.2441\n#>  西部 - 中部     -272 115 20  -2.365  0.0531\n#> \n#> P value adjustment: dunnettx method for 2 tests"},{"path":"anova.html","id":"二元配置分散分析","chapter":"7 分散分析","heading":"7.5 二元配置分散分析","text":"","code":""},{"path":"anova.html","id":"正規性と等分散性の確認","chapter":"7 分散分析","heading":"7.6 正規性と等分散性の確認","text":"分散分析を行う前に、Levene Test と Shapiro-Wilk Normality Test でデータの等分散性9 と正規性10 を確認します。\nルビーン検定とシャピロウィルク検定については、t 検定の資料を参考にしてください。\nここで紹介する解析は 海藻 に対してです。ルビーン検定FY1990とFY2018 データの等分散性検定結果は P = 0.0996 でしたので、\n帰無仮説は棄却できません。\nつまり、等分散性であると判断できます。シャピロウィルク検定FY1990とFY2018 データの等分散性について、P < 0.0001 だったので、\n帰無仮説を棄却できます。\nデータの母集団は正規分布に従わないかもしれないです。","code":"\ndall = bind_rows(fy1990 = d19,\n                 fy2018 = d20, \n                 .id = \"year\")\ndall = dall |> mutate(year = factor(year))\nleveneTest(seaweed ~ site*year, data = dall) \n#> Levene's Test for Homogeneity of Variance (center = median)\n#>       Df F value Pr(>F)  \n#> group  5  1.9994 0.0996 .\n#>       40                 \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nshapiro.test(x = dall$seaweed) # FY1990 の処理\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  dall$seaweed\n#> W = 0.63263, p-value = 1.731e-09"},{"path":"anova.html","id":"二元配置分散分析-1","chapter":"7 分散分析","heading":"7.7 二元配置分散分析","text":"では、二元配置分散分析を実施します。\nまず、分散分析の平方和を正しく求めるためには、contr.sum を設定することです。\nその処理のあと、lm() 関数でモデルを当てはめます。\nlm() 関数に渡すモデルは、 〜 の右辺に説明変数、左辺に観測値を指定しましょう。FY1990 海藻藻場面積の一元配置分散分析の結果は次のとおりです。site 効果のP値は P = 0.3274、\nyear 効果のP値は P = 0.1323、\n相互作用のP値は P = 0.0200 でした。\n相互作用のP値は有意水準 (α = 0.05) より大きいので、相互作用の帰無仮説は棄却できますが、主効果の帰無仮説は棄却できません。","code":"\ncontrasts(dall$site) = contr.sum\ncontrasts(dall$year) = contr.sum\nmall = lm(seaweed ~ site*year, data = dall)\nAnova(mall, type =3)\n#> Anova Table (Type III tests)\n#> \n#> Response: seaweed\n#>              Sum Sq Df F value    Pr(>F)    \n#> (Intercept) 2230084  1 19.9421 6.382e-05 ***\n#> site         256846  2  1.1484   0.32738    \n#> year         263994  1  2.3607   0.13230    \n#> site:year    966928  2  4.3233   0.01996 *  \n#> Residuals   4473114 40                      \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"anova.html","id":"多重比較-1","chapter":"7 分散分析","heading":"7.8 多重比較","text":"調査海域の全ペアの比較をしるので、Tukey HSDを用います。FY2019 の場合、全ペアを比較したら、有意な結果はありません。","code":"\neall = emmeans(mall, specs = pairwise ~ site:year, adjust = \"tukey\")\neall \n#> $emmeans\n#>  site year   emmean  SE df lower.CL upper.CL\n#>  東部 fy1990  118.3 137 40   -157.6      394\n#>  中部 fy1990  204.8 111 40    -20.5      430\n#>  西部 fy1990  577.6 118 40    338.7      817\n#>  東部 fy2018  109.3 137 40   -166.6      385\n#>  中部 fy2018  301.0 111 40     75.7      526\n#>  西部 fy2018   29.2 118 40   -209.7      268\n#> \n#> Confidence level used: 0.95 \n#> \n#> $contrasts\n#>      contrast              estimate  SE df t.ratio p.value\n#>  東部 fy1990 - 中部 fy1990    -86.4 176 40  -0.490  0.9962\n#>  東部 fy1990 - 西部 fy1990   -459.3 181 40  -2.543  0.1360\n#>  東部 fy1990 - 東部 fy2018      9.0 193 40   0.047  1.0000\n#>  東部 fy1990 - 中部 fy2018   -182.7 176 40  -1.036  0.9027\n#>  東部 fy1990 - 西部 fy2018     89.1 181 40   0.493  0.9961\n#>  中部 fy1990 - 西部 fy1990   -372.8 162 40  -2.295  0.2201\n#>  中部 fy1990 - 東部 fy2018     95.4 176 40   0.542  0.9940\n#>  中部 fy1990 - 中部 fy2018    -96.2 158 40  -0.610  0.9897\n#>  中部 fy1990 - 西部 fy2018    175.5 162 40   1.080  0.8863\n#>  西部 fy1990 - 東部 fy2018    468.3 181 40   2.593  0.1226\n#>  西部 fy1990 - 中部 fy2018    276.6 162 40   1.702  0.5383\n#>  西部 fy1990 - 西部 fy2018    548.4 167 40   3.280  0.0245\n#>  東部 fy2018 - 中部 fy2018   -191.7 176 40  -1.087  0.8835\n#>  東部 fy2018 - 西部 fy2018     80.1 181 40   0.443  0.9977\n#>  中部 fy2018 - 西部 fy2018    271.8 162 40   1.672  0.5573\n#> \n#> P value adjustment: tukey method for comparing a family of 6 estimates\nemmeans(mall, specs = pairwise ~ site|year, adjust = \"tukey\")\n#> $emmeans\n#> year = fy1990:\n#>  site emmean  SE df lower.CL upper.CL\n#>  東部  118.3 137 40   -157.6      394\n#>  中部  204.8 111 40    -20.5      430\n#>  西部  577.6 118 40    338.7      817\n#> \n#> year = fy2018:\n#>  site emmean  SE df lower.CL upper.CL\n#>  東部  109.3 137 40   -166.6      385\n#>  中部  301.0 111 40     75.7      526\n#>  西部   29.2 118 40   -209.7      268\n#> \n#> Confidence level used: 0.95 \n#> \n#> $contrasts\n#> year = fy1990:\n#>     contrast estimate  SE df t.ratio p.value\n#>  東部 - 中部    -86.4 176 40  -0.490  0.8762\n#>  東部 - 西部   -459.3 181 40  -2.543  0.0389\n#>  中部 - 西部   -372.8 162 40  -2.295  0.0681\n#> \n#> year = fy2018:\n#>     contrast estimate  SE df t.ratio p.value\n#>  東部 - 中部   -191.7 176 40  -1.087  0.5273\n#>  東部 - 西部     80.1 181 40   0.443  0.8976\n#>  中部 - 西部    271.8 162 40   1.672  0.2282\n#> \n#> P value adjustment: tukey method for comparing a family of 3 estimates\nemmeans(mall, specs = pairwise ~ year|site, adjust = \"tukey\")\n#> $emmeans\n#> site = 東部:\n#>  year   emmean  SE df lower.CL upper.CL\n#>  fy1990  118.3 137 40   -157.6      394\n#>  fy2018  109.3 137 40   -166.6      385\n#> \n#> site = 中部:\n#>  year   emmean  SE df lower.CL upper.CL\n#>  fy1990  204.8 111 40    -20.5      430\n#>  fy2018  301.0 111 40     75.7      526\n#> \n#> site = 西部:\n#>  year   emmean  SE df lower.CL upper.CL\n#>  fy1990  577.6 118 40    338.7      817\n#>  fy2018   29.2 118 40   -209.7      268\n#> \n#> Confidence level used: 0.95 \n#> \n#> $contrasts\n#> site = 東部:\n#>  contrast        estimate  SE df t.ratio p.value\n#>  fy1990 - fy2018      9.0 193 40   0.047  0.9631\n#> \n#> site = 中部:\n#>  contrast        estimate  SE df t.ratio p.value\n#>  fy1990 - fy2018    -96.2 158 40  -0.610  0.5451\n#> \n#> site = 西部:\n#>  contrast        estimate  SE df t.ratio p.value\n#>  fy1990 - fy2018    548.4 167 40   3.280  0.0022"},{"path":"anova.html","id":"等分散性と正規性の事後確認","chapter":"7 分散分析","heading":"7.9 等分散性と正規性の事後確認","text":"plot() に渡している mall は前章に当てはめた二元配置分散分析のモデルです。","code":""},{"path":"anova.html","id":"等分散性の確認に使うプロット","chapter":"7 分散分析","heading":"7.10 等分散性の確認に使うプロット","text":"\nFigure 7.1: 残渣 vs. 期待値\nFig. 7.1 は残渣11\nと期待値12 の関係を理解するてめに使います。\n等分散性に問題がない場合、残渣は y = 0 の周りを均一に、変動なくばらつきます。\nところが Fig. 7.1 の場合、期待値が高いとき、残渣のばらつきが大きい。\nFigure 7.2: スケール・位置プロット\nFig. 7.2 はスケール・ロケーションプロットといいます。\nスケール13 は確率密度分布のばらつきのパラメータです。\n位置（ロケーション）14 は確率分布の中心のパラメータです。\nたとえば、正規分布のスケールパラメータは分散、位置パラメータは平均値です。\nFig. 7.2 の横軸は位置、縦長はスケールパラメータで標準化した残渣の平方根です。\n示されている標準化残渣のばらつきが均一で、期待値15 と無関係であれば、ばらつきは均一であると考えられます。\nFig. 7.2 の場合、標準化残渣は期待値と正の関係があるので、ばらつきは均一であると考えられません。","code":"\nplot(mall, which = 1)\nplot(mall, which = 3)"},{"path":"anova.html","id":"正規性の確認に使うプロット","chapter":"7 分散分析","heading":"7.11 正規性の確認に使うプロット","text":"\nFigure 7.3: QQプロット\n","code":"\nplot(mall, which = 2)"},{"path":"anova.html","id":"飛び値異常値の確認プロット","chapter":"7 分散分析","heading":"7.12 飛び値・異常値の確認プロット","text":"\nFigure 7.4: クックの距離とてこ比\n","code":"\nplot(mall, which = 5)"},{"path":"linear-models.html","id":"linear-models","chapter":"8 線形モデル","heading":"8 線形モデル","text":"","code":""},{"path":"linear-models.html","id":"必要なパッケージ-7","chapter":"8 線形モデル","heading":"8.1 必要なパッケージ","text":"","code":"\nlibrary(tidyverse)\nlibrary(car)\nlibrary(patchwork)"},{"path":"linear-models.html","id":"線形モデルとは","chapter":"8 線形モデル","heading":"8.2 線形モデルとは？","text":"線形モデルの 線型 (linear) は直線 (straight line) と全く関係ないです。線型モデルの線型は 線型結合 (linear combination) を意味しています。\\[\ny = b_1 x_1 + b_2 x_2 + b_3 x_3 + \\cdots + b_n x_n\n\\]\n\\(b_i\\) は係数，\\(x_i\\) はベクトル（変数, 説明変数）です。\\(y\\) が \\(x_i\\) の線型結合です。","code":""},{"path":"linear-models.html","id":"線型モデルは直線じゃなくてもいい","chapter":"8 線形モデル","heading":"8.3 線型モデルは直線じゃなくてもいい","text":"線形モデル\\(\\mu = \\beta_0 + \\beta_1 x_1\\)\\(\\mu = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2^2\\)\\(\\mu = \\beta_0 + \\beta_1\\exp(x_1)\\)非線形モデル\\(\\mu = \\frac{\\beta_1 x_1}{\\beta_2 + x_1}\\)\\(\\mu = \\beta_1 \\left(1-\\exp(\\beta_2 x_1)\\right) + \\beta_3\\)\\(\\mu = \\beta_0 + \\beta_1\\exp(\\beta_2 x_1)\\)線形モデルは直線じゃなくてもいい。\nただし，変数 (\\(x_i\\)) は線型結合であることが重要なポイントです。\nでは、検討しているモデルは線形モデルかどうかを確認したいなら、パラメータに対してモデルの偏微分方程式を求めてください。たとえば、モデルパラメータ \\((\\beta_i)\\) に対して，\\(\\mu\\)を微分します。\\[\n\\mu = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2^2\n\\]パラメータの数ほど微分方程式があります。\\[\n\\frac{\\partial \\mu}{\\partial \\beta_0}  = 1 \\qquad\n\\frac{\\partial \\mu}{\\partial \\beta_1}  = x_1 \\qquad\n\\frac{\\partial \\mu}{\\partial \\beta_2}  = x_2\n\\]\\(\\beta_i\\) はどの微分方程式に残らないので，このモデルは線形モデルでしょう。次のモデルは非線形モデルです。\\[\n\\mu = \\frac{\\beta_1 x_1}{\\beta_2 + x_1}\n\\]モデルパラメータは \\(\\beta_1\\), \\(\\beta_2\\) ですね。\nではパラメータに対する偏微分方程式は次の通りです。\\[\n\\begin{aligned}\n\\frac{\\partial \\mu}{\\partial \\beta_1}  &= \\frac{x_1}{\\beta_2 x_1} \\\\\n\\frac{\\partial \\mu}{\\partial \\beta_2}  &= -\\frac{x_1}{(\\beta_2 x_2)^2}\n\\end{aligned}\n\\]パラメータはお互いの方程式に残るので，このモデルは線形モデルではないですね。","code":""},{"path":"linear-models.html","id":"線形モデルの仮定","chapter":"8 線形モデル","heading":"8.4 線形モデルの仮定","text":"線形モデルを用いるときに守るべき仮定は分散分析と同じです。残差が正規分布に従うこと。残差またはデータはお互いに独立し，同一分布から発生していること。モデルに対する注意する点説明変数もお互いに関係性（相関関係）が低いこと（関係ないほうがいい）。\n関係性が高いとき，モデルパラメータの推定量の精度 (precision) と正確度 (accuracy) が落ちます。\nこの現象は多重共線性 (multicollinearity) とよびます。\n関係性が高いとき，モデルパラメータの推定量の精度 (precision) と正確度 (accuracy) が落ちます。この現象は多重共線性 (multicollinearity) とよびます。説明変数はお互いに関係がなければ，直交性 (orthogonal) のあるモデルとなり，変数の推定量はお互いとの相関性がないです。野外データは上の条件を満たすことは珍しいが、すべての条件を満たさなくても解析はできます。\nただし、結果の解釈と考察には気をつけましょう。","code":""},{"path":"linear-models.html","id":"一般線形モデルの例-general-linear-model","chapter":"8 線形モデル","heading":"8.5 一般線形モデルの例 (General Linear Model)","text":"解析例にはアヤメのデータをつかっています。\nちなみに、一般線形モデルは一般化線形モデル (Generalized Linear Model, GLM) の特例です。検討する線型モデルは次の通りです。\\[\nE(\\text{Petal Length}) = b_0 + b_1\\text{Petal Width} + b_2\\text{Sepal Length} + b_3\\text{Sepal Width}\n\\]\\(E(\\text{Petal Length})\\) は花びらの長さの期待値 (expected value) といいます。線形モデルなので，lm() 関数で解析できます。\nさらに、分散分析表も存在します。分散分析表は summary.aov() でだします。モデルに入れた全ての説明変数のP値は \\(< 0.0001\\) ですね。では、線形モデルの係数表を出してみましょう。Petal.Length に対して，Petal.Width と Sepal.Length は正の効果があり，Sepal.Width とは負の効果がありました。\nつまり， Petal.Width と Sepal.Length が上昇すると， Petal.Length も上昇します。分散分析表の場合は、それぞれの変数に対するF値がでたが、係数表の場合は t値 がでました。\n当然それぞれの値も異なった。何が違うのか？まず、分散分析の平方和って、さまざま求め方がるあることに気づきましょう。Type-Sum--squares\nSS(), SS(B|), SS(AB|B, )\nSS(), SS(B|), SS(AB|B, )Type-II Sum--squares\nSS(|B), SS(B|)\nSS(|B), SS(B|)Type-III Sum--squares\nSS(|B), SS(B|), SS(AB|B,)\nSS(|B), SS(B|), SS(AB|B,)他にもありますが、この 3 つが一般的にです。Type-の場合、結果は変数の順序に依存します。\nType-II の場合、順序に依存しないが、相互作用はない。\nType-III の場合、順序に依存しないが、相互作用の影響も計算されます。Type-はRのデフォルトなので、Type-II または Type-II を使いたいなら、\ncar パッケージの Anova() 関数が必要です。といくおとで、説明変数の順序をかえるた、次のように異なる結果が返ってきます。このとき、car パッケージの Anova() 関数を使って平方和の求め方を指定します。\n相互作用なしのモデルなので、Type-II を指定しました。\n相互作用も調べたいなら、type=\"III\" を渡してください。係数表は t値の結果は Wald’s test と呼びます。このとき、\\(\\beta = 0\\) の帰無仮説を検証しています。\nつまり、係数の値は 0 か 0 じゃないかですね。Wald’s test と F-test のこちらもモデルの解析に使えます。\nところが、一般的には、モデル選択に F-test を使いますが、\n係数が 0 か 0 じゃないかの検証には Wald’s test を使います。\nちなみに、このモデルの場合、\\((t_\\nu)^2 = F_{(1,\\nu)}\\) なので、\n係数表のt値の2乗は Type-II 分散分析のF値と同じです。","code":"\niris = iris %>% as_tibble()\niris %>% print(n = 3)\n#> # A tibble: 150 × 5\n#>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#>          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n#> 1          5.1         3.5          1.4         0.2 setosa \n#> 2          4.9         3            1.4         0.2 setosa \n#> 3          4.7         3.2          1.3         0.2 setosa \n#> # … with 147 more rows\nm1 = lm(Petal.Length ~ Petal.Width + Sepal.Length + Sepal.Width, data = iris)\nm1 %>% summary.aov() %>% print(signif.stars = F)\n#>               Df Sum Sq Mean Sq F value Pr(>F)\n#> Petal.Width    1  430.5   430.5 4231.49 <2e-16\n#> Sepal.Length   1    9.9     9.9   97.74 <2e-16\n#> Sepal.Width    1    9.0     9.0   88.95 <2e-16\n#> Residuals    146   14.9     0.1\nm1 %>% summary() %>% print(signif.stars = F)\n#> \n#> Call:\n#> lm(formula = Petal.Length ~ Petal.Width + Sepal.Length + Sepal.Width, \n#>     data = iris)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -0.99333 -0.17656 -0.01004  0.18558  1.06909 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)\n#> (Intercept)  -0.26271    0.29741  -0.883    0.379\n#> Petal.Width   1.44679    0.06761  21.399   <2e-16\n#> Sepal.Length  0.72914    0.05832  12.502   <2e-16\n#> Sepal.Width  -0.64601    0.06850  -9.431   <2e-16\n#> \n#> Residual standard error: 0.319 on 146 degrees of freedom\n#> Multiple R-squared:  0.968,  Adjusted R-squared:  0.9674 \n#> F-statistic:  1473 on 3 and 146 DF,  p-value: < 2.2e-16\nm1a  = lm(Petal.Length ~ Petal.Width + Sepal.Length + Sepal.Width, data = iris)\nm1b = lm(Petal.Length ~ Petal.Width + Sepal.Width + Sepal.Length, data = iris)\nm1c = lm(Petal.Length ~ Sepal.Width + Petal.Width + Sepal.Length, data = iris)\nm1a |> summary.aov()\n#>               Df Sum Sq Mean Sq F value Pr(>F)    \n#> Petal.Width    1  430.5   430.5 4231.49 <2e-16 ***\n#> Sepal.Length   1    9.9     9.9   97.74 <2e-16 ***\n#> Sepal.Width    1    9.0     9.0   88.95 <2e-16 ***\n#> Residuals    146   14.9     0.1                   \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nm1b |> summary.aov()\n#>               Df Sum Sq Mean Sq F value   Pr(>F)    \n#> Petal.Width    1  430.5   430.5 4231.49  < 2e-16 ***\n#> Sepal.Width    1    3.1     3.1   30.37 1.57e-07 ***\n#> Sepal.Length   1   15.9    15.9  156.31  < 2e-16 ***\n#> Residuals    146   14.9     0.1                     \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nm1c |> summary.aov()\n#>               Df Sum Sq Mean Sq F value Pr(>F)    \n#> Sepal.Width    1   85.2    85.2   837.8 <2e-16 ***\n#> Petal.Width    1  348.3   348.3  3424.1 <2e-16 ***\n#> Sepal.Length   1   15.9    15.9   156.3 <2e-16 ***\n#> Residuals    146   14.9     0.1                   \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nm1a |> Anova(type = \"II\")\n#> Anova Table (Type II tests)\n#> \n#> Response: Petal.Length\n#>              Sum Sq  Df F value    Pr(>F)    \n#> Petal.Width  46.584   1 457.905 < 2.2e-16 ***\n#> Sepal.Length 15.902   1 156.312 < 2.2e-16 ***\n#> Sepal.Width   9.049   1  88.947 < 2.2e-16 ***\n#> Residuals    14.853 146                      \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nm1b |> Anova(type = \"II\")\n#> Anova Table (Type II tests)\n#> \n#> Response: Petal.Length\n#>              Sum Sq  Df F value    Pr(>F)    \n#> Petal.Width  46.584   1 457.905 < 2.2e-16 ***\n#> Sepal.Width   9.049   1  88.947 < 2.2e-16 ***\n#> Sepal.Length 15.902   1 156.312 < 2.2e-16 ***\n#> Residuals    14.853 146                      \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nm1c |> Anova(type = \"II\")\n#> Anova Table (Type II tests)\n#> \n#> Response: Petal.Length\n#>              Sum Sq  Df F value    Pr(>F)    \n#> Sepal.Width   9.049   1  88.947 < 2.2e-16 ***\n#> Petal.Width  46.584   1 457.905 < 2.2e-16 ***\n#> Sepal.Length 15.902   1 156.312 < 2.2e-16 ***\n#> Residuals    14.853 146                      \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nm1 |> summary()\n#> \n#> Call:\n#> lm(formula = Petal.Length ~ Petal.Width + Sepal.Length + Sepal.Width, \n#>     data = iris)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -0.99333 -0.17656 -0.01004  0.18558  1.06909 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  -0.26271    0.29741  -0.883    0.379    \n#> Petal.Width   1.44679    0.06761  21.399   <2e-16 ***\n#> Sepal.Length  0.72914    0.05832  12.502   <2e-16 ***\n#> Sepal.Width  -0.64601    0.06850  -9.431   <2e-16 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.319 on 146 degrees of freedom\n#> Multiple R-squared:  0.968,  Adjusted R-squared:  0.9674 \n#> F-statistic:  1473 on 3 and 146 DF,  p-value: < 2.2e-16"},{"path":"linear-models.html","id":"モデル診断","chapter":"8 線形モデル","heading":"8.6 モデル診断","text":"モデルを当てはめたあと、最も重要な解析はモデル残渣の確認です。\nモデル残渣に異常があると、モデルとデータの相性が悪いです。\nモデルとデータが合わない場合、求めた係数を信用できません。一般線形モデルのとき、標準化残渣 (standardized residuals) をもとめて、モデルの良さを目で確認します。\nつまり、標準化残渣を様々形の図を作ります。まず、残渣を持てめて、図に使うデータを準備します。","code":"\nfiris = fortify(m1) |> as_tibble()\nfiris\n#> # A tibble: 150 × 10\n#>    Petal.Length Petal.Width Sepal.Length Sepal.Width   .hat\n#>           <dbl>       <dbl>        <dbl>       <dbl>  <dbl>\n#>  1          1.4         0.2          5.1         3.5 0.0205\n#>  2          1.4         0.2          4.9         3   0.0212\n#>  3          1.3         0.2          4.7         3.2 0.0201\n#>  4          1.5         0.2          4.6         3.1 0.0221\n#>  5          1.4         0.2          5           3.6 0.0230\n#>  6          1.7         0.4          5.4         3.9 0.0327\n#>  7          1.4         0.3          4.6         3.4 0.0255\n#>  8          1.5         0.2          5           3.4 0.0189\n#>  9          1.4         0.2          4.4         2.9 0.0293\n#> 10          1.5         0.1          4.9         3.1 0.0225\n#> # … with 140 more rows, and 5 more variables: .sigma <dbl>,\n#> #   .cooksd <dbl>, .fitted <dbl>, .resid <dbl>,\n#> #   .stdresid <dbl>"},{"path":"linear-models.html","id":"残渣の正規性","chapter":"8 線形モデル","heading":"8.6.1 残渣の正規性","text":"まずは残渣は正規分布に従うかを評価します。\n正規性はヒストグラムとQQプロットでします。\n残渣が正規分布に従うなら、ヒストグラムは正規分布に似ていて、\nQQプロットでは残渣を示す点が赤色の直線上に並びます。\nFigure 8.1: 残渣のヒストグラムと正規分布。\n\nFigure 8.2: QQプロット\nどちらの図を確認しても、標準化残渣は正規分布に従っているように見えます。","code":"\nggplot(firis) + \n  geom_histogram(aes(x = .stdresid, y = ..density..)) +\n  stat_function(fun = dnorm, color = \"red\") +\n  labs(x = \"Standardized residual\", y = \"\") \nggplot(firis) + \n  geom_qq(aes(sample = .stdresid)) +\n  geom_abline(color = \"red\") +\n  labs(x = \"Theoretical Quantile\", y = \"Standardized residual\") "},{"path":"linear-models.html","id":"残渣のばらつき","chapter":"8 線形モデル","heading":"8.6.2 残渣のばらつき","text":"正規性に問題がなければ、次は標準化残渣のばらつきを確認したいです。\nすべての変数に対して残渣の図をつくります。\nまた、求めた期待値に対しても残渣を確認します。\n残渣になんかしらのパタンがあると、問題です。\nFigure 8.3: 標準化残渣と応答変数、それぞれの説明変数との関係。\n残渣は 0 をまたいで均等にばらついていることが確認できました。\nさらに、ばらつきに明確なパタンがないので、この点についてモデルには問題なさそうです。\n次は残渣と期待値の関係を確認します。\nFigure 8.4: 標準化残渣と期待値の関係。\n期待値と標準化残渣の関係を確認すると、特に問題はないですね。\n場合によって、標準化残渣の絶対値の平方根で確認しやすい場合もあります。\nこの解析の場合は不必要ですが、コードと結果は次の通りです。\nFigure 8.5: 標準化残渣の平方根と期待値の関係。\n最後に、クックの距離を確認します。クックの距離 (Cook’s distance) はモデルの当てはめに強く影響する値を検出してくれます。\nデータ点のクックの距離が \\(P(F_{(n, n-p)}=0.5)\\) を超えた場合、ちょっと怪しいかもしれないです。診断図から問題ないと判断できたら、説明変数の多重鏡線性を確認しましょう。\nまず，説明変数がお互いに相関があるかを確認します。Petal.Width と Sepal.Length の相関は高いですが，他は 0 に近いので相関関係は低いです。\n多重共線性をしっかりと確認したければ，分散拡大係数 (Variance Inflation Factor; VIF) をもとめます。VIF は決定係数 \\((R_i^2)\\) をつかって計算するので，方程式は次の通りです。\\[\n\\text{VIF} = \\frac{1}{1-R_i^2}\n\\]VIF を計算するには，説明変数 \\(x_i\\) を他の説明変数 \\(x_{j \\neq }\\) との線形モデル組み立てて，決定係数を求める必要があります。つまり，先ほどの相関係数の結果をつかうと，car パッケージの vif() 関数の方が便利です。\\(VIF(\\beta_i) > 10\\) であれば，多重共線性の問題は大きいと考えられます。\nこのときの決定係数は \\(1-1/10 = 0.90\\) です。","code":"\nfiris |> \n  select(matches(\"Petal|Sepal\"), .stdresid) |> \n  pivot_longer(matches(\"Petal|Sepal\")) |> \n  ggplot() + \n  geom_point(aes(x = value, y = .stdresid)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  facet_wrap(vars(name), scales = \"free\")\nfiris |> \n  select(.fitted, .stdresid) |> \n  ggplot() + \n  geom_point(aes(x = .fitted, y = .stdresid)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") \nfiris |> \n  select(.fitted, .stdresid) |> \n  ggplot() + \n  geom_point(aes(x = .fitted, y = sqrt(abs(.stdresid)))) \ndof = summary(m1) |> pluck(\"df\")\nthold = qf(0.5, dof[1], dof[2])\nfiris |> \n  mutate(n = 1:n()) |> \n  mutate(above = ifelse(.cooksd > thold, T, F)) |> \n  ggplot() + \n  geom_point(aes(x = n, y = .cooksd, color = above)) + \n  geom_hline(yintercept = thold, color = \"red\", linetype = \"dashed\")\niris %>% select(-Species, - Petal.Length) %>% cor()\n#>              Sepal.Length Sepal.Width Petal.Width\n#> Sepal.Length    1.0000000  -0.1175698   0.8179411\n#> Sepal.Width    -0.1175698   1.0000000  -0.3661259\n#> Petal.Width     0.8179411  -0.3661259   1.0000000\nx1 = lm(Petal.Width ~ Sepal.Length + Sepal.Width, \n        data = iris) %>% summary() %>% pluck(\"r.squared\")\nx2 = lm(Sepal.Length ~ Petal.Width + Sepal.Width, \n        data = iris) %>% summary() %>% pluck(\"r.squared\")\nx3 = lm(Sepal.Width ~ Sepal.Length + Petal.Width, \n        data = iris) %>% summary() %>% pluck(\"r.squared\")\n1 / (1-c(x1,x2,x3)) # VIF\n#> [1] 3.889961 3.415733 1.305515\ncar::vif(m1)\n#>  Petal.Width Sepal.Length  Sepal.Width \n#>     3.889961     3.415733     1.305515"},{"path":"linear-models.html","id":"ガラパゴス諸島における種数の解析","chapter":"8 線形モデル","heading":"8.7 ガラパゴス諸島における種数の解析","text":"iris データの解析に大きな問題がなかったので、あまり参考にならなかったので、\nガラパゴス諸島のデータを解析してみましょう。faraway パッケージの gala を解析します。\ngala にはガラパゴス諸島の生態系に対しての 7 つの変数があります。Species は植物の種数，Endemics は植物の固有種，\nArea は島の平面積 (m2)，Elevation は島の最も高い場所 (m)，Nearest は最も近い島からの距離 (km)，Scruz はサンタクルス島からの距離 (km)，Adjacent は最も近い島の面積 (m2)です。解析する前に、データの可視化をします。説明変数 (explanatory variable)、または予測子 (Predictor) に対する種数の変動は予測子によって変わることがわかります。では、モデルと帰無仮設を決めます。「種数の増減は予測子に依存する」を作業仮説にしたとき，モデルは次のようになります。\\[\nE(\\text{Species}) = b_0 + b_1\\text{Area}+b_2\\text{Elevation}+b_3\\text{Nearest}+b_4\\text{Scruz}+b_5\\text{Adjacent}\n\\]ネイマン＝ピアソン (Neyman-Pearson) の枠組みの中で解析するなら，帰無仮設と対立仮設を建てなければなりません。帰無仮設： \\(b_0 = b_1 = b_2 = b_3 = b_4 = b_5\\)対立仮説： \\(b_0 \\neq b_1 \\neq b_2 \\neq b_3 \\neq b_4 \\neq b_5\\)解析は次の通りです。\nこの度、帰無仮説のモデルもわさわさくみました。相互作用を入れていないが、Type-III SS を求めます。P値は < 0.0001 なので，0.05 より小さいです。帰無仮設は棄却できます。\n帰無仮設を棄却したら，採択するモデル（モデルは採択できるが，仮説は採択できません）の\n診断図を作図して，残差のばらつきや正規性などを確認します。では、HF の診断図を確認しましょう。データ数は少ないが，残差は 0 を中心にしていて左右対称です。\n殆どの残差はQQプロットの直線に沿っています。\nよって，残差の正規性に大きな問題はなさそうです。予測子に対する残差の分布を確認すると，均等に分布していないように見えます。\n例えば Residual vs. Nearest の場合，波状を描いているように見えます。","code":"\ndata(gala, package = \"faraway\")\ngala = gala %>% as_tibble() # tibble に変換\ngala %>% print(n = 3) # 最初の 3 行を表示\n#> # A tibble: 30 × 7\n#>   Species Endemics  Area Elevation Nearest Scruz Adjacent\n#>     <dbl>    <dbl> <dbl>     <dbl>   <dbl> <dbl>    <dbl>\n#> 1      58       23 25.1        346     0.6   0.6     1.84\n#> 2      31       21  1.24       109     0.6  26.3   572.  \n#> 3       3        3  0.21       114     2.8  58.7     0.78\n#> # … with 27 more rows\ngala_out = gala %>% select(-Endemics) %>% gather(Variable, Predictor, -Species)\nggplot(gala_out) + geom_point(aes(x=Predictor, y=Species)) + facet_wrap(\"Variable\", scales = \"free_x\")\nH0 = lm(Species ~ 1, data = gala)  # これが帰無仮設のモデルです。\nHF = lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, data = gala) # これが対立仮説のモデルです。\n# anova(H0, HF) # Type-I SS\nAnova(H0, HF, type = \"III\")   # Type=III SS\n#> Anova Table (Type III tests)\n#> \n#> Response: Species\n#>             Sum Sq Df F value    Pr(>F)    \n#> (Intercept) 217942  1  58.618 6.805e-08 ***\n#> Residuals    89231 24                      \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nfgala = fortify(HF)\np1 = ggplot(fgala) + \n  geom_histogram(aes(x = .stdresid, y = ..density..)) +\n  stat_function(fun = dnorm, color = \"red\") +\n  labs(x = \"Standardized residual\", y = \"\") \np2 = ggplot(fgala) + \n  geom_qq(aes(sample = .stdresid)) +\n  geom_abline(color = \"red\") +\n  labs(x = \"Theoretical Quantile\", y = \"Standardized residual\") \np1 | p2\nfgala |> \n  select(Species, Area, Elevation, Nearest, Scruz, Adjacent, .stdresid) |> \n  pivot_longer(-.stdresid) |> \n  ggplot() + \n  geom_point(aes(x = value, y = .stdresid)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  facet_wrap(vars(name), scales = \"free\")"},{"path":"linear-models.html","id":"モデルの診断図期待値に対する残差のばらつき","chapter":"8 線形モデル","heading":"8.8 モデルの診断図：期待値に対する残差のばらつき","text":"残差のばらつきは期待値と関係性が有るように見えます (Residual vs. Fitted Values Plot)。\nScale–Location Plot では，その関係が明確です。次は異常値を探してみましょう。クックの距離が \\(P(F_{(p, n-p)}=0.5)\\) を超えれば，影響力の高い点だと考えられます。\nこのとき，16番目のデータが明らかに超えています。\\(P(F_{(p, n-p)}=0.5)\\)は自由度 \\(p\\) (パラメータの数) と \\(n-p\\) (データ数からパラメータ数の差) のときのF値の中央値です。いろいろと問題があったので、モデルの改良は必要ですね。\nところが、モデルを改良するときの問題点を意識してください。帰無仮設は棄却できたが，診断図を確認すると多数の問題点がありました。\nこのようなとき，モデルを改良する必要があります。\nただし，ネイマン=ピアソンの帰無仮設検定法のとき，検討するモデルが増えれば増えるほど第１種の誤りを起こす確率も上がります。5 つの予測子（変数）があるので，交互作用ありの一次式のモデルの場合，パラメータの数は32 です。\n検証できるパラメータ数はデータ数に制限されるので，\nパラメータ数とデータ数が等しいときのモデルは飽和モデルとよびます。\nちなみに，32 パラメータのときの第 1 種の誤りを起こす確率は 0.8063 です。\n飽和モデルのとき，分散を推定することができないので，飽和モデルは理論上のモデルです。","code":"\np1 = fgala |> \n  select(.fitted, .stdresid) |> \n  ggplot() + \n  geom_point(aes(x = .fitted, y = .stdresid)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") \np2 = fgala |> \n  select(.fitted, .stdresid) |> \n  ggplot() + \n  geom_point(aes(x = .fitted, y = sqrt(abs(.stdresid)))) \np1 | p2\ndof = summary(HF) |> pluck(\"df\")\nthold = qf(0.5, dof[1], dof[2])\nfgala |> \n  mutate(n = 1:n()) |> \n  mutate(above = ifelse(.cooksd > thold, T, F)) |> \n  ggplot() + \n  geom_point(aes(x = n, y = .cooksd, color = above)) + \n  geom_hline(yintercept = thold, color = \"red\", linetype = \"dashed\")\nwhich(cooks.distance(HF) > thold)\n#> 16 \n#> 16"},{"path":"linear-models.html","id":"モデル構築の考え方","chapter":"8 線形モデル","heading":"8.9 モデル構築の考え方","text":"モデルを設計するときに最も大事なことは：統計解析の仮定を守る科学的・統計学的にありえる説明しやすいシンプル・単純であるモデルの改良点：応答変数を変換する説明変数を変換する誤差項の分布を変えるモデルパラメータを増やす（二次関数・三次関数・交互作用など）","code":""},{"path":"linear-models.html","id":"応答変数の変換","chapter":"8 線形モデル","heading":"8.9.1 応答変数の変換","text":"残差に問題があるとき，応答変数を変換することが一般的に行われています。\n応答変数の変換は，残差を正規分布に従わせるためにします。たとえば個体数はつねに \\(y\\ge 0\\) です。負の個体数は存在しません。\nこのとき，正規分布を仮定したら，0 近辺の値の 95% 信頼区間は負の値をとることもあります。\n実データとの整合性がとれなくなります。\nまたは，0 から 1 の値しか取れないデータのときでも正規性に問題がでます。応答変数が個体数のような正の整数のとき，ログ変換することが多いです。log(x)\nまたは，0 から 1 の比率の様なデータのとき，アークサイン変換をします。 asin(x)","code":""},{"path":"linear-models.html","id":"応答変数変換した解析","chapter":"8 線形モデル","heading":"8.10 応答変数変換した解析","text":"応答変数をログ変換した解析と変換していない解析の結果はつぎの通りです。ログ変換後の分散分析表変換なしの分散分析表変数（パラメータ）に対するP値はが変わりました。\n変換なしの解析に比べて，ログ変換後のElevation と Nearest のP値は下がりましたが，その他のP値は上がりました。ログ変換なしの結果よりちょっと良くなったと思います。説明変数が上昇すると残差のばらつきが減少する傾向があるので，残差のばらつきに問題があります。期待値に対しても，残差のばらつきの均一性が問題です。さらに Scale - Location Plot には明らか傾向があります。16番目のデータに対して，クックの距離は下がったが，\\(PF_{(n, n-p)}=0.5)\\) 以上のままなので，課題としてのこります。","code":"\ngala = gala %>% mutate(logSpecies = log(Species))\nlogHF = lm(logSpecies~ Area + Elevation + Nearest + Scruz + Adjacent, data = gala)\nlogHF %>% Anova(type = \"III\") %>% print(signif.stars = F)\n#> Anova Table (Type III tests)\n#> \n#> Response: logSpecies\n#>             Sum Sq Df F value    Pr(>F)\n#> (Intercept) 53.956  1 48.7039 3.238e-07\n#> Area         3.998  1  3.6088   0.06955\n#> Elevation   26.168  1 23.6211 5.927e-05\n#> Nearest      0.918  1  0.8289   0.37164\n#> Scruz        1.217  1  1.0984   0.30505\n#> Adjacent     7.597  1  6.8575   0.01505\n#> Residuals   26.588 24\nHF %>% Anova(type = \"III\") %>% print(signif.stars = F)\n#> Anova Table (Type III tests)\n#> \n#> Response: Species\n#>             Sum Sq Df F value    Pr(>F)\n#> (Intercept)    506  1  0.1362 0.7153508\n#> Area          4238  1  1.1398 0.2963180\n#> Elevation   131767  1 35.4404 3.823e-06\n#> Nearest          0  1  0.0001 0.9931506\n#> Scruz         4636  1  1.2469 0.2752082\n#> Adjacent     66406  1 17.8609 0.0002971\n#> Residuals    89231 24\nfgala = fortify(logHF)\np1 = ggplot(fgala) + \n  geom_histogram(aes(x = .stdresid, y = ..density..)) +\n  stat_function(fun = dnorm, color = \"red\") +\n  labs(x = \"Standardized residual\", y = \"\") \np2 = ggplot(fgala) + \n  geom_qq(aes(sample = .stdresid)) +\n  geom_abline(color = \"red\") +\n  labs(x = \"Theoretical Quantile\", y = \"Standardized residual\") \np1 | p2\nfgala |> \n  select(logSpecies, Area, Elevation, Nearest, Scruz, Adjacent, .stdresid) |> \n  pivot_longer(-.stdresid) |> \n  ggplot() + \n  geom_point(aes(x = value, y = .stdresid)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  facet_wrap(vars(name), scales = \"free\")\np1 = fgala |> \n  select(.fitted, .stdresid) |> \n  ggplot() + \n  geom_point(aes(x = .fitted, y = .stdresid)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") \np2 = fgala |> \n  select(.fitted, .stdresid) |> \n  ggplot() + \n  geom_point(aes(x = .fitted, y = sqrt(abs(.stdresid)))) \np1 | p2\ndof = summary(logHF) |> pluck(\"df\")\nthold = qf(0.5, dof[1], dof[2])\nfgala |> \n  mutate(n = 1:n()) |> \n  mutate(above = ifelse(.cooksd > thold, T, F)) |> \n  ggplot() + \n  geom_point(aes(x = n, y = .cooksd, color = above)) + \n  geom_hline(yintercept = thold, color = \"red\", linetype = \"dashed\")\nwhich(cooks.distance(logHF) > thold)\n#> 16 \n#> 16"},{"path":"linear-models.html","id":"応答変数以外の解決手法","chapter":"8 線形モデル","heading":"8.10.1 応答変数以外の解決手法","text":"応答変数をログ変換しても問題は解決できませんでした。\n次に検討することは説明変数の変換または、削除です。\nまずは，VIF の高い変数から外してみみます。Elevation の値が一番高かったので，Elevation なしのモデルを再解析します。診断図を確認したら，結果は良くなかったので，Nearest か Scruz も外します。\n種数は島と島の間の距離に依存するかもしれないが，一つの島（Santa Cruz島）との距離の影響は考えにくいので，Scruz を外します。","code":"\ncar::vif(logHF)\n#>      Area Elevation   Nearest     Scruz  Adjacent \n#>  2.928145  3.992545  1.766099  1.675031  1.826403\nlogHF2 = lm(logSpecies~ Area + Nearest + Scruz + Adjacent, data = gala)\ncar::vif(logHF2)\n#>     Area  Nearest    Scruz Adjacent \n#> 1.047496 1.669984 1.658583 1.073529"},{"path":"linear-models.html","id":"単純化したモデル","chapter":"8 線形モデル","heading":"8.11 単純化したモデル","text":"結果として，つぎのモデルを解析することになりました。\\[\nE(\\log(\\text{Species})) = b_0 + b_1\\text{Area}+b_2\\text{Nearest}+b_3\\text{Adjacent}\n\\]Area 以外の変数のP値は 0.05 より高いです。","code":"\nlogSF = lm(logSpecies~ Area + Nearest + Adjacent, data = gala)\nlogSF %>% Anova(type = \"III\") %>% print(signif.stars = F)\n#> Anova Table (Type III tests)\n#> \n#> Response: logSpecies\n#>              Sum Sq Df F value    Pr(>F)\n#> (Intercept) 159.281  1 74.7879 3.983e-09\n#> Area         13.201  1  6.1981   0.01951\n#> Nearest       2.372  1  1.1139   0.30095\n#> Adjacent      0.180  1  0.0847   0.77335\n#> Residuals    55.374 26"},{"path":"linear-models.html","id":"診断図","chapter":"8 線形モデル","heading":"8.12 診断図","text":"残差のばらつきや正規性は良くなったが，クックの距離の問題が残っています。","code":"\nfgala = fortify(logSF)\np1 = ggplot(fgala) + \n  geom_histogram(aes(x = .stdresid, y = ..density..)) +\n  stat_function(fun = dnorm, color = \"red\") +\n  labs(title = \"Histogram\", x = \"Standardized residual\", y = \"\") \np2 = ggplot(fgala) + \n  geom_qq(aes(sample = .stdresid)) +\n  geom_abline(color = \"red\") +\n  labs(title = \"QQ plot\", x = \"Theoretical Quantile\", y = \"Standardized residual\") \np3 = fgala |> \n  select(logSpecies, Area, Nearest, Adjacent, .stdresid) |> \n  pivot_longer(-.stdresid) |> \n  ggplot() + \n  geom_point(aes(x = value, y = .stdresid)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Resid. vs. predictor\", x = \"Predictor value\", y = \"Standardized residual\") +\n  facet_wrap(vars(name), scales = \"free\")\np4 = fgala |> \n  select(.fitted, .stdresid) |> \n  ggplot() + \n  geom_point(aes(x = .fitted, y = .stdresid)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\")  +\n  labs(title = \"Resid. vs. fit\", x = \"Fitted value\", y = \"Standardized residual\")\np5 = fgala |> \n  select(.fitted, .stdresid) |> \n  ggplot() + \n  geom_point(aes(x = .fitted, y = sqrt(abs(.stdresid))))  +\n  labs(title = \"Scale - location\", x = \"Fitted value\", y = \"Standardized residual\")\ndof = summary(logSF) |> pluck(\"df\")\nthold = qf(0.5, dof[1], dof[2])\np6 = fgala |> \n  mutate(n = 1:n()) |> \n  mutate(above = ifelse(.cooksd > thold, T, F)) |> \n  ggplot() + \n  geom_point(aes(x = n, y = .cooksd, color = above)) + \n  geom_hline(yintercept = thold, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Cook's distance\", x = \"Index\", y = \"Cook's distance\")\n\np1 + p2 + p3 + p4 + p5 + p6"},{"path":"linear-models.html","id":"説明変数も変換する","chapter":"8 線形モデル","heading":"8.13 説明変数も変換する","text":"こんどは，説明変数も変換します。\\[\nE(\\log(\\text{Species})) = b_0 + b_1\\log(\\text{Area})+b_2\\text{Nearest}+b_3\\log(\\text{Adjacent})\n\\]残差のばらつき，正規性，クックの距離の問題は解決できました。結果帰無仮設の有意性検定によると，logArea と Nearest の効果は有意ですが，logAdjacent の効果は有意じゃない。ところが，この結果まで導くには，5 種類のモデルを検証しました。\\(\\alpha_\\text{fwer}\\) は\n\\(1 - (1-0.05)^5 = 0.2262\\) ですので，\\(\\alpha_\\text{fwer}=0.05\\) にしたければ，\\(\\alpha = 0.0102\\) に設定しなければなりません。このとき，logArea 以外の要因の効果は有意ではありません。","code":"\ngala = gala %>% mutate(logSpecies = log(Species), logArea = log(Area), logAdjacent = log(Adjacent))\nlogSF2 = lm(logSpecies~ logArea + Nearest + logAdjacent, data = gala)\nlogSF2 %>% Anova(type = \"III\") %>% print(signif.stars = F)\n#> Anova Table (Type III tests)\n#> \n#> Response: logSpecies\n#>              Sum Sq Df  F value    Pr(>F)\n#> (Intercept) 151.442  1 239.3574 1.247e-14\n#> logArea      52.429  1  82.8653 1.445e-09\n#> Nearest       0.618  1   0.9764    0.3322\n#> logAdjacent   0.159  1   0.2512    0.6204\n#> Residuals    16.450 26\nfgala = fortify(logSF2)\np1 = ggplot(fgala) + \n  geom_histogram(aes(x = .stdresid, y = ..density..)) +\n  stat_function(fun = dnorm, color = \"red\") +\n  labs(title = \"Histogram\", x = \"Standardized residual\", y = \"\") \np2 = ggplot(fgala) + \n  geom_qq(aes(sample = .stdresid)) +\n  geom_abline(color = \"red\") +\n  labs(title = \"QQ plot\", x = \"Theoretical Quantile\", y = \"Standardized residual\") \np3 = fgala |> \n  select(logSpecies, logArea, Nearest, logAdjacent, .stdresid) |> \n  pivot_longer(-.stdresid) |> \n  ggplot() + \n  geom_point(aes(x = value, y = .stdresid)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") + \n  labs(title = \"Resid. vs. predictor\", x = \"Predictor value\", y = \"Standardized residual\") +\n  facet_wrap(vars(name), scales = \"free\")\np4 = fgala |> \n  select(.fitted, .stdresid) |> \n  ggplot() + \n  geom_point(aes(x = .fitted, y = .stdresid)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\")  +\n  labs(title = \"Resid. vs. fit\", x = \"Fitted value\", y = \"Standardized residual\")\np5 = fgala |> \n  select(.fitted, .stdresid) |> \n  ggplot() + \n  geom_point(aes(x = .fitted, y = sqrt(abs(.stdresid))))  +\n  labs(title = \"Scale - location\", x = \"Fitted value\", y = \"Standardized residual\")\ndof = summary(logSF) |> pluck(\"df\")\nthold = qf(0.5, dof[1], dof[2])\np6 = fgala |> \n  mutate(n = 1:n()) |> \n  mutate(above = ifelse(.cooksd > thold, T, F)) |> \n  ggplot() + \n  geom_point(aes(x = n, y = .cooksd, color = above)) + \n  geom_hline(yintercept = thold, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Cook's distance\", x = \"Index\", y = \"Cook's distance\")\n\np1 + p2 + p3 + p4 + p5 + p6\nlogSF2 %>% Anova(type = \"III\") %>% print(signif.stars = F)\n#> Anova Table (Type III tests)\n#> \n#> Response: logSpecies\n#>              Sum Sq Df  F value    Pr(>F)\n#> (Intercept) 151.442  1 239.3574 1.247e-14\n#> logArea      52.429  1  82.8653 1.445e-09\n#> Nearest       0.618  1   0.9764    0.3322\n#> logAdjacent   0.159  1   0.2512    0.6204\n#> Residuals    16.450 26"},{"path":"linear-models.html","id":"一般化線形モデル","chapter":"8 線形モデル","heading":"8.14 一般化線形モデル","text":"一般化線形モデル (GLM) は今までの線型モデルと同じように，線型結合した変数から成り立っています。さらに，正規分布を仮定した線型モデルのとき，予測残差は Residuals (残差) とよびましたが，GLMの予測残差は Deviance (尤離度・逸脱度・デビアンス) とよびます。正規分布以外の指数型分布族にぞくする分布も使えます。一般化線形モデルに 3 つの成分が存在します。\n誤差構造またはランダム成分 (random component)\n線型予測子 (linear predictor)または系統成分(systematic component)\n連結関数またはリンク関数 (link function)\n誤差構造またはランダム成分 (random component)線型予測子 (linear predictor)または系統成分(systematic component)連結関数またはリンク関数 (link function)","code":""},{"path":"linear-models.html","id":"指数型分布族誤差項","chapter":"8 線形モデル","heading":"8.15 指数型分布族・誤差項","text":"\\[\nf(y|\\theta, \\phi) = \\exp\\left(\\frac {y\\theta - b(\\theta)} {(\\phi)} + c(y, \\phi)\\right)\n\\]\\(\\theta\\) は正準パラメータ (canonical parameter)，\\(\\phi\\) はばらつきのパラメータ (dispersion parameter, 分散パラメータ)，\\((\\cdot)\\), \\(b(\\cdot)\\), \\(c(\\cdot)\\) は存知の関数です。さらに，平均値は \\(\\mu = E(y) = b'(\\theta)\\)，分散は \\(var(y) = (\\phi) b''(\\theta)\\) です。つまり，平均値は \\(\\theta\\) の関数できまるが，分散は二つの関数の積です。\\((\\phi) = \\phi/w\\) であれば，\\(v(y) = \\phi b''(\\theta)/w\\) であり，分散関数とよびます。正規分布の場合，分散と平均値は独立しているのと，\\(\\phi=1\\) なので，\\(var(y) = 1/w\\) です。\\(w\\) は存知の重みです。","code":""},{"path":"linear-models.html","id":"線形予測子または系統成分","chapter":"8 線形モデル","heading":"8.15.1 線形予測子または系統成分","text":"応答変数における予測子の影響は次のように書けます。\\[\n\\eta = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_n x_n\n\\]この線形予測子は一般線形モデルと同じ用に構築します。","code":""},{"path":"linear-models.html","id":"連結関数-リンク関数","chapter":"8 線形モデル","heading":"8.15.2 連結関数 (リンク関数)","text":"応答変数の期待値と予測子の関係はリンク関数を通して関係づけます。\\[\ng(\\mu) = \\eta\n\\]リンク関数は単調な微分可能な関数です。","code":""},{"path":"linear-models.html","id":"一般化線形モデルの推定方法","chapter":"8 線形モデル","heading":"8.15.3 一般化線形モデルの推定方法","text":"一般化線形モデルのパラメータ推定には最尤推定 (maximum likelihood estimation; maximum likelihood method; 最尤推定法) を用います。\\(y_1, \\dots, y_n\\) は \\(n\\) 数の独立な確率変数とします。さらに，\\(y_i\\) は同じパラメータの正規分布に従うとしたら，同時確立分布はつぎの通りです。\\[\nP(y_1, \\dots, y_n | \\mu, \\sigma^2) = \\prod_{=1}^{N} \\exp\\left(y_i\\frac{\\mu}{\\sigma^2} -\\frac{\\mu^2}{2\\sigma^2} - \\frac{1}{2}\\log(2\\pi\\sigma^2) -\\frac{y_i^2}{2\\sigma^2}\\right) = \\mathcal{L}(\\mu, \\sigma^2 | y_1, \\dots, y_n)\n\\]ここの \\(\\mathcal{L}(\\cdots)\\) が尤度関数 (log-likelihood function)。両側の自然対数を求めれば，対数尤度関数 (log-likelihood function) に導きます。\\[\n\\log(\\mathcal{L}(\\mu,\\sigma^2)) = - \\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{=1}^{N}(y_i - \\mu)^2\n\\]この対数尤度関数が最大または負の対数尤度関数 (negative log-likelihood function) が最小になるパラメータをさがすことが目的です。","code":""},{"path":"linear-models.html","id":"一般的な誤差項","chapter":"8 線形モデル","heading":"8.15.4 一般的な誤差項","text":"連続型確率分布正規分布ガンマ分布ベータ分布指数分布離散型確率分布ポアソン分布二項分布負の二項分布categorical 分布他にありますが，上記の分布が一般的につかわれています。","code":""},{"path":"linear-models.html","id":"ガラパゴス諸島のデータのglm解析例","chapter":"8 線形モデル","heading":"8.16 ガラパゴス諸島のデータのGLM解析例","text":"まず，帰無仮設の気無モデル（ヌルモデル）を組み立てます。\\[\n\\begin{aligned}\n\\text{Species} &\\sim Pois(\\mu)\\\\\nE(\\text{Species}) &= \\mu \\\\\nE(\\text{Species}) &= \\exp(\\eta) \\\\\n\\eta &= b_0  \\\\\n\\end{aligned}\n\\]\\(\\eta = b_0\\) は切片のみのモデルです。","code":""},{"path":"linear-models.html","id":"ポアソンglm解析の出力","chapter":"8 線形モデル","heading":"8.16.1 ポアソンGLM解析の出力","text":"パラメータの推定量の表の後に，(Dispersion parameter poisson family taken 1) の記述があります。\nこれは，ポアソン分布のばらつきのパラメータ \\((\\phi)\\) を 1 に設定したと意味します。\nヌルモデルを当てはめたので，ヌル逸脱度 (Null Deviance) と残渣逸脱度 (Residual Deviance) は同じです。","code":"\nH0_poisson = glm(Species ~ 1, data = gala, family = poisson(link = \"log\"))\nH0_poisson %>% summary() %>% print(signif.stars = F)\n#> \n#> Call:\n#> glm(formula = Species ~ 1, family = poisson(link = \"log\"), data = gala)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -12.307   -9.782   -5.200    1.142   27.351  \n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)\n#> (Intercept)  4.44539    0.01978   224.8   <2e-16\n#> \n#> (Dispersion parameter for poisson family taken to be 1)\n#> \n#>     Null deviance: 3510.7  on 29  degrees of freedom\n#> Residual deviance: 3510.7  on 29  degrees of freedom\n#> AIC: 3673.6\n#> \n#> Number of Fisher Scoring iterations: 6"},{"path":"linear-models.html","id":"対立モデル","chapter":"8 線形モデル","heading":"8.16.2 対立モデル","text":"次は対立モデルです。\\[\n\\begin{aligned}\n\\text{Species} &\\sim Pois(\\mu)\\\\\nE(\\text{Species}) &= \\mu \\\\\nE(\\text{Species}) &= \\exp(\\eta) \\\\\n\\eta &= b_0 + b_1\\text{Area}+b_2\\text{Elevation}+b_3\\text{Nearest}+b_4\\text{Scruz}+b_5\\text{Adjacent}\\\\\n\\end{aligned}\n\\]","code":""},{"path":"linear-models.html","id":"対立モデルの出力","chapter":"8 線形モデル","heading":"8.16.3 対立モデルの出力","text":"最初に当てはめた線形モデルの結果と全く違います。","code":"\nHF_poisson = glm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, \n                 data = gala, family = poisson(link = \"log\"))\nHF_poisson %>% summary() %>% print(signif.stars = F)\n#> \n#> Call:\n#> glm(formula = Species ~ Area + Elevation + Nearest + Scruz + \n#>     Adjacent, family = poisson(link = \"log\"), data = gala)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -8.2752  -4.4966  -0.9443   1.9168  10.1849  \n#> \n#> Coefficients:\n#>               Estimate Std. Error z value Pr(>|z|)\n#> (Intercept)  3.155e+00  5.175e-02  60.963  < 2e-16\n#> Area        -5.799e-04  2.627e-05 -22.074  < 2e-16\n#> Elevation    3.541e-03  8.741e-05  40.507  < 2e-16\n#> Nearest      8.826e-03  1.821e-03   4.846 1.26e-06\n#> Scruz       -5.709e-03  6.256e-04  -9.126  < 2e-16\n#> Adjacent    -6.630e-04  2.933e-05 -22.608  < 2e-16\n#> \n#> (Dispersion parameter for poisson family taken to be 1)\n#> \n#>     Null deviance: 3510.73  on 29  degrees of freedom\n#> Residual deviance:  716.85  on 24  degrees of freedom\n#> AIC: 889.68\n#> \n#> Number of Fisher Scoring iterations: 5"},{"path":"linear-models.html","id":"尤度比検定とaicにおけるモデル比較","chapter":"8 線形モデル","heading":"8.17 尤度比検定とAICにおけるモデル比較","text":"今までは，正規分布を仮定した解析手法をつかっていたので，比較はF検定で行いました。\nポアソンGLMののとき，尤度比検定で，帰無モデル（ヌルモデル）と対立モデルを比較します。AICでも比較できます。尤度比検定のとき，NHSTの検証になるので，帰無仮設を棄却することになります。\nAICのとき，モデル選択するので，AICの最も低いモデルを採択します。\n正規分布やガンマ分布などの他の分布をつかったときでも，尤度比検定やAICを用いてもいいです。\nもちろん，正規分布の場合，F検定でも問題はありません (このとき，test = \"F\")。","code":"\nanova(H0_poisson, HF_poisson, test = \"LRT\")\n#> Analysis of Deviance Table\n#> \n#> Model 1: Species ~ 1\n#> Model 2: Species ~ Area + Elevation + Nearest + Scruz + Adjacent\n#>   Resid. Df Resid. Dev Df Deviance  Pr(>Chi)    \n#> 1        29     3510.7                          \n#> 2        24      716.8  5   2793.9 < 2.2e-16 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAIC(H0_poisson, HF_poisson)\n#>            df       AIC\n#> H0_poisson  1 3673.5596\n#> HF_poisson  6  889.6767"},{"path":"linear-models.html","id":"ポアソン分布のときに必ず確認する値","chapter":"8 線形モデル","heading":"8.17.1 ポアソン分布のときに必ず確認する値","text":"ポアソン分布のGLMを実施したあと，必ず確認しなければならい値は過分散 (-dispersion)です。\n過分散があるとき，パラメータの標準誤差，信頼区間，モデルの予測値は誤っています。\n過分散をパラメータとして推定するモデル（正規分布やガンマ分布など）の場合は，過分散の推定を無視できます。感覚的に考えると，モデル結果の Residual Deviance の値が degrees freedom の値と同じであれば，過分散は存在しません。\nただし，どの手法をつかっても，平均値は 5 以上じゃなければなりません。\n上記のP値はとても小さいので，過分散が存在すると考えられます。\nまたは，Residual Deviance (761.98) は自由度 (24) より大きいので，検定をしなくても過分散の問題は明らかです\n過分散はデータがポアソン分布に従わないときとモデル変数が不十分なときに起こります。過小分散も存在しますが，過分散ほどの問題ではありません。","code":"\n## https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html\noverdisp_fun = function(model) {\n    rdf = df.residual(model)\n    rp = residuals(model,type=\"pearson\")\n    Pearson.chisq = sum(rp^2)\n    prat = Pearson.chisq/rdf\n    pval = pchisq(Pearson.chisq, df=rdf, lower.tail=FALSE)\n    mu = mean(predict(model, type = \"response\"))\n    c(mu=mu, chisq=Pearson.chisq,ratio=prat,rdf=rdf,p=pval)\n}\noverdisp_fun(HF_poisson)\n#>            mu         chisq         ratio           rdf \n#>  8.523333e+01  7.619792e+02  3.174914e+01  2.400000e+01 \n#>             p \n#> 2.187190e-145"},{"path":"linear-models.html","id":"モデルの改良","chapter":"8 線形モデル","heading":"8.17.2 モデルの改良","text":"過分散の問題があったので，診断図を確認するよりも，他のモデルを当てはめてみます。\nここでは説明変数のログ変換したモデルを解析します。Residual deviance と degrees freedom を確認すると，\\(371.78\\; /\\; 26 > 1\\) なので，過分散が残っています。\nガラパゴス諸島の種数のデータに対して，どのように変換を変換しても，過分散は \\(>1\\) です。","code":"\nH2_poisson = glm(Species ~ logArea + Nearest + logAdjacent, \n                 data = gala, family = poisson(link = \"log\"))\nH2_poisson %>% summary() %>% print(signif.stars = F)\n#> \n#> Call:\n#> glm(formula = Species ~ logArea + Nearest + logAdjacent, family = poisson(link = \"log\"), \n#>     data = gala)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -5.6281  -3.4817  -0.3344   2.7419   8.2056  \n#> \n#> Coefficients:\n#>              Estimate Std. Error z value Pr(>|z|)\n#> (Intercept)  3.361898   0.046784  71.860  < 2e-16\n#> logArea      0.371771   0.007961  46.698  < 2e-16\n#> Nearest     -0.006244   0.001312  -4.759 1.94e-06\n#> logAdjacent -0.097529   0.006106 -15.974  < 2e-16\n#> \n#> (Dispersion parameter for poisson family taken to be 1)\n#> \n#>     Null deviance: 3510.73  on 29  degrees of freedom\n#> Residual deviance:  371.78  on 26  degrees of freedom\n#> AIC: 540.61\n#> \n#> Number of Fisher Scoring iterations: 5"},{"path":"linear-models.html","id":"過分散をパラメータとして扱う","chapter":"8 線形モデル","heading":"8.17.3 過分散をパラメータとして扱う","text":"何をしても過分散が残るとき，過分散をパラメータ化することができます。\nポアソン分布で説明できなかった分散を負の二項分布で説明できるかもしれません。【重要】MASS パッケージのselect() 関数は tidyverse の select() 関数と同じ名前なので、\nあとに読み込んだパッケージの関すが優先されます。\nつまり、MASS パッケージは tidyverse の先に読み込みましょう。\nあるいは、不要になったらディタッチ (detach) しましょう。誤差項の分布は負の二項分布にしたので，分散は \\(Variance = \\mu + \\frac{\\mu}{\\theta}\\) として推定されます。\nこのモデルの\\(\\theta\\) は 2.8371 です。\nこのモデルの解析を進めたいので，次は診断図の確認です。","code":"\ndetach(name = package:MASS)\nH2_negbinom = MASS::glm.nb(Species ~ logArea + Nearest + logAdjacent, data = gala, link = \"log\")\nH2_negbinom %>% summary() %>% print(signif.stars = F)\n#> \n#> Call:\n#> MASS::glm.nb(formula = Species ~ logArea + Nearest + logAdjacent, \n#>     data = gala, link = \"log\", init.theta = 2.83714993)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -2.2174  -0.9672  -0.3006   0.5165   2.0350  \n#> \n#> Coefficients:\n#>              Estimate Std. Error z value Pr(>|z|)\n#> (Intercept)  3.369739   0.157235  21.431   <2e-16\n#> logArea      0.364309   0.035028  10.400   <2e-16\n#> Nearest     -0.012389   0.008211  -1.509    0.131\n#> logAdjacent -0.034399   0.036005  -0.955    0.339\n#> \n#> (Dispersion parameter for Negative Binomial(2.8371) family taken to be 1)\n#> \n#>     Null deviance: 144.45  on 29  degrees of freedom\n#> Residual deviance:  32.82  on 26  degrees of freedom\n#> AIC: 284.88\n#> \n#> Number of Fisher Scoring iterations: 1\n#> \n#> \n#>               Theta:  2.837 \n#>           Std. Err.:  0.830 \n#> \n#>  2 x log-likelihood:  -274.884"},{"path":"linear-models.html","id":"負の二項分布glmの診断図","chapter":"8 線形モデル","heading":"8.17.4 負の二項分布GLMの診断図","text":"診断図はよくつくるので、診断図用の関数を定義します。","code":""},{"path":"linear-models.html","id":"診断図関数残差のヒストグラム","chapter":"8 線形モデル","heading":"8.17.5 診断図関数：残差のヒストグラム","text":"","code":"\n# 残差のヒストグラム\ngg_resid_hist = function(fitted.model) {\n  require(tidyvers)\n  require(statmod)\n  data = fortify(fitted.model) %>% as_tibble()\n  if(class(fitted.model)[1] != \"lm\") {\n    data = data %>% mutate(qresid = statmod::qresiduals(fitted.model))\n    data = data %>% mutate(residual = qresid)\n    xlabel = \"Randomized Quantile Residuals\"\n  } else {\n    data = data %>% mutate(residual = .resid)\n    xlabel = \"Standardized Residuals\"\n  }\n  ggplot(data) +\n    geom_histogram(aes(x = residual)) +\n    labs(x = xlabel) + ggtitle(\"Histogram of Residuals\")\n}"},{"path":"linear-models.html","id":"診断図関数残差のqqプロット","chapter":"8 線形モデル","heading":"8.17.6 診断図関数：残差のQQプロット","text":"","code":"\n# 残差のQQプロット\ngg_qq = function(fitted.model) {\n  require(tidyvers)\n  require(statmod)\n  data = fortify(fitted.model) %>% as_tibble()\n   if(class(fitted.model)[1] != \"lm\") {\n    data = data %>% mutate(qresid = statmod::qresiduals(fitted.model))\n    data = data %>% mutate(residual = qresid)\n    ylabel = \"Randomized Quantile Residuals\"\n  } else {\n    data = data %>% mutate(residual = .stdresid)\n    ylabel = \"Standardized Residuals\"\n  }\n  ggplot(data) +\n    geom_qq(aes(sample =residual)) +\n    geom_abline(color = \"red\") +\n    labs(x = \"Theoretical Quantile\", y = ylabel) +\n    ggtitle(\"Normal-QQ Plot\")\n}"},{"path":"linear-models.html","id":"診断図関数変数に対する残差のばらつき","chapter":"8 線形モデル","heading":"8.17.7 診断図関数：変数に対する残差のばらつき","text":"","code":"\n# 変数に対する残差のプロット\ngg_resX = function(fitted.model, ncol=NULL, ...) {\n  require(tidyvers)\n  require(statmod)\n  residlab = function(string) {\n    sprintf(\"Residuals vs. %s\", string)\n  }\n  data = fortify(fitted.model) %>% as_tibble()\n  varnames = as.character(formula(fitted.model)) %>% pluck(3)\n  varnames = str_split(varnames, \" \\\\+ \") %>% pluck(1)\n  if(class(fitted.model)[1] != \"lm\") {\n    data = data %>% mutate(qresid = statmod::qresiduals(fitted.model))\n    data = data %>% mutate(residual = qresid)\n    ylabel = \"Randomized Quantile Residuals\"\n  } else {\n    data = data %>% mutate(residual = .resid)\n    ylabel = \"Standardized Residuals\"\n  }\n  varnames = names(data)[names(data) %in% varnames]\n  data = data %>% dplyr::select(varnames, residual) %>% gather(var, value, varnames)\n  ggplot(data) + \n    geom_point(aes(x = value, y = residual)) +\n    geom_hline(yintercept=0, color = \"red\", linetype = \"dashed\") +\n    labs(x=\"Value\", y = ylabel) +\n    facet_wrap(\"var\", labeller=labeller(var=residlab), scales = \"free_x\",\n               ncol = ncol)\n}"},{"path":"linear-models.html","id":"診断図関数期待値に対する残差のばらつき","chapter":"8 線形モデル","heading":"8.17.8 診断図関数：期待値に対する残差のばらつき","text":"","code":"\n# 期待値に対する残差のプロット\ngg_resfitted = function(fitted.model) {\n  require(tidyvers)\n  require(statmod)\n  data = fortify(fitted.model) %>% as_tibble()\n  if(class(fitted.model)[1] != \"lm\") {\n    data = data %>% mutate(qresid = statmod::qresiduals(fitted.model))\n    data = data %>% mutate(residual = qresid)\n    ylabel = \"Randomized Quantile Residuals\"\n  } else {\n    data = data %>% mutate(residual = .resid)\n    ylabel = \"Standardized Residuals\"\n  }\n  ggplot(data) + \n    geom_point(aes(x = .fitted, y = residual)) +\n    geom_hline(yintercept=0, color = \"red\", linetype = \"dashed\") +\n    labs(x=\"Fitted Values\", y = ylabel) +\n    ggtitle(\"Residuals vs. Fitted Values\")\n}"},{"path":"linear-models.html","id":"診断図関数スケールロケーションプロット","chapter":"8 線形モデル","heading":"8.17.9 診断図関数：スケール・ロケーションプロット","text":"","code":"\n# スケール・ロケーションプロット\ngg_scalelocation = function(fitted.model) {\n  require(tidyvers)\n  require(statmod)\n  data = fortify(fitted.model) %>% as_tibble()\n  if(class(fitted.model)[1] != \"lm\") {\n    data = data %>% mutate(qresid = statmod::qresiduals(fitted.model))\n    data = data %>% mutate(residual = qresid)\n    ylabel = expression(sqrt(\"|RQR|\"))\n  } else {\n    data = data %>% mutate(residual = .resid)\n    ylabel = expression(sqrt(\"|Standardized Residuals|\"))\n  }\n  ggplot(data) + \n    geom_point(aes(x = .fitted, y = sqrt(abs(residual)))) +\n    geom_smooth(aes(x = .fitted, y = sqrt(abs(residual))), \n                se = F, color = \"red\", linetype = \"dashed\") +\n    labs(x=\"Fitted Values\", y = ylabel) +\n    ggtitle(\"Scale - Location Plot\")\n}"},{"path":"linear-models.html","id":"診断図関数クックの距離","chapter":"8 線形モデル","heading":"8.17.10 診断図関数：クックの距離","text":"残差の確認は線形モデルと同じようにしますが，解析に使う残差は ダン=スミス残差 (Dunn-Smyth Residuals; randomized quantile residuals) です。\nとくに，ポアソンGLMと疑似ポアソンGLMのダン=スミス残差を求めます。\nダン=スミス残差は残差の累積分布関数を用いて残差のランダム化を行います。\nランダム化した残差は正規分布に近似するので，QQプロットで使用した分布のよさを評価できます。\n]ランダム化残差が直線に沿っていたら，分布に問題がないと示唆します。\n左側はポアソン分布GLMのランダム化残差のQQプロットです。右側は負の二項分布GLMのランダム化残差のQQプロットです。","code":"\n# クックの距離\ngg_cooksd = function(fitted.model) {\n  require(tidyvers)\n  require(statmod)\n  data = fortify(fitted.model) %>% as_tibble()\n  data = data %>% mutate(n = seq_along(.cooksd))\n  dof = summary(fitted.model) %>% pluck(\"df\")\n  thold = qf(0.5, dof[1], dof[2])\n  data2 = data %>% mutate(above = ifelse(.cooksd > thold, T, F)) %>% filter(above)\n  ggplot(data) + \n    geom_point(aes(x = n, y = .cooksd)) +\n    geom_segment(aes(x = n, y = 0, xend = n, yend = .cooksd)) +\n    geom_hline(yintercept=thold, color = \"red\", linetype = \"dashed\") +\n    geom_text(aes(x = n, y = .cooksd, label = n), data = data2, nudge_x=3, color = \"red\") +\n    labs(x=\"Sample\", y = \"Cook's Distance\") +\n    ggtitle(\"Cook's Distance Plot\")\n}\np1 = gg_qq(H2_poisson)\np2 = gg_qq(H2_negbinom)\np1 | p2"},{"path":"linear-models.html","id":"η-に対するランダム化残差のばらつき","chapter":"8 線形モデル","heading":"8.17.11 η に対するランダム化残差のばらつき","text":"ランダム化残差と \\(\\eta\\) の間に明確な関係はありません。\n\\(\\sqrt{\\left(|\\text{Randomized Quantile Residuals}~\\right)}\\) は若干山形な形をしています。\nところが，期待値の両端のデータが点線を引っ張っているように見えるので，\n明確ではない。全てクックの距離は\\(P(F_{(n, n-p)}=0.5)\\) より低いので，モデルを引っ張る点はありません。","code":"\np1 = gg_resfitted(H2_negbinom)\np2 = gg_scalelocation(H2_negbinom)\np1 | p2\ngg_cooksd(H2_negbinom)"},{"path":"linear-models.html","id":"結果","chapter":"8 線形モデル","heading":"8.18 結果","text":"診断図と過分散の確認から，次のモデルにたどり着きました。\\[\n\\begin{aligned}\n\\text{Species} &\\sim NegBin(\\mu)\\\\\nE(\\text{Species}) &= \\mu \\\\\nE(\\text{Species}) &= \\exp(\\eta) \\\\\nVariance &= \\mu + \\frac{\\mu}{\\theta} \\\\\n\\eta &= b_0 + b_1\\log(\\text{Area})+b_3\\text{Nearest}+b_5\\log(\\text{Adjacent})\\\\\n\\end{aligned}\n\\]ポアソン分布GLMのAICが高いので，負の二項分布GLMを採択します。","code":"\nAIC(H2_poisson, H2_negbinom)\n#>             df      AIC\n#> H2_poisson   4 540.6134\n#> H2_negbinom  5 284.8838"},{"path":"linear-models.html","id":"負の二項分布の結果係数表","chapter":"8 線形モデル","heading":"8.18.1 負の二項分布の結果（係数表）","text":"係数表だけ出力しました。切片と logArea の効果ははっきりしています (P < 0.0001)。ところが，Nearest と logAdjacent のP値は \\(>0.05\\) でした。\nさらに，変数をへらしてもAICは大きく変わりません。","code":"\nH2_negbinom %>% summary() %>% coefficients() %>% print(digits = 3)\n#>             Estimate Std. Error z value  Pr(>|z|)\n#> (Intercept)   3.3697    0.15723  21.431 6.83e-102\n#> logArea       0.3643    0.03503  10.400  2.47e-25\n#> Nearest      -0.0124    0.00821  -1.509  1.31e-01\n#> logAdjacent  -0.0344    0.03600  -0.955  3.39e-01\n# H2_negbinom %>% summary() # 全情報の出力"},{"path":"linear-models.html","id":"aic","chapter":"8 線形モデル","heading":"8.18.2 AIC","text":"最も小さいAICは H2_negbinom2 になりましたがAICがの差が 0 ~ 7 の範囲に入るので，どのモデルでもいいです。\nこのとき，尤度比検定もした方がいい。","code":"\nH2_negbinom2 = MASS::glm.nb(Species ~ logArea + Nearest , data = gala, link = \"log\")\nH2_negbinom3 = MASS::glm.nb(Species ~ logArea +  logAdjacent, data = gala, link = \"log\")\nH2_negbinom4 = MASS::glm.nb(Species ~ logArea , data = gala, link = \"log\")\nAIC(H2_negbinom, H2_negbinom2, H2_negbinom3, H2_negbinom4) |> as_tibble(rownames = \"model\") |> arrange(AIC)\n#> # A tibble: 4 × 3\n#>   model           df   AIC\n#>   <chr>        <dbl> <dbl>\n#> 1 H2_negbinom2     4  284.\n#> 2 H2_negbinom4     3  284.\n#> 3 H2_negbinom      5  285.\n#> 4 H2_negbinom3     4  285."},{"path":"linear-models.html","id":"尤度比検定","chapter":"8 線形モデル","heading":"8.18.3 尤度比検定","text":"自由度の高い順からペア毎の尤度比検定を行っています。\\(E(\\text{Species})\\sim b_0 + b_1\\text{logArea}\\) のモデルでいいかもしれないです。","code":"\nanova(H2_negbinom, H2_negbinom2, H2_negbinom3, H2_negbinom4, test = \"LRT\")\n#> Likelihood ratio tests of Negative Binomial Models\n#> \n#> Response: Species\n#>                             Model    theta Resid. df\n#> 1                         logArea 2.533343        28\n#> 2               logArea + Nearest 2.730433        27\n#> 3           logArea + logAdjacent 2.619569        27\n#> 4 logArea + Nearest + logAdjacent 2.837150        26\n#>      2 x log-lik.   Test    df  LR stat.   Pr(Chi)\n#> 1       -277.7721                                 \n#> 2       -275.7711 1 vs 2     1  2.000994 0.1571961\n#> 3       -276.9859 2 vs 3     0 -1.214826 1.0000000\n#> 4       -274.8838 3 vs 4     1  2.102117 0.1470954"},{"path":"linear-models.html","id":"モデル診断図","chapter":"8 線形モデル","heading":"8.18.4 モデル診断図","text":"モデル診断図を確認したら，残差の問題はないですので，\\(E(\\text{Species})\\sim b_0 + b_1\\text{logArea}\\) のモデルを採択することになりました。","code":"\np1 = gg_qq(H2_negbinom4)\np2 = gg_resX(H2_negbinom4, ncol = 2)\np3 = gg_resfitted(H2_negbinom4)\np4 = gg_cooksd(H2_negbinom4)\np1 + p2 + p3 + p4"},{"path":"linear-models.html","id":"採択したモデル","chapter":"8 線形モデル","heading":"8.18.5 採択したモデル","text":"","code":"\nH2_negbinom4 %>% summary()\n#> \n#> Call:\n#> MASS::glm.nb(formula = Species ~ logArea, data = gala, link = \"log\", \n#>     init.theta = 2.533342912)\n#> \n#> Deviance Residuals: \n#>     Min       1Q   Median       3Q      Max  \n#> -1.9737  -0.9216  -0.2155   0.5056   1.8969  \n#> \n#> Coefficients:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  3.22480    0.13669  23.592   <2e-16 ***\n#> logArea      0.34989    0.03543   9.877   <2e-16 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> (Dispersion parameter for Negative Binomial(2.5333) family taken to be 1)\n#> \n#>     Null deviance: 130.161  on 29  degrees of freedom\n#> Residual deviance:  32.604  on 28  degrees of freedom\n#> AIC: 283.77\n#> \n#> Number of Fisher Scoring iterations: 1\n#> \n#> \n#>               Theta:  2.533 \n#>           Std. Err.:  0.719 \n#> \n#>  2 x log-likelihood:  -277.772\nndata = gala |> expand(logArea = seq(min(logArea), max(logArea), length = 21)) \nndata = bind_cols(ndata, predict(H2_negbinom4, newdata = ndata, type = \"link\", se = T) |> as_tibble())\nndata = ndata |> \n  mutate(expect = exp(fit),\n         lower = exp(fit - se.fit),\n         upper = exp(fit + se.fit))\ngala %>% \n  ggplot(aes(x = logArea)) +\n  geom_ribbon(aes(ymin = lower, ymax = upper), data = ndata, alpha = 0.5) +\n  geom_point(aes(y = Species)) +\n  geom_line(aes(y = expect), data = ndata)"},{"path":"gam.html","id":"gam","chapter":"9 一般化加法モデル","heading":"9 一般化加法モデル","text":"そのうちあげます。","code":""},{"path":"model-selection.html","id":"model-selection","chapter":"10 モデル選択","heading":"10 モデル選択","text":"","code":""},{"path":"model-selection.html","id":"必要なパッケージ-8","chapter":"10 モデル選択","heading":"10.1 必要なパッケージ","text":"","code":"\nlibrary(tidyverse)"},{"path":"model-selection.html","id":"aic-akaikes-information-criterion","chapter":"10 モデル選択","heading":"10.2 AIC: Akaike’s Information Criterion","text":"AIC（赤池情報量規準, Akaike’s Information Criterion）はモデルの良さを評価するために開発された指標です。\\[\n\\text{AIC} = -2 \\log(\\mathcal{L}) + 2(K+1) \\approx -2 \\log(\\mathcal{L}) + 2K\n\\]\\(\\mathcal{L}\\) は尤度 (likelihood)，\\(K\\) はパラメータの数です。AICは尤度が存在する複数のモデルを比較するために使いますが，帰無仮設を棄却するような作業はありません。","code":""},{"path":"model-selection.html","id":"尤度","chapter":"10 モデル選択","heading":"10.3 尤度","text":"統計解析で考える尤度とは，あるモデルの尤もらしさの度合いを意味します。\n尤度 \\((\\mathcal{L})\\) を求めるためには，誤差項の確率分布を指定する必要があります。ベクトル \\(x\\) の平均値は 4.6 です。\\[\nx = \\{5, 3, 6, 3, 6\\}\n\\]\n平均値の尤度は次の正規分布確率密度関数の総乗で求められます。\\[\n\\mathcal{L}(x|\\mu, \\sigma^2) = \\prod_i^5\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\n\\]","code":""},{"path":"model-selection.html","id":"総乗について","chapter":"10 モデル選択","heading":"10.4 総乗について","text":"総和は \\(\\Sigma\\) （\\(\\sigma\\)の大文字）と示すが，総乗は\\(\\Pi\\) （\\(\\pi\\)の大文字）です。\\[\n\\begin{aligned}\n\\sum_{=1}^3 &= x_1 + x_2 + x_3 \\\\\n\\prod_{=1}^3 &= x_1 \\times x_2 \\times x_3 \\\\\n\\end{aligned}\n\\]","code":""},{"path":"model-selection.html","id":"尤度の算出","chapter":"10 モデル選択","heading":"10.5 尤度の算出","text":"\\(x\\) の尤度は次のように算出できますが。対数尤度は -8.6191，尤度は 2^{-4} です。このときのAICは\n\\[\nAIC = -2\\times -8.62 + 2\\times2=21.24\n\\]他のモデルと比較しないかぎり, AICに意味はありません。","code":"\nx = tibble(x = c(5, 3, 6, 3, 6))\n\nnll = function(x, mean, sd) {\n  # 負の対数尤度関数\n  -sum(dnorm(x, mean, sd, log = TRUE)) \n}\n# bbmle パッケージの mle2() 関数\nbbmle::mle2(nll, data = x, start = list(mean = 5, sd = 1))\n#> \n#> Call:\n#> bbmle::mle2(minuslogl = nll, start = list(mean = 5, sd = 1), \n#>     data = x)\n#> \n#> Coefficients:\n#>     mean       sd \n#> 4.599997 1.356469 \n#> \n#> Log-likelihood: -8.62"},{"path":"model-selection.html","id":"aic-による-t-検定","chapter":"10 モデル選択","heading":"10.6 AIC による t 検定","text":"アヤメのデータをつかった例です。対数尤度AICもっとも小さいAICのモデルを採択するので，out1（独立した平均値と分散のモデル）を採択します。\nAICのとき，帰無仮設を棄却することはしません。帰無仮設の有意性検定とフィッシャーの有意性検定とは全く違う考え方です。AICのとき，検討している複数のモデルの中からもっともありえるモデルを採択します。最もありうるモデルは尤度の高いモデル，つまりAICの低いモデルです。","code":"\nX = iris %>% as_tibble() %>% \n  filter(str_detect(Species, \"setosa|versicolor\")) %>% \n  select(Petal.Length, Species) \n\n# 帰無仮設の関数・種間の平均値は等しい\nnll0 = function(m,s) {\n    # 負の対数尤度関数\n  -sum(dnorm(X$Petal.Length, m, s, log = TRUE)) \n}\n\n# 対立仮説の関数・種間平均値は違う\nnll1 = function(m1, s1, m2, s2, x) {\n  m = c(m1, m2)[as.numeric(X$Species)]\n  s = c(s1, s2)[as.numeric(x$Species)]\n  # 負の対数尤度関数\n  -sum(dnorm(x$Petal.Length, m, s, log = TRUE))\n}\n# 対数尤度を求める\nout0 = bbmle::mle2(nll0, start = list(m = 3, s = 1.5))\nout1 = bbmle::mle2(nll1, data = list(x = X), \n                   start = list(m1 = 1, m2 = 5, \n                                s1 = 0.5, s2 = 0.5))\nbbmle::logLik(out0)\n#> 'log Lik.' -178.5166 (df=2)\nbbmle::logLik(out1)\n#> 'log Lik.' -15.59147 (df=4)\nAIC(out0, out1)\n#>      df       AIC\n#> out0  2 361.03310\n#> out1  4  39.18294"},{"path":"model-selection.html","id":"ガラパゴス諸島の解析モデル選択の結果","chapter":"10 モデル選択","heading":"10.7 ガラパゴス諸島の解析・モデル選択の結果","text":"ガラパゴス諸島のデータを解析しているなかで，次のモデルが検討されました。\\[\n\\begin{alignedat}{2}\n\\text{H0:}\\qquad & E(\\text{Species}) = b_0\\\\\n\\text{HF:}\\qquad & E(\\text{Species}) = b_0 + b_1\\text{Area}+b_2\\text{Elevation}+b_3\\text{Nearest}+b_4\\text{Scruz}+b_5\\text{Adjacent} \\\\\n\\text{logHF:}\\qquad & E(\\log(\\text{Species})) = b_0 + b_1\\text{Area}+b_2\\text{Nearest}+b_3\\text{Adjacent} \\\\\n\\text{logHF2:}\\qquad & E(\\log(\\text{Species})) = b_0 + b_1\\log(\\text{Area})+b_2\\text{Nearest}+b_3\\log(\\text{Adjacent}) \\\\\n\\end{alignedat}\n\\]HF のAICがもっとも低いですね。\nところが，HFのクックの距離に問題があったので，その次に低い logHF2 を採択します。","code":"\ndata(gala, package = \"faraway\")\ngala = gala %>% as_tibble() # tibble に変換\ngala = gala |> mutate(logSpecies = log(Species), logArea = log(Area), logAdjacent = log(Adjacent))\nH0     = lm(Species ~ 1, data = gala)\nHF     = lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, data = gala)\nlogHF  = lm(Species ~ Area + Nearest + Adjacent, data = gala)\nlogHF2 = lm(Species ~ logArea + Nearest + logAdjacent, data = gala)\nAIC(H0, HF, logHF, logHF2) |> as_tibble(rownames = \"model\") |> arrange(AIC)\n#> # A tibble: 4 × 3\n#>   model     df   AIC\n#>   <chr>  <dbl> <dbl>\n#> 1 HF         7  339.\n#> 2 logHF2     5  342.\n#> 3 logHF      5  364.\n#> 4 H0         2  373."},{"path":"non-linear-model01.html","id":"non-linear-model01","chapter":"11 非線形モデルI","heading":"11 非線形モデルI","text":"ここで紹介する非線形モデルは、海洋生物科学実験III用に準備しました。\n光合成光曲線の解析を紹介します。","code":""},{"path":"non-linear-model01.html","id":"必要なパッケージ-9","chapter":"11 非線形モデルI","heading":"11.1 必要なパッケージ","text":"","code":"\nlibrary(tidyverse)\nlibrary(googlesheets4)\nlibrary(ggcorrplot)\nlibrary(patchwork)\nlibrary(minpack.lm)\nlibrary(nlstools)\nlibrary(broom)"},{"path":"non-linear-model01.html","id":"データの読み込み-2","chapter":"11 非線形モデルI","heading":"11.2 データの読み込み","text":"データは Google Drive に共有しています。\ngooglesheets4 のパッケージをつかて、データをクラウドからダウンロードしましょう。\nデータは共有制限なしで公開したので、authentication なしでアクセスできます。\nこのときは gs4_deauth() を実行します。Google Drive で共有したデータは次のURLの通りです。\nリンクは実験日１〜３の google sheets に飛びます。\n* 実験日 1\n* 実験日 2\n* 実験日 3google sheets にはつぎのシートが入っています。光合成データ海藻資料データ光環境データデータの読み込みは read_sheet() で行います。\n各シートの構造は同じにしているので、map() をつかって一度にデータを読み込みます。\nfnames 変数は解析に使わないので、select(-fnames) で外します。","code":"\ngs4_deauth()\nspreadsheet1 = \"https://docs.google.com/spreadsheets/d/1CsY7ILKZRFlwQEIzSgu1veMQ964IPVegIJOo04lIGVE/edit#gid=1846404397\"\nspreadsheet2 = \"https://docs.google.com/spreadsheets/d/1yeC-rJdxdiVa_icoNHZ1xrt4HWHyGCeQMnUt1r2_hnk/edit#gid=540001236\" \nspreadsheet3 = \"https://docs.google.com/spreadsheets/d/1Im8Qg-ukk8uh_3z4H6IwirTc4nhxPqKrDWrjhK4gZ0o/edit#gid=2099964525\"\nmgldata = tibble(fnames = c(spreadsheet1, spreadsheet2, spreadsheet3), day = 1:3) |> \n  mutate(data = map(fnames, read_sheet, sheet = \"光合成データ\")) |> \n  select(-fnames)\nseaweed = tibble(fnames = c(spreadsheet1, spreadsheet2, spreadsheet3), day = 1:3) |> \n  mutate(data = map(fnames, read_sheet, sheet = \"海藻資料データ\"))|> \n  select(-fnames)\nlightdata = tibble(fnames = c(spreadsheet1, spreadsheet2, spreadsheet3), day = 1:3) |> \n  mutate(data = map(fnames, read_sheet, sheet = \"光環境データ\"))|> \n  select(-fnames)"},{"path":"non-linear-model01.html","id":"光環境データの処理","chapter":"11 非線形モデルI","heading":"11.2.1 光環境データの処理","text":"班毎に光量子の平均値を計算するよりも、光条件毎の平均値を求めた方がいいとおもいます。\nデータそのものには、アルミホイルの情報がないので、光環境データと結合します。アルミホイルの tmp データ。","code":"\nlightdata = lightdata |> \n  unnest(data) |> \n  select(day,\n         han = \"班\",\n         light = \"光環境\",\n         sample = matches(\"サンプル\"),\n         ppfd = matches(\"光量子\")) |> \n  group_by(day, light) |> \n  summarise(ppfd = mean(ppfd))\ntmp = bind_rows(tibble(light = rep(\"アルミホイル\",3), \n                       ppfd =  rep(0, 3),\n                       day = 1:3))\n\nlightdata = bind_rows(lightdata, tmp) |> \n  mutate(light = factor(light), day = factor(day))"},{"path":"non-linear-model01.html","id":"全データを結合","chapter":"11 非線形モデルI","heading":"11.2.2 全データを結合","text":"光合成データ mgldata と海藻資料データ seaweed を結合する。han, sample, day は因子に変換します。結合したあとの，溶存酸素濃度の時間変動を確認します。ネットがないとき (0ネット) 溶存酸素濃度が増加するが、アルミホイルのときは減少しています。","code":"\nmgldata = mgldata |> unnest(data) |> \n  select(day,\n         han = \"班\",\n         sample = matches(\"サンプル\"),\n         min = matches(\"時間\"),\n         mgl = matches(\"酸素\"),\n         temperature = matches(\"水温\"),\n         light = matches(\"光環境\"),\n         seaweed = matches(\"海藻\"))\n\nseaweed = seaweed |> unnest(data) |> \n  select(day,\n         seaweed = matches(\"海藻\"),\n         han = \"班\",\n         sample = matches(\"サンプル\"),\n         vol = matches(\"容量\"),\n         gww = matches(\"湿重量\"))\n\nmgldata = full_join(mgldata, seaweed, by = c(\"han\", \"sample\", \"day\"))\nmgldata = mgldata |> mutate(across(c(han, sample, day), as.factor))\nggplot(mgldata) +\n  geom_point(aes(x = min, y = mgl, color = han)) +\n  facet_grid(rows = vars(light),\n             cols = vars(seaweed))"},{"path":"non-linear-model01.html","id":"光合成速度を求める","chapter":"11 非線形モデルI","heading":"11.2.3 光合成速度を求める","text":"光合成速度を求めるための関数を定義します。\nfit_model() は線形モデルを溶存酸素濃度時系列データに当てはめ用です。\nget_rate() は 2 つのモデル係数 \\((y = b_0+ b_1 x)\\) から傾き \\((b_1)\\) を抽出するためです。ここでデータをグループ化して、グループ毎の傾きを求めます。求めた係数と環境データを結合します。係数は湿重量と実験容器の容積で割って、湿重量あたり純光合成速度を求めます。\nここで、係数の単位は mg O2 l-1 min-1 から mg O2 gww min-1 に変わります。\n単位の求め方：$$\n^{} _{} ^{} $$解析をする前に、光量子量、種、班ごとの平均値を求めます。標準化した光合成速度は次の通りです。","code":"\nfit_model = function(df) {\n  lm(mgl ~ min, data = df)\n}\nget_rate = function(m) {\n  coefficients(m)[2] \n}\nmgldata = \n  mgldata |> \n  group_nest(day, han, sample, light, gww, vol, seaweed) |> \n  mutate(model = map(data, fit_model)) |> \n  mutate(rate = map_dbl(model, get_rate)) |> \n  mutate(stats = map(model, glance)) |> \n  unnest(stats) \nalldata = full_join(mgldata, lightdata, by = c(\"day\", \"light\"))\nalldata = \n  alldata %>% \n  mutate(normalized_rate = rate / gww * vol)\n  \ndataset = \n  alldata |> \n  group_by(ppfd, seaweed, han) |> \n  summarise(np = mean(normalized_rate))\nxlabel = expression(paste(\"PPFD\"~(mu*mol~m^{-2}~s^{-1})))\nylabel = '\"Net photosynthesis rate\"~(mu*g~O[2]~g[ww]^{-1}~min^{-1})'\nylabel = as.expression(parse(text = ylabel))\ndataset |> \n  ggplot() + \n  geom_point(aes(x = ppfd, y = np, color = han)) +\n  scale_color_viridis_d(\"\", end = 0.8) +\n  scale_x_continuous(xlabel) +\n  scale_y_continuous(ylabel) +\n  facet_grid(col = vars(seaweed)) "},{"path":"non-linear-model01.html","id":"モデルの当てはめ","chapter":"11 非線形モデルI","heading":"11.3 モデルの当てはめ","text":"非線形モデルの当てはめに便利な nlstools パッケージを読み込みます。当てはめたいモデルは次のとおりです。\\[\n\\overbrace{P_{net}}^{\\text{純光合成速度}} = \\underbrace{P_{max}\\left(1 - \\exp\\left(-\\frac{\\alpha}{P_{max}}\\right)\\right)}_{\\text{総光合成速度}} - \\overbrace{R_d}^{\\text{暗記呼吸速度}}\n\\]\\(P_{net}\\) は純光合成速度 (normalized_rate)\\(\\) 光量子量 (ppfd)\\(P_{max}\\) は光合成飽和速度 (pmax)\\(\\alpha\\) は初期勾配 (alpha)\\(R_d\\) は暗呼吸速度 (rd)このモデルをR関数に書き換えると次のようになります。nlstools の preview() 関数をつかって，モデル当てはめ用の関数 (nls()) に必要なの初期値を探します。\nvariable 引数に ppfd 変数の位置情報を渡してください。+ 記号がデータの中心に通る用になったら，そのときの初期値を nls() nlsLM() 関数に渡してモデルの当てはめをします。\nちなみに、nls() 関数はGauss-Newton アルゴリズムによってパラメータ推定をしますが、nlsLM() は Levenberg-Marquardt アルゴリズムを用います。\nLevenberg-Marquardt法のほうが優秀です。\nその理由について、詳細はいつか紹介します。ここではデータを海藻毎に当てはめるので、解析関数をつくります。当てはめたモデルの結果は次の通りです。次はモデルの期待値を求めます。\nこの関数は期待値を擬似データから計算します。\n擬似データは tibble() で作っています。\nFigure 11.1: 観測とモデル期待値。\n","code":"\nlibrary(nlstools)\npecurve = function(ppfd, pmax, rd, alpha) {\n  pmax * (1-exp(-alpha / pmax * ppfd)) - rd\n}\nSTART = list(pmax = 10, rd = 3, alpha = 0.3)\n\ncrispifolium = dataset |> filter(str_detect(seaweed, \"コブクロ\"))\nthunbergii = dataset |> filter(str_detect(seaweed, \"ウミトラノオ\"))\njuvenile = dataset |> filter(str_detect(seaweed, \"幼体\"))\npreview(np ~ pecurve(ppfd, pmax, rd, alpha), \n            data = crispifolium, \n        variable = 1,\n        start = START)#> \n#> RSS:  167\npreview(np ~ pecurve(ppfd, pmax, rd, alpha), \n            data = thunbergii, \n        variable = 1,\n        start = START)#> \n#> RSS:  501\npreview(np ~ pecurve(ppfd, pmax, rd, alpha), \n            data = juvenile, \n        variable = 1,\n        start = START)#> \n#> RSS:  246\nfit_nls = function(df) {\n  START = list(pmax = 14, rd = 3, alpha = 0.3)\n  # nls(np ~ pecurve(ppfd, pmax, rd, alpha),  data = df, start = START)\n  nlsLM(np ~ pecurve(ppfd, pmax, rd, alpha),  data = df, start = START)\n}\n\ndataset = dataset |> ungroup() |> \n  group_nest(seaweed) |>\n  mutate(model = map(data, fit_nls))\ndataset\n#> # A tibble: 3 × 3\n#>   seaweed                    data model \n#>   <chr>        <list<tibble[,3]>> <list>\n#> 1 ウミトラノオ           [24 × 3] <nls> \n#> 2 コブクロモク           [24 × 3] <nls> \n#> 3 幼体                   [24 × 3] <nls>\ndataset = dataset |> \n  mutate(summary =map(model, glance)) |> \n  unnest(summary)\ndataset\n#> # A tibble: 3 × 12\n#>   seaweed       data model sigma isConv  finTol logLik   AIC\n#>   <chr>     <list<t> <lis> <dbl> <lgl>    <dbl>  <dbl> <dbl>\n#> 1 ウミトラ… [24 × 3] <nls>  2.51 TRUE   1.49e-8  -54.5 117. \n#> 2 コブクロ… [24 × 3] <nls>  2.21 TRUE   1.49e-8  -51.5 111. \n#> 3 幼体      [24 × 3] <nls>  1.31 TRUE   1.49e-8  -39.0  86.0\n#> # … with 4 more variables: BIC <dbl>, deviance <dbl>,\n#> #   df.residual <int>, nobs <int>\ncalc_fitted = function(data, model) {\n  N = 21 # 擬似データの長さ\n  ndata = tibble(ppfd = seq(min(data$ppfd), max(data$ppfd),length = N))\n  tmp = predict(model, newdata = ndata) |> as_tibble()\n  bind_cols(ndata,tmp)\n}\n\ndataset = dataset |> mutate(fitted = map2(data, model, calc_fitted))\nggplot() +\n  geom_point(aes(x = ppfd, y = np), data = unnest(dataset, data))+\n  geom_line(aes(x = ppfd, y = value), data = unnest(dataset, fitted)) +\n  facet_grid(rows = vars(seaweed))"},{"path":"non-linear-model01.html","id":"診断図-1","chapter":"11 非線形モデルI","heading":"11.4 診断図","text":"線形モデルと同様に、モデルを当てはめたら、残渣の診断図も確認しましょう。\nFigure 11.2: \nモデル残渣と残渣の絶対値の平方根の図で、モデルの当てはめの良さが分かります。\n（左）はモデル残渣対期待値です。期待値が増加するとモデル残渣の散らばりが大きくなるのがはっきりしています。\n点線 (0) の周りを均一にばらつくのが理想です。\n（右）は残渣の絶対値の平方根です。期待値が上がると残渣が増加しています。\nこれらの図を確認すると、残渣の正規性に問題あると考えられます。\n","code":"\nshindan = dataset |> \n  select(seaweed, data, model) |> \n  mutate(residuals = map(model, residuals)) |> \n  mutate(fitted = map(model, fitted)) |> \n  select(seaweed, data, residuals, fitted) |> \n  unnest(everything())\np1 = ggplot(shindan) +\n  geom_point(aes(x = fitted, y = residuals,\n                 color = seaweed),\n             size = 3) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  scale_color_viridis_d(end = 0.8)\n\np2 = ggplot(shindan) +\n  geom_point(aes(x = fitted, y = sqrt(abs(residuals)),\n                 color = seaweed),\n             size = 3) +\n  scale_color_viridis_d(end = 0.8)\np1+p2"},{"path":"non-linear-model01.html","id":"パラメータの集計","chapter":"11 非線形モデルI","heading":"11.5 パラメータの集計","text":"残渣プロットの結果はひどかったが、とりあえず、光飽和点 \\((I_k)\\) と光補償点 \\((I_c)\\) を求めましょう。光飽和点：\\(I_k = P_{max} / \\alpha\\)光補償点：\\(I_c = \\frac{P_{max}}{\\alpha} \\ln\\left(\\frac{P_{max}}{P_{max} - R_d}\\right)\\)まずは係数を抽出するための関数を定義します。ここで光飽和点と光補償点を求めるための関数を定義します。map() を使って係数を求めます。","code":"\nget_cfs = function(m) {\n  cfs = coef(m)\n  tibble(pmax = cfs[\"pmax\"],\n         alpha = cfs[\"alpha\"],\n         rd = cfs[\"rd\"])\n}\ncalc_ik = function(m) {\n  cfs = coef(m)\n  cfs[\"pmax\"] / cfs[\"alpha\"]*log(cfs[\"pmax\"]/(cfs[\"pmax\"] - cfs[\"rd\"]))\n}\ncalc_ic = function(m) {\n  cfs = coef(m)\n  cfs[\"pmax\"] / cfs[\"alpha\"]\n}\nmodelcfs = dataset %>% \n  select(seaweed, model) |> \n  mutate(cfs = map(model, get_cfs)) |> \n  mutate(ik = map(model, calc_ik)) |> \n  mutate(ic = map(model, calc_ic)) |> \n  unnest(c(ik, ic, cfs))\nmodelcfs\n#> # A tibble: 3 × 7\n#>   seaweed      model   pmax  alpha    rd    ik    ic\n#>   <chr>        <list> <dbl>  <dbl> <dbl> <dbl> <dbl>\n#> 1 ウミトラノオ <nls>  23.3  0.0858 2.08  25.4  271. \n#> 2 コブクロモク <nls>   5.24 0.165  0.431  2.74  31.8\n#> 3 幼体         <nls>   4.50 0.0718 0.827 12.7   62.7\ncnames = c(\"海藻類\", \"P~max~\",\n          \"α\", \"R~d~\", \"I~k~\", \"I~c~\")\nmodelcfs |> \n  select(-model) |> \n  kableExtra::kbl(digits = c(0,1,3,1,1,1),\n                  col.names = cnames)"},{"path":"non-linear-model01.html","id":"多重比較-2","chapter":"11 非線形モデルI","heading":"11.6 多重比較","text":"数種類のデータ群に、一つのモデルを当てはめたら、群ごとに推定したパラメータの違いが気になります。\n複数群をお互いに比較することは多重比較といいます。\n一般的には、\n群ごとにパラメータを推定する full model から群ごとのデータをまとめて、一つのパラメータを推定する pooled model まで考えられます。\nこのとき、nlsLM() は使用できないので、nls() 関数を使います。\nnls() の収束を助けるために、まずは pooled model の nlsLM() の結果を full model のパラメータ初期値にします。では、データをすくし整理します。フルモデル (full model) の場合はそれぞれの群ごとに、\\(P_{max}\\)、\\(\\alpha\\)、\\(R_d\\) を推定します。\nプールモデル (pooled model) の場合は、群の区別をせずに、1 セットのパラメータを推定します。AIC を確認すると、フルモデルのAICが最も低いです。すべてのモデルを当てはめて、AICで比較することはできますが、\nWald’s 検定でパラメータ比較をしたほうが便利です。パラメータの多重比較は\n[aomisc](devtools::install_github(“onofriAndreaPG/aomisc”)\nの個人パッケージが有ると楽です。\nインストール方法は remotes パッケージをつかって、github からインストールします。多重比較は Holm法を使います。Holm法はp 値を小さい順になれべてから実施します。\n最も小さいP値の有意水準は \\(\\alpha / N\\) です。\n\\(N\\) は比較する回数です。\nここで\\(P \\leq \\alpha / N\\) なら、\n次のP値を \\(\\alpha / (N-1)\\) で評価します。\n\\(P > \\alpha / (N-1)\\) なら、ここで検定が終わります。\n帰無仮説を棄却できないまで、\\(N-k\\) の補正でP値を評価続けます。pmax の多重比較は次のとおりです。\n3つ目の比較まで評価しましたalphaとrdの場合、\\(P > \\alpha/N\\) ので、3つ目比較までしません。","code":"\ndataset = dataset |> select(seaweed, data) |> unnest(data)\ndataset = dataset |> mutate(seaweed = factor(seaweed))\npoolmodel = nlsLM(np ~ pecurve(ppfd, pmax, rd, alpha), \n    start = list(pmax = 10, rd = 3, alpha = 0.3),\n    data = dataset, \n    lower = c(pmax = 0, rd = 0, alpha = 0))\n\nSTART = lapply(coef(poolmodel), rep, 3)\nfullmodel = nls(np ~ pecurve(ppfd, pmax[seaweed], rd[seaweed], alpha[seaweed]), \n    start = START, \n    data = dataset)\nAIC(poolmodel, fullmodel) |> as_tibble(rownames = \"model\") |> arrange(AIC)\n#> # A tibble: 2 × 3\n#>   model        df   AIC\n#>   <chr>     <dbl> <dbl>\n#> 1 fullmodel    10  320.\n#> 2 poolmodel     4  353.\n# remotes::install_github(\"onofriAndreaPG/aomisc\")\nlibrary(aomisc)\ncfs = summary(fullmodel)$coef\ndf = summary(fullmodel)$df\nrows = 1:3\npairComp(cfs[rows,1], cfs[rows,2],dfr = df[2], adjust = \"holm\")\n#> $pairs\n#> \n#>   Simultaneous Tests for General Linear Hypotheses\n#> \n#> Linear Hypotheses:\n#>                  Estimate Std. Error t value Pr(>|t|)   \n#> pmax1-pmax2 == 0   18.045      4.915   3.671  0.00102 **\n#> pmax1-pmax3 == 0   18.785      4.956   3.790  0.00102 **\n#> pmax2-pmax3 == 0    0.740      1.960   0.377  0.70711   \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> (Adjusted p values reported -- holm method)\n#> \n#> \n#> $Letters\n#>            Mean       SE CLD\n#> pmax1 23.286128 4.737174   a\n#> pmax2  5.240990 1.311692   b\n#> pmax3  4.501039 1.456917   b\nrows = 4:6\npairComp(cfs[rows,1], cfs[rows,2],dfr = df[2], adjust = \"holm\")\n#> $pairs\n#> \n#>   Simultaneous Tests for General Linear Hypotheses\n#> \n#> Linear Hypotheses:\n#>              Estimate Std. Error t value Pr(>|t|)\n#> rd1-rd2 == 0   1.6452     1.5549   1.058    0.882\n#> rd1-rd3 == 0   1.2494     1.5164   0.824    0.882\n#> rd2-rd3 == 0  -0.3958     1.5815  -0.250    0.882\n#> (Adjusted p values reported -- holm method)\n#> \n#> \n#> $Letters\n#>          Mean       SE CLD\n#> rd1 2.0767329 1.052591   a\n#> rd2 0.4315057 1.144426   a\n#> rd3 0.8273007 1.091576   a\nrows = 7:9\npairComp(cfs[rows,1], cfs[rows,2],dfr = df[2], adjust = \"holm\")\n#> $pairs\n#> \n#>   Simultaneous Tests for General Linear Hypotheses\n#> \n#> Linear Hypotheses:\n#>                    Estimate Std. Error t value Pr(>|t|)\n#> alpha1-alpha2 == 0 -0.07882    0.10219  -0.771        1\n#> alpha1-alpha3 == 0  0.01404    0.05846   0.240        1\n#> alpha2-alpha3 == 0  0.09286    0.11428   0.813        1\n#> (Adjusted p values reported -- holm method)\n#> \n#> \n#> $Letters\n#>              Mean         SE CLD\n#> alpha1 0.08579966 0.01998607   a\n#> alpha2 0.16461744 0.10021636   a\n#> alpha3 0.07175792 0.05493227   a\ndetach(package:aomisc)\ndetach(package:drc)\ndetach(package:MASS)"},{"path":"non-linear-model01.html","id":"モデルの改良-1","chapter":"11 非線形モデルI","heading":"11.7 モデルの改良","text":"上で多重比較をしましたが、実は非線形モデルのパラメータはお互いとの相関関係が強いことが多いです。\nつまり、多重比較に若干無理があります。\n光合成光曲線の解析から推定したパラメータの相関関係は次の図の通りです。\nFigure 11.3: \n光合成光曲線の解析からもとめたパラメータの相関関係を示しています。\n種ごとにまとめて、3つの相関プロットに示した。\n（右）ウミトラノオ、（中）コブクロモク、（左）幼体。\n相関係数は -1 から　1 をとります。\nウミトラノオ以外の場合では、パラメータごとの関係に正の相関があります。\nさらに、共分散も確認すると、負の値をとったペアもあります。\n分散は 0 から　1の値しか取れませんが、共分散は負の値をとっても問題ないです。\n診断図の結果によると、このモデルには色々と問題がありますね。モデル係数が絶対に正の値になるように、モデルは次の通りに改良した。\\[\n\\begin{aligned}\nP_{net} &= P_{max}\\left(1 - \\exp\\left(-\\frac{\\alpha}{P_{max}}\\right)\\right) - R_d \\\\\nP_{max} &= \\exp(\\log P_{max}) \\\\\n\\alpha &= \\exp(\\log \\alpha) \\\\\nR_{d} &= \\exp(\\log R_{d}) \\\\\n\\end{aligned}\n\\]ログスケールのパラメータは \\(-\\infty \\leq x \\leq \\infty\\)　ですが、\n指数関数で変換すると、\\(\\exp(x) > 0\\) です。\nFigure 11.4: \n改良したモデルでも、共分散行列に負の値がでました。\nコブクロモクの場合、光合成が飽和するほどの光ではなかったと考えられる。\n問題を残しながら、\\(I_c\\) と \\(I_k\\) の誤差を求めます。","code":"\nfm_cov  = vcov(fullmodel) # モデルパラメータの共分散行列\nfm_corr = cov2cor(fm_cov) # モデルパラメータの相関行列\nvname1 = str_c(c(\"pmax\", \"alpha\", \"rd\"), \"1\")\nvname2 = str_c(c(\"pmax\", \"alpha\", \"rd\"), \"2\")\nvname3 = str_c(c(\"pmax\", \"alpha\", \"rd\"), \"3\")\np1 = ggcorrplot(fm_corr[vname1, vname1],     type = \"upper\", show.diag = T, lab =T)\np2 = ggcorrplot(fm_corr[vname2, vname2], type = \"upper\", show.diag = T, lab = T)\np3 = ggcorrplot(fm_corr[vname3, vname3], type = \"upper\", show.diag = T, lab = T)\np1 + p2 + p3 + plot_layout(ncol = 1)\nvname1 = str_c(c(\"pmax\", \"alpha\", \"rd\"), \"1\")\nvname2 = str_c(c(\"pmax\", \"alpha\", \"rd\"), \"2\")\nvname3 = str_c(c(\"pmax\", \"alpha\", \"rd\"), \"3\")\np1 = ggcorrplot(fm_cov[vname1, vname1],     type = \"upper\", show.diag = T, lab =T)\np2 = ggcorrplot(fm_cov[vname2, vname2], type = \"upper\", show.diag = T, lab = T)\np3 = ggcorrplot(fm_cov[vname3, vname3], type = \"upper\", show.diag = T, lab = T)\np1 + p2 + p3 + plot_layout(ncol = 1)\npecurve_exp = function(ppfd, pmax, rd, alpha) {\n  pmax = exp(pmax)\n  alpha = exp(alpha)\n  rd = exp(rd)\n  pmax * (1 - exp(-alpha / pmax * ppfd)) - rd\n}\n\npoolmodel_exp = nlsLM(np ~ pecurve_exp(ppfd, pmax, rd, alpha), \n    start = list(pmax = log(10), rd = log(3), alpha = log(0.3)),\n    data = dataset)\npoolmodel_start = nlsLM(np ~ pecurve_exp(ppfd, pmax, rd, alpha), \n    start = list(pmax = log(10), rd = log(3), alpha = log(0.3)),\n    data = dataset |> filter(ppfd < 300))\n\nSTART = lapply(coef(poolmodel_start), rep, 3)\n\nfullmodel_exp = nls(np ~ pecurve_exp(ppfd, pmax[seaweed], rd[seaweed], alpha[seaweed]), \n    start = START, data = dataset)\n\nAIC(poolmodel_exp, fullmodel_exp) |> as_tibble(rownames = \"model\") |> arrange(AIC)\n#> # A tibble: 2 × 3\n#>   model            df   AIC\n#>   <chr>         <dbl> <dbl>\n#> 1 fullmodel_exp    10  320.\n#> 2 poolmodel_exp     4  353.\nfm_cov_exp  = vcov(fullmodel_exp) # モデルパラメータの共分散行列\nfm_corr_exp = cov2cor(fm_cov_exp) # モデルパラメータの相関行列\n\nvname1 = str_c(c(\"pmax\", \"alpha\", \"rd\"), \"1\")\nvname2 = str_c(c(\"pmax\", \"alpha\", \"rd\"), \"2\")\nvname3 = str_c(c(\"pmax\", \"alpha\", \"rd\"), \"3\")\np1 = ggcorrplot(fm_corr_exp[vname1, vname1],     type = \"upper\", show.diag = T, lab =T)+\n  labs(title = \"相関行列\")\np2 = ggcorrplot(fm_corr_exp[vname2, vname2], type = \"upper\", show.diag = T, lab = T)\np3 = ggcorrplot(fm_corr_exp[vname3, vname3], type = \"upper\", show.diag = T, lab = T) \np4 = ggcorrplot(fm_cov_exp[vname1, vname1],     type = \"upper\", show.diag = T, lab =T)+\n  labs(title = \"共分散行列\")\np5 = ggcorrplot(fm_cov_exp[vname2, vname2], type = \"upper\", show.diag = T, lab = T)\np6 = ggcorrplot(fm_cov_exp[vname3, vname3], type = \"upper\", show.diag = T, lab = T) \n\np1 + p2 + p3 + p4 + p5 + p6 + plot_layout(ncol = 2, byrow = F)"},{"path":"non-linear-model01.html","id":"ガウス誤差伝播法","chapter":"11 非線形モデルI","heading":"11.8 ガウス誤差伝播法","text":"誤差伝播法は直接推定したパラメータから間接パラメータの誤差を推定するための手法です。\n他変数関数のテイラー展開を第2次の項まで求めた式で誤差を計算します。\nただし、直接推定したパラメータはお互いに独立していたら、第1次の項まで十分です。\\[\n\\sigma_f^2 = \\mathbf{g}^T\\mathbf{V}\\mathbf{g}\n\\]\n\\[\n\\mathbf{g} =\n\\begin{bmatrix}\n\\frac{\\partial f}{\\partial \\beta_x}\\\\\n\\frac{\\partial f}{\\partial \\beta_y}\\\\\n\\frac{\\partial f}{\\partial \\beta_z}\\\\\n\\end{bmatrix}\n\\]\\[\n\\mathbf{V} =\n\\begin{bmatrix}\n\\sigma_{xx}^2 & \\sigma_{xy}^2 & \\sigma_{xz}^2 \\\\\n\\sigma_{xy}^2 & \\sigma_{yy}^2 & \\sigma_{yz}^2 \\\\\n\\sigma_{xz}^2 & \\sigma_{yz}^2 & \\sigma_{zz}^2 \\\\\n\\end{bmatrix}\n\\]\\[\n\\sigma_f^2 =\n\\begin{bmatrix}\n\\frac{\\partial f}{\\partial \\beta_x}&\n\\frac{\\partial f}{\\partial \\beta_y}&\n\\frac{\\partial f}{\\partial \\beta_z}\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\sigma_{xx}^2 & \\sigma_{xy}^2 & \\sigma_{xz}^2 \\\\\n\\sigma_{xy}^2 & \\sigma_{yy}^2 & \\sigma_{yz}^2 \\\\\n\\sigma_{xz}^2 & \\sigma_{yz}^2 & \\sigma_{zz}^2 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\frac{\\partial f}{\\partial \\beta_x}\\\\\n\\frac{\\partial f}{\\partial \\beta_y}\\\\\n\\frac{\\partial f}{\\partial \\beta_z}\\\\\n\\end{bmatrix}\n\\]\\[\n\\sigma_f^2 =\n\\begin{bmatrix}\n\\frac{\\partial f}{\\partial \\beta_x}\\sigma_{xx}^2 +\\frac{\\partial f}{\\partial \\beta_y}\\sigma_{xy}^2 +\\frac{\\partial f}{\\partial \\beta_y}\\sigma_{xz}^2&\n\\frac{\\partial f}{\\partial \\beta_x}\\sigma_{xy}^2 +\\frac{\\partial f}{\\partial \\beta_y}\\sigma_{yy}^2 +\\frac{\\partial f}{\\partial \\beta_y}\\sigma_{yz}^2&\n\\frac{\\partial f}{\\partial \\beta_x}\\sigma_{xz}^2 +\\frac{\\partial f}{\\partial \\beta_y}\\sigma_{yz}^2 +\\frac{\\partial f}{\\partial \\beta_y}\\sigma_{zz}^2\n\\end{bmatrix}\n\\begin{bmatrix}\n\\frac{\\partial f}{\\partial \\beta_x}\\\\\n\\frac{\\partial f}{\\partial \\beta_y}\\\\\n\\frac{\\partial f}{\\partial \\beta_z}\\\\\n\\end{bmatrix}\n\\]\n\\[\n\\sigma_f^2 =\n\\frac{\\partial f}{\\partial \\beta_x}\\left(\\frac{\\partial f}{\\partial \\beta_x}\\sigma_{xx}^2 +\\frac{\\partial f}{\\partial \\beta_y}\\sigma_{xy}^2 +\\frac{\\partial f}{\\partial \\beta_z}\\sigma_{xz}^2\\right) +\n\\frac{\\partial f}{\\partial \\beta_y}\\left(\\frac{\\partial f}{\\partial \\beta_x}\\sigma_{xy}^2 +\\frac{\\partial f}{\\partial \\beta_y}\\sigma_{yy}^2 +\\frac{\\partial f}{\\partial \\beta_z}\\sigma_{yz}^2\\right) +\n\\frac{\\partial f}{\\partial \\beta_z}\\left(\\frac{\\partial f}{\\partial \\beta_x}\\sigma_{xz}^2 +\\frac{\\partial f}{\\partial \\beta_y}\\sigma_{yz}^2 +\\frac{\\partial f}{\\partial \\beta_z}\\sigma_{zz}^2\\right)\n\\]\\[\n\\sigma_f^2 = \\left(\\frac{\\partial f}{\\partial \\beta_x}\\right)^2\\sigma_{xx}^2 +\n                   \\frac{\\partial f}{\\partial \\beta_x}\\frac{\\partial f}{\\partial \\beta_y}\\sigma_{xy}^2 +\n                   \\frac{\\partial f}{\\partial \\beta_x}\\frac{\\partial f}{\\partial \\beta_z}\\sigma_{xz}^2 +\n             \\left(\\frac{\\partial f}{\\partial \\beta_y}\\right)^2\\sigma_{yy}^2 +\n                   \\frac{\\partial f}{\\partial \\beta_y}\\frac{\\partial f}{\\partial \\beta_x}\\sigma_{xy}^2 +\n                   \\frac{\\partial f}{\\partial \\beta_y}\\frac{\\partial f}{\\partial \\beta_z}\\sigma_{yz}^2 +\n             \\left(\\frac{\\partial f}{\\partial \\beta_z}\\right)^2\\sigma_{zz}^2 +\n                   \\frac{\\partial f}{\\partial \\beta_z}\\frac{\\partial f}{\\partial \\beta_x}\\sigma_{xz}^2 +\n                   \\frac{\\partial f}{\\partial \\beta_z}\\frac{\\partial f}{\\partial \\beta_y}\\sigma_{yz}^2\n\\]\\[\n\\sigma_f^2 = \\left(\\frac{\\partial f}{\\partial \\beta_x}\\right)^2\\sigma_{xx}^2 +\n             \\left(\\frac{\\partial f}{\\partial \\beta_y}\\right)^2\\sigma_{yy}^2 +\n             \\left(\\frac{\\partial f}{\\partial \\beta_z}\\right)^2\\sigma_{zz}^2 +\n                   2\\frac{\\partial f}{\\partial \\beta_x}\\frac{\\partial f}{\\partial \\beta_y}\\sigma_{xy}^2 +\n                   2\\frac{\\partial f}{\\partial \\beta_x}\\frac{\\partial f}{\\partial \\beta_z}\\sigma_{xz}^2 +\n                   2\\frac{\\partial f}{\\partial \\beta_y}\\frac{\\partial f}{\\partial \\beta_z}\\sigma_{yz}^2\n\\]参考文献：\n* Tellinghuisen J. 2001. Statistical error propagation. Journal Physical Chemistry 105: 3917 - 3921.\n* Lo E. 2005. Gaussian error propagation applied ecological data: Post-ice-storm-downed woody biomass. Ecological Monographs 75: 451-466.\\[\n\\sigma_f^2=\\sum_{= 1}^n \\left(\\frac{\\partial q}{\\partial x_i}\\sigma_{x_i}\\right)^2 + 2\\sum_{= 1}^n\\sum_{j = 1,j\\neq }^n \\left(\\frac{\\partial q}{\\partial x_i}\\frac{\\partial q}{\\partial x_j}\\rho_{x_i x_j}\\sigma_{x_i}\\sigma_{x_j}\\right)\n\\]共分散と相関の関係は次の通りです。\\[\n\\overbrace{\\rho_{xy}}^\\text{相関}= \\overbrace{\\sigma_{xy}}^\\text{共分散} / \\underbrace{(\\sigma_x\\sigma_y)}_{x,y\\;\\text{の標準偏差}}\n\\]では、Rで解析に使ったモデルの偏微分方程式を求めます。\n光飽和点 \\((I_k)\\) と光補償点 \\((I_c)\\) の formula は次のように定義します。\\[\n\\begin{aligned}\nI_k & = f_1 = P_{max} / \\alpha \\\\\nI_c & = f_2 = \\frac{P_{max}}{\\alpha} \\ln\\left(\\frac{P_{max}}{P_{max} - R_d}\\right) \\\\\n\\end{aligned}\n\\]fic と fik は 3つの要素で組み立てられた formula です。\\[\n\\begin{aligned}\n\\frac{\\partial f_1}{\\partial \\alpha} &= \\frac{-P_{max}}{\\alpha^2} \\\\\n\\frac{\\partial f_1}{\\partial P_{max}} &= \\frac{1}{\\alpha} \\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\frac{\\partial f_2}{\\partial \\alpha} &= -\\frac{P_{max}}{\\alpha^2} \\log\\left(\\frac{P_{max}}{P_{max} - R_d}\\right) \\\\\n\\frac{\\partial f_2}{\\partial P_{max}} &= \\frac{1}{\\alpha} \\log\\left(\\frac{P_{max}}{P_{max} - R_d}\\right) + \\frac{P_{max}}{\\alpha}\\left(\\frac{1}{P_{max} - R_d} - \\frac{P_{max}}{(P_{max}-R_d)^2}\\right)/\\left(\\frac{P_{max}}{P_{max} - R_d}\\right) \\\\\n\\frac{\\partial f_2}{\\partial R_{d}} &= \\frac{P_{max}}{\\alpha} \\frac{P_{max}} {\\left(P_{max}-R_d\\right)^2}/\\left(\\frac{P_{max}}{P_{max} - R_d}\\right) \\\\\n\\end{aligned}\n\\]ウミトラノオの共分散値は負の値になっていたので、ウミトラノオの \\(I_c\\) と \\(I_k\\) の標準誤差の推定はできません。\n残りのコブクロモクと幼体の方だけ推定してみます。","code":"\nfic = ic ~ pmax/alpha\nfik = ik ~ pmax/alpha * log(pmax/(pmax - rd))\nclass(fic)\n#> [1] \"formula\"\nlength(fic)\n#> [1] 3\nfic[[1]]\n#> `~`\nfic[[2]]\n#> ic\nfic[[3]]\n#> pmax/alpha\npropagate_error = function(model, dmodel, parameters) {\n  require(rlang)  \n  cfs = coefficients(model)\n  vars = all.vars(dmodel[[3]])\n  V = vcov(model)\n  \n  V = V[parameters, parameters] \n  expectation = cfs[parameters]\n  \n  gradient_fn = deriv(dmodel[[3]], vars, function.arg = T)\n  ff = expr(gradient_fn(!!!syms(vars)))\n  tmp =  exprs(!!!expectation)\n  tmp = set_names(tmp, vars)\n  \n  for(i in 1:length(vars)) {\n    call2(\"=\", expr(!!vars[i]), expectation[i]) |> eval()\n  }\n  G = eval(ff)\n  G = attributes(G)$gradient |> matrix(ncol = 1)\n  sqrt((t(G) %*% V) %*% G)\n}\ncfsout = summary(fullmodel)$coef |> as_tibble(rownames = \"parameter\") |> \n  dplyr::select(parameter, est=Estimate, se = `Std. Error`) |> \n  mutate(id = str_extract(parameter, \"[0-9]\"),\n         parameter = str_extract(parameter, \"[A-z]+\")) |> \n  pivot_wider(names_from = parameter,\n              values_from = c(est, se),\n              names_glue = \"{.value}_{parameter}\")\ncnames = c(\"Seaweed\", \"Parameter\", \"Estimate\", \"SE\")\ncfsout = bind_cols(modelcfs |> select(seaweed, est_ic = ic, est_ik = ik), cfsout) |> \n  mutate(pmax = str_c(\"pmax\", id),\n  alpha = str_c(\"alpha\", id),\n  rd = str_c(\"rd\", id)) |> \n  mutate(se_ic = pmap_dbl(list(pmax, alpha, rd), \\(pmax, alpha, rd) {\n    fic = ic ~ exp(pmax) / exp(alpha)\n    propagate_error(fullmodel, fic, c(pmax, alpha))\n  })) |> \n  mutate(se_ik = pmap_dbl(list(pmax, alpha, rd), \\(pmax, alpha, rd) {\n    fik = ik ~ exp(pmax) / exp(alpha) * log(exp(pmax) / (exp(pmax) - exp(rd)))\n    propagate_error(fullmodel, fik, c(pmax, alpha, rd))\n  })) |> select(-alpha, -pmax, -rd) |> \n  pivot_longer(cols = matches(\"est_|se_\"),\n               names_to = c(\"stat\", \"par\"),\n               names_pattern = \"(est|se)_(.*)\") |> \n  pivot_wider(names_from = stat, values_from = value) |> \n  arrange(par, id) |> \n  select(-id)\ncfsout |> \n  kableExtra::kbl(digits = c(0,0,3,2),\n                  col.names = cnames)"},{"path":"ggplot.html","id":"ggplot","chapter":"12 ggplot","heading":"12 ggplot","text":"未完成","code":"\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(lemon)\nlibrary(scales)\nlibrary(ggrepel)\nlibrary(patchwork)\niris = as_tibble(iris)"},{"path":"maps.html","id":"maps","chapter":"13 地図の作り方","heading":"13 地図の作り方","text":"","code":""},{"path":"maps.html","id":"必要なパッケージ-10","chapter":"13 地図の作り方","heading":"13.1 必要なパッケージ","text":"次の2つは地図専用のパッケージです。Noto Sans のフォントが好きなので、ここで Google Fonts からアクセスします。ggplot のデフォルトテーマも設定し、フォント埋め込みも可能にします。\nここでデフォルトを設定すると、毎回 theme_pubr() を ggplotのチェインにたさなくていい。","code":"\nlibrary(tidyverse)　# Essential package\nlibrary(ggpubr)     # Publication-oriented figures\nlibrary(kableExtra) # Tables\nlibrary(magick)     # Imagemagick R API\nlibrary(patchwork)  # Simplified figure tiling\nlibrary(showtext)   # I want to use google fonts in the figures\nlibrary(ggspatial)  # Essential for map-making with ggplot\nlibrary(sf)         # Essential for map data manipulation\nfont_add_google(\"Noto Sans\",\"notosans\")\ntheme_pubr(base_size = 10, base_family = \"notosans\") |> theme_set()\nshowtext_auto() # Automatically embed the Noto Sans fonts into the ggplots."},{"path":"maps.html","id":"シェープファイルの読み込み","chapter":"13 地図の作り方","heading":"13.2 シェープファイルの読み込み","text":"シェープファイル (shapefile) は地図データのことです。\n基本的の拡張子は shp, shx, dbf　ですが、その他に prj と xml もあります。研究室用にダウンロードした 国土交通省・国土数値情報ダウンロードサービス のシェープファイルは ~/Lab_Data/Japan_map_data/Japan に入っています。mlit に読み込んだシェープファイルはここへ。シェープファイルの 座標参照系 (CRS: Coordinate Reference System) を確認しましょう。CRSには 地理座標系 と 投影座標系 の2種類があります。\n座標系にはEPSGコードもつけられています。このデータは政策区域のデータなので、とても重いです。\nまずは、都道府県ごとにまとめた RDS ファイルを作って保存します。\n都道府県ごとに st_union() を使って polgyon データを結合します。\n結合したデータを unnest して、simple feature に戻してかた保存します。\n121158 features もあるので、数時間もかります。沿岸のデータだけなら軽いですので、C23 シリーズのファイルを読み込みます。では、ここで地図の確認をします。mlit のデータは細かい政策区域まで分けられているので、全国スケールの図には向いていません。\nst_union() をつかって、都道府県ごとに polygon を結合したファイルは、~/Lab_Data/Japan_map_data/Japan/todofuken.rds に保存しています。\n次のコードで、都道府県ごとにまとめましたが、並列処理でも５時間以上もかかったので、RDS ファイルを使いましょう。","code":"\nmlit = read_sf(\"~/Lab_Data/Japan_map_data/Japan/N03-20210101_GML/\")\nst_crs(mlit)\n#> Coordinate Reference System:\n#>   User input: JGD2011 \n#>   wkt:\n#> GEOGCRS[\"JGD2011\",\n#>     DATUM[\"Japanese Geodetic Datum 2011\",\n#>         ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n#>             LENGTHUNIT[\"metre\",1]]],\n#>     PRIMEM[\"Greenwich\",0,\n#>         ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>     CS[ellipsoidal,2],\n#>         AXIS[\"geodetic latitude (Lat)\",north,\n#>             ORDER[1],\n#>             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>         AXIS[\"geodetic longitude (Lon)\",east,\n#>             ORDER[2],\n#>             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>     USAGE[\n#>         SCOPE[\"Horizontal component of 3D system.\"],\n#>         AREA[\"Japan - onshore and offshore.\"],\n#>         BBOX[17.09,122.38,46.05,157.65]],\n#>     ID[\"EPSG\",6668]]\n# HTML 用テーブル\ntibble(`EPSG Code` = c(4326,6668,6677),\n       `CRS` = c(\"WGS84\", \"JGD2011\", \"JGD2011 / Japan Plane Rectangular CS IX\"),\n       `Units` = c(\"degrees\", \"degrees\", \"meters\")) |> \n  kbl() |> \n  kable_styling(bootstrap_options = c(\"hover\"))\nmlit = tibble(folder = dir(\"~/Lab_Data/Japan_map_data/Coastline/\", full = TRUE)) |> \n  mutate(data = map(folder, read_sf)) |> select(data) |> \n  unnest(data) |> \n  st_as_sf(crs = st_crs(6668))\nmlit |> ggplot() + geom_sf()\n# Takes 5.5 hours to complete with 30 cores!\n# library(furrr)\n# plan(multisession, workers = 30)\n# Group by prefecture\n# mlit1 = mlit |> group_nest(N03_001) |> \n#   # mutate(data = future_map(data, st_union)) |> \n#   unnest(data) |> st_as_sf() \n# mlit1 |> write_rds(\"~/Lab_Data/Japan_map_data/Japan/todofuken.rds\")\nmlit1 = read_rds(\"~/Lab_Data/Japan_map_data/Japan/todofuken.rds\")\nmlit1 |> ggplot() + geom_sf()"},{"path":"maps.html","id":"調査地点のデータを準備する","chapter":"13 地図の作り方","heading":"13.3 調査地点のデータを準備する","text":"形上湾アマモ場調査のステーションの GPS tibble を準備する。zostera に緯度経度を設定する。\nCRS は mlit と同じにします。","code":"\nzostera = read_csv(\"~/Lab_Data/matsumuro/Katagami_Bay/longlat_info.csv\")\nzostera |> print(n = 3)\n#> # A tibble: 105 × 6\n#>    Name   lat  long datetime            eelgrass\n#>   <dbl> <dbl> <dbl> <dttm>              <chr>   \n#> 1     1  33.0  130. 2021-05-25 09:14:48 absent  \n#> 2     2  33.0  130. 2021-05-25 09:30:32 absent  \n#> 3     3  33.0  130. 2021-05-25 09:37:16 present \n#> # … with 102 more rows, and 1 more variable:\n#> #   `coverage(%)` <dbl>\nzostera = zostera |> st_as_sf(coords = c(\"long\", \"lat\"), crs = st_crs(mlit))\nzostera |> print(n = 3)\n#> Simple feature collection with 105 features and 4 fields\n#> Geometry type: POINT\n#> Dimension:     XY\n#> Bounding box:  xmin: 129.7845 ymin: 32.90032 xmax: 129.806 ymax: 32.95375\n#> Geodetic CRS:  JGD2011\n#> # A tibble: 105 × 5\n#>    Name datetime            eelgrass `coverage(%)`\n#> * <dbl> <dttm>              <chr>            <dbl>\n#> 1     1 2021-05-25 09:14:48 absent               0\n#> 2     2 2021-05-25 09:30:32 absent               0\n#> 3     3 2021-05-25 09:37:16 present              5\n#> # … with 102 more rows, and 1 more variable:\n#> #   geometry <POINT [°]>"},{"path":"maps.html","id":"九州データの抽出","chapter":"13 地図の作り方","heading":"13.4 九州データの抽出","text":"九州のデータと長崎のデータを抽出します。\n重要：長崎の名前が誤っています。Nagasaki のはずが、Naoasaki として記録されています。海岸線のデータ (mlit) から長崎の情報を抽出したいが、このデータの位置情報はコードで記述されています。長崎の海岸線は次のようになります。九州は mlit1 から抽出したので、都道府県政策区域として作図されます。長崎をハイライトしましょう。この図には、違和感を感じるので、山口、島根、愛媛、広島と高知も追加します。\nそしれ、最初に作った kyushu の範囲を抽出しておきます。長崎、九州、その他の色分けをして、 kyushu をクロップします。\nクロップ範囲は kbbox です。この地図は次のようになりました。","code":"\ntoget = \"長崎|福岡|大分|佐賀|熊本|鹿児島|宮崎\"\nkyushu = mlit1 |> filter(str_detect(N03_001, toget))\nadmincode = readxl::read_xlsx(\"~/Lab_Data/Japan_map_data/AdminiBoundary_CD.xlsx\", skip = 2)\nadmincode = admincode |> select(code = matches(\"行政\"), N03_001 = matches(\"都道府県*.*漢字\"))\ncodes = admincode |> filter(str_detect(N03_001, \"長崎\")) |> pull(code)\nnagasaki = mlit |> filter(str_detect(C23_001, str_c(codes, collapse = \"|\"))) \nggplot() + geom_sf(data = nagasaki)\nggplot() + geom_sf(data = kyushu)\nkyushu |> \n  mutate(fillme = str_detect(N03_001, \"長崎\")) |> \n  ggplot() + geom_sf(aes(fill = fillme), color = NA) +\n  guides(fill = \"none\") +\n  scale_fill_viridis_d() +\n  theme(panel.background = element_rect(fill = \"lightblue\", color = \"black\"),\n        axis.line = element_blank())\nkbbox = kyushu |> st_bbox()\ntoget = \"長崎|福岡|大分|佐賀|熊本|鹿児島|宮崎|山口|島根|愛媛|高知|広島\"\nkyushu = mlit1 |> filter(str_detect(N03_001, toget))\nkyushu = kyushu |>\n  mutate(fillme = case_when(str_detect(N03_001, \"長崎\") ~ \"Nagasaki\",\n                            str_detect(N03_001, \"福岡|大分|佐賀|熊本|鹿児島|宮崎\") ~ \"Kyushu\",\n                            TRUE ~ \"Honshu\")) |> \n  st_crop(kbbox)\nggplot(kyushu) + \n  geom_sf(aes(fill = fillme), color = NA) +\n  guides(fill = \"none\") +\n  coord_sf(expand = FALSE) +\n  scale_fill_viridis_d() +\n  theme(panel.background = element_rect(fill = \"lightblue\", color = \"black\"),\n        axis.line = element_blank())"},{"path":"maps.html","id":"調査地点の図","chapter":"13 地図の作り方","heading":"13.5 調査地点の図","text":"形上湾と大村湾の図を作ります。\n形上湾の方には、調査地点と結果ものせます。\nまずは形上湾と大村湾の範囲を決めます。\n範囲は Google Map で選びました。ここで、それぞれの湾のデータを kyushu からぬきます。アマモの被度データの simple features データを準備します。九州の図を先につくります。大村湾と形上湾の図を次に作りますが、先にラベルの tibble を準備します。\ntibble の long と lat のデータは試行錯誤で来ました。\nもっといい方法はあるはずです。では、大村湾と形上湾の地図をつくります。patchwork のパッケージをつかって、図を組み立てます。\n図は PDF に保存したら、magick を使って、PNGにも変換します。","code":"\nkatagami = rbind(rev(c(32.95809069048365, 129.7669185309373)),\n                 rev(c(32.89802000729197, 129.82832411747583))) |>\n  as_tibble(.name_repair = \\(x) c(\"long\", \"lat\")) |>\n  st_as_sf(coords = c(\"long\", \"lat\"), crs = st_crs(kyushu))\n\nomurabay = rbind(rev(c(33.103196388120104, 129.67183787501082)),\n                 rev(c(32.817013859622804, 130.03298144413574))) |> \n  as_tibble(.name_repair = \\(x) c(\"long\", \"lat\")) |>\n  st_as_sf(coords = c(\"long\", \"lat\"), crs = st_crs(kyushu))\nomurabay_area = kyushu |> filter(str_detect(N03_001, \"長崎\")) |> st_crop(st_bbox(omurabay)) \nkatagami_area = kyushu |> filter(str_detect(N03_001, \"長崎\")) |> st_crop(st_bbox(katagami)) \nzostera = zostera |>\n  st_as_sf(coords = c(\"long\", \"lat\"), crs = st_crs(kyushu)) |> \n  rename(coverage = matches(\"cover\")) |> \n  mutate(rank = cut(coverage, \n                    c(-Inf, 1, 10, 40, 70, Inf),\n                    labels = c(\"E\", \"D\", \"C\", \"B\", \"A\"))) |> \n  mutate(rank = factor(rank, \n                       levels = LETTERS[1:5],\n                       labels = LETTERS[1:5]))\n# The main plot of kyushu\npmain = ggplot(kyushu) + \n  geom_sf(aes(fill = fillme), color = NA) +\n  guides(fill = \"none\") +\n  coord_sf(expand = FALSE) +\n  scale_fill_viridis_d() +\n  theme(panel.grid = element_blank(),\n        panel.background = element_rect(fill = \"lightblue\", color =\"black\"),\n        panel.border  = element_rect(fill = NA, color =\"black\"),\n        plot.background =  element_rect(fill = NA, color =NA),\n        axis.title = element_blank(),\n        axis.line = element_blank())\n# Build plots for Omura Bay and Katagami Bay.\ntmp1 = omurabay_area |> st_transform(crs = st_crs(6677)) |> st_bbox()\ntmp2 = katagami_area |> st_transform(crs = st_crs(6677)) |> st_bbox()\n# tibble for labeling figures. The long and lat are by trial-and-error.\n# Need to find a better method.\nlabel1 = tibble(long = tmp1[3] -2500,\n                lat = tmp1[2] +1700,\n                label = \"Omura Bay, Nagasaki, Japan\") |> \n  st_as_sf(coords = c(\"long\", \"lat\"), crs = st_crs(6677), agr = \"constant\") |> \n  st_transform(crs = st_crs(omurabay_area))\n\nlabel2 = tibble(long = tmp2[1] +800,\n                lat = tmp2[4] -150,\n                label = \"Katagami Bay, Nagasaki, Japan\") |> \n  st_as_sf(coords = c(\"long\", \"lat\"), crs = st_crs(6677), agr = \"constant\") |> \n  st_transform(crs = st_crs(omurabay_area))\npomura = ggplot() +\n  geom_sf(fill = \"grey50\", data = omurabay_area, size = 0) +\n  geom_sf_text(aes(label = label), \n               data = label1,\n               color = \"white\",\n               family = \"notosans\", \n               fontface = \"bold\",\n               vjust = 1, hjust = 1,\n               size = 5)  + \n  coord_sf(expand = FALSE) +\n  annotation_north_arrow(style = north_arrow_minimal(text_family = \"notosans\", \n                                                     text_face = \"bold\",\n                                                     line_width = 2,\n                                                     text_size = 20),\n                         pad_y = unit(0.3, \"npc\")) + \n  theme(panel.background = element_rect(fill = \"lightblue\", color =\"black\"),\n        panel.border  = element_rect(fill = NA, color =\"black\"),\n        plot.background =  element_rect(fill = \"white\", color =NA),\n        axis.title = element_blank(),\n        axis.line = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank())\n\npkatagami = ggplot() +\n  geom_sf(fill = \"grey50\", data = katagami_area, size = 0) +\n  geom_sf(aes(fill = rank), data = zostera,\n          pch = 21, size = 3,\n          color = \"white\", stroke = 1) +\n  geom_sf_text(aes(label = label), \n               data = label2,\n               color = \"white\",\n               family = \"notosans\", \n               fontface = \"bold\",\n               vjust = 1.0, hjust = 0.0,\n               size = 5)  + \n  annotation_north_arrow(style = north_arrow_minimal(text_family = \"notosans\", \n                                                     text_face = \"bold\",\n                                                     line_width = 2,\n                                                     text_size = 20)) + \n  coord_sf(expand = FALSE, crs = st_crs(katagami_area)) +\n  scale_fill_viridis_d(end = 0.8) +\n  theme(panel.grid = element_blank(),\n        panel.background = element_rect(fill = \"lightblue\", color =\"black\"),\n        panel.border  = element_rect(fill = NA, color =\"black\"),\n        plot.background =  element_rect(fill = \"white\", color =NA),\n        axis.title = element_blank(),\n        axis.line = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank())\npout = pmain + (pomura / pkatagami)\npdfname = \"Images/katagami-map-v1.pdf\"\npngname = str_replace(pdfname, \"pdf\", \"png\")\nggsave(pdfname, plot= pout, width = 300, height = 300, units = \"mm\")\nimage_read_pdf(pdfname, density = 600) |> image_trim() |> image_border(color = \"white\") |> image_write(pngname)"},{"path":"wavelet.html","id":"wavelet","chapter":"14 Wavelet 解析","heading":"14 Wavelet 解析","text":"","code":""},{"path":"wavelet.html","id":"必要なパッケージ-11","chapter":"14 Wavelet 解析","heading":"14.1 必要なパッケージ","text":"ウェーブレット変換は時系列データから時間的変化の特徴と周波数成分を調べるために使います。\n細かい説明はいつか追加します。\n本章を完成するまでは、次の論文を参考にしてください。Cazelles B., Chavez M., Berteaux D., Menard F., Vik J.O., Jenouvrier S., Stenseth N.C. 2008. Wavelet analysis ecological time series. Oecologia 156: 287-304.Grinsted ., Moore J.C., Jevrejeva S. 2004. Application cross wavelet transform wavelet coherence geophysical time series. Nonlinear Processes Geophysics 11: 561-566.Torrence C. Compo G.P. 1998. practical guide wavelet analysis. Bulletin American Meteorological Society 79: 61-78.Torrence C Webster P.J. 1998. annual cycle persistence El Nino/Southern Oscillation. Quarterly Journal Royal Meteorological Society 124: 1985-2004.\\[\n\\psi(\\eta) = \\frac{1}{\\sqrt[\\leftroot{-2}\\uproot{3}4]{\\pi}}\\;e^{\\eta^2/2}\\;e^{ik\\eta}\n\\]\nパラメータ \\(\\eta\\) は時間 \\((t)\\) と ウェーブレットスケール \\((s)\\) の比率です。\nウェーブレットスケールを下げると時間軸方向の分解能が上がります。\nウェーブレットパラメータ \\(k\\) はモレー・ウェーブレットの振動数（山の数）を制御しています。\nよって、\\(k\\) を上げると、周波数分解能が上がります。では、biwavelet パッケージで次の波形を解析してみましょう。\\[\n\\begin{aligned}\ny &= \\sin(2 \\pi \\omega) \\\\\n\\omega &= 50 t^2 + 10\n\\end{aligned}\n\\]\nこの波形の角周波数 \\((\\omega)\\) は徐々に高くなります。biwavelet の wt() 関数でウェーブレット解析をします。\nwt() に渡すデータは行列としてわたしましょう。\n行列の1列目には時間情報、2列名には解析したい値です。\nFigure 14.1: \nこれはウェーブレット解析のパワーを示す図です。\nパワーの低いところは青色、パワーの高いところは赤色です。\n黒線は統計学的に優位な領域を示しています。\n赤色の部分が時間につれ、周期が上昇しています。\nウェーブレット解析で角周波数の傾向を十分抽出できたとおもいます。\n白くなっているところは cone influence (COI) です。\nCOIは解析アルゴリズムの精度が落ちているところを示しているので、\n示されたパワーは両端から \\(e^{-2}\\) に従って下がります。\n","code":"\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(gnnlab)\nlibrary(furrr)\nlibrary(biwavelet)\nft = function(t) {\n  f = 50 * t^2 + 10\n  sin(2 * pi * f )\n}\nz = tibble(t = seq(0, 1, length = 3000)) |> mutate(y = ft(t))\nggplot(z) + geom_line(aes(x = t, y = y)) +\n  scale_x_continuous(\"t\") +\n  scale_y_continuous(\"y\")\nwtout = wt(as.matrix(z), mother = \"morlet\")\nplot(wtout)"},{"path":"wavelet.html","id":"解析に使う関数","chapter":"14 Wavelet 解析","heading":"14.2 解析に使う関数","text":"Wavelet を求めるための関数です。\nコアは biwavelet パッケージの wt() 関数です。\nマザー・ウェーブレット (mother wavelet) はモレーウェーブレット (Morlet wavelet) に固定しています。\nwt() には DoG (derivative gaussian) と Paul ウェーブレットも使えます。関数を適応した場合、エラーがでたらスクリプトはエラーが発生したところで止まります。\n関数を possibly()のラッパーにとおせば、エラーがでたとき、NULL を返すようにする。\nこれで、スクリプトは止まりません。","code":"\ncalculate_wavelet = function(df, obs, tau = NULL, fs = 6, dB = TRUE) {\n  pad_cname = function(x, w) {\n    x = str_pad(x, width = w, pad = \"0\")\n    str_c(\"V\",x)\n  }\n  \n  # \n  N = nrow(df)\n  datetime  = df %>% pull({{tau}})\n  if(!near(day(datetime[N]), day(datetime[N-1]))) {\n    df = df |> slice(1:(N-1))\n    datetime  = df %>% pull({{tau}})\n  }\n  \n  hours = as.double(datetime - datetime[1], units = \"hours\")\n  observation = df %>% pull({{obs}})\n  wtout = wt(cbind(hours, observation), mother = \"morlet\")\n  \n  xval = wtout$t        # Vector of times\n  yval = wtout$period   # Vector of periods\n  sigma2 = wtout$sigma2 # Vector of variance of time series\n  coi = wtout$coi       # Vector of cone of influence\n  signif = wtout$signif # Matrix of significance levels\n  # Matrix of bias-corrected power\n  if(dB) {\n    Z = 10 * log10(abs(wtout$power.corr / sigma2))\n  } else {\n    Z = log2(abs(wtout$power.corr / sigma2))\n  }\n  \n  zlim = range(c(-1, 1) * max(Z))\n  Z[Z < zlim[1]] = zlim[1]\n  n = dim(Z)\n  tmp1 = signif |> as_tibble(.name_repair = ~pad_cname(1:n[2], nchar(n[2]) + 1)) |> \n    mutate(period = yval) |> pivot_longer(starts_with(\"V\"), values_to = \"signif\") |> \n    group_nest(name, .key = \"signif\") |> \n    arrange(name) |> \n    mutate(datetime)\n  tmp2 = Z |> as_tibble(.name_repair =  ~pad_cname(1:n[2], nchar(n[2]) + 1)) |> \n    mutate(period = yval) |> pivot_longer(starts_with(\"V\"), values_to = \"power\") |> \n    group_nest(name, .key = \"power\") |> \n    arrange(name) |> \n    mutate(datetime)\n  \n  full_join(tmp1, tmp2, by = c(\"name\", \"datetime\")) |> mutate(coi, hours = xval) |> \n    select(name, datetime, hours, coi, power, signif)\n}\ncalc_wt = possibly(calculate_wavelet, NULL)"},{"path":"wavelet.html","id":"補足関数","chapter":"14 Wavelet 解析","heading":"14.3 補足関数","text":"","code":"\nse = function(x, na.rm=FALSE) {\n  # 標準誤差\n  N = sum(!is.na(x))\n  sd(x, na.rm) / sqrt(N - 1)\n} \n\ndate_gnn = function(x) {\n  # ggplot の時間軸のlabel 関数\n  tmp0 = year(x)\n  tmp1 = month.abb[month(x)]\n  tmp2 = day(x)\n  tmp0[duplicated(tmp0)] = \"\"\n  \n  str_c(tmp2, \"\\n\" ,tmp1, \"\\n\",tmp0) \n}\n\ndate_gnn2 = function(x) {\n  # ggplot の時間軸のlabel 関数\n  tmp0 = year(x)\n  tmp1 = month.abb[month(x)]\n  tmp2 = day(x)\n  tmp0[duplicated(tmp0)] = \"\"\n  \n  str_c(tmp1, \"\\n\",tmp0) \n}\n# ggplot 用の関数\nlog2reverse = function(x) {-log2(x)}\nlog2reverseinv = function(x) {2^(-x)}\n\ncontiguous = function(df, tau, deltaT = 10) {\n  # 隣接データを確認するための関数\n  x = df |> pull({{tau}})\n  xout = as.double(x - lag(x), units = \"mins\")\n  df |> mutate(contig = xout) |> \n    mutate(group = as.numeric(!near(contig, deltaT))) |> \n    mutate(group = replace_na(group, 0)) |> \n    mutate(group = cumsum(group)) |> \n    mutate(group = factor(group))\n}"},{"path":"wavelet.html","id":"データの前処理","chapter":"14 Wavelet 解析","heading":"14.4 データの前処理","text":"データの読み込みは並列で行うので、map() じゃなくて future_map() を使います。\nread_onset() は 研究室の gnnlab パッケージの関数です。次はファイル名の処理をします。観測期間のデータを読み込んで、観測インターバルを設定します。観測インストールをつかて、データをフィルタにかけます。\nインストール以外のデータはここで外します。さらに、データの数を求めてフィルタにかけます。\n10分間隔で測定しているので、一日あたりに 144 のデータがあります。\n条件に合わないデータは外します。Wavelet 解析を実施するときは、データが隣接しているかを確認しましょう。\n隣接しているデータをグループ化してから解析をします。\nここでは、contiguous() 関数に datetime をわたして、必要な情報を加えます。グループ化が完了したら、wavelet 解析をします。ウェーブレット解析はとても重いので、研究室のサーバなら並列処理で解析します。では、future_map() を使って並列処理で解散をします。普通に逐次処理のときは、map() で実行しましょう。","code":"\nlabdatafolder = \"~/Lab_Data/kawatea/Oxygen\"\ndset = tibble(fnames = dir(labdatafolder, pattern = \"DO.*arikawa.*csv\", full = TRUE)) |> \n  filter(str_detect(fnames, \"calib\", negate = TRUE))\ndset = dset |>\n  mutate(data = future_map(fnames, read_onset)) \ndset\n#> # A tibble: 327 × 2\n#>    fnames                                           data    \n#>    <chr>                                            <list>  \n#>  1 /home/gnishihara/Lab_Data/kawatea/Oxygen/DO_02_… <tibble>\n#>  2 /home/gnishihara/Lab_Data/kawatea/Oxygen/DO_02_… <tibble>\n#>  3 /home/gnishihara/Lab_Data/kawatea/Oxygen/DO_03_… <tibble>\n#>  4 /home/gnishihara/Lab_Data/kawatea/Oxygen/DO_03_… <tibble>\n#>  5 /home/gnishihara/Lab_Data/kawatea/Oxygen/DO_03_… <tibble>\n#>  6 /home/gnishihara/Lab_Data/kawatea/Oxygen/DO_03_… <tibble>\n#>  7 /home/gnishihara/Lab_Data/kawatea/Oxygen/DO_03_… <tibble>\n#>  8 /home/gnishihara/Lab_Data/kawatea/Oxygen/DO_03_… <tibble>\n#>  9 /home/gnishihara/Lab_Data/kawatea/Oxygen/DO_03_… <tibble>\n#> 10 /home/gnishihara/Lab_Data/kawatea/Oxygen/DO_03_… <tibble>\n#> # … with 317 more rows\ndset = dset |> filter(str_detect(fnames, \"edge|sand\", negate = T)) |> \n  mutate(fnames = basename(fnames)) |> \n  mutate(fnames = str_remove(fnames, \".csv\")) |> \n  separate(fnames, into = c(\"type\", \"id\", \"location\", \"position\", \"surveydate\"))\ndset = dset |> unnest(data)\nsurveyperiod = read_csv(\"~/Lab_Data/kawatea/period_info_220422.csv\")\nsurveyperiod\n#> # A tibble: 185 × 5\n#>    location  start_date          end_date            comment\n#>    <chr>     <dttm>              <dttm>              <chr>  \n#>  1 arikawaa… 2017-04-09 00:00:00 2017-05-21 00:00:00 <NA>   \n#>  2 arikawag… 2017-04-09 00:00:00 2017-05-21 00:00:00 <NA>   \n#>  3 tainoura  2017-04-09 00:00:00 2017-05-21 00:00:00 <NA>   \n#>  4 arikawaa… 2017-05-25 00:00:00 2017-06-13 00:00:00 <NA>   \n#>  5 arikawag… 2017-05-25 00:00:00 2017-06-13 00:00:00 <NA>   \n#>  6 tainoura  2017-05-25 00:00:00 2017-06-13 00:00:00 <NA>   \n#>  7 arikawaa… 2017-07-20 00:00:00 2017-08-04 00:00:00 台風の…\n#>  8 arikawag… 2017-07-20 00:00:00 2017-08-04 00:00:00 台風の…\n#>  9 tainoura  2017-07-20 00:00:00 2017-08-18 00:00:00 <NA>   \n#> 10 arikawaa… 2017-08-09 00:00:00 2017-08-18 00:00:00 <NA>   \n#> # … with 175 more rows, and 1 more variable: remarks <chr>\nsurveyperiod = surveyperiod |> mutate(interval = interval(start = start_date, end = end_date))\nsurveyperiod\n#> # A tibble: 185 × 6\n#>    location  start_date          end_date            comment\n#>    <chr>     <dttm>              <dttm>              <chr>  \n#>  1 arikawaa… 2017-04-09 00:00:00 2017-05-21 00:00:00 <NA>   \n#>  2 arikawag… 2017-04-09 00:00:00 2017-05-21 00:00:00 <NA>   \n#>  3 tainoura  2017-04-09 00:00:00 2017-05-21 00:00:00 <NA>   \n#>  4 arikawaa… 2017-05-25 00:00:00 2017-06-13 00:00:00 <NA>   \n#>  5 arikawag… 2017-05-25 00:00:00 2017-06-13 00:00:00 <NA>   \n#>  6 tainoura  2017-05-25 00:00:00 2017-06-13 00:00:00 <NA>   \n#>  7 arikawaa… 2017-07-20 00:00:00 2017-08-04 00:00:00 台風の…\n#>  8 arikawag… 2017-07-20 00:00:00 2017-08-04 00:00:00 台風の…\n#>  9 tainoura  2017-07-20 00:00:00 2017-08-18 00:00:00 <NA>   \n#> 10 arikawaa… 2017-08-09 00:00:00 2017-08-18 00:00:00 <NA>   \n#> # … with 175 more rows, and 2 more variables:\n#> #   remarks <chr>, interval <Interval>\nints = surveyperiod |> pull(interval)\ndset = dset |> filter(datetime %within% as.list(ints))\ndset = dset |> \n  mutate(date = as_date(datetime)) |> \n  group_by(date, location, position) |> \n  filter(near(n(), 144))\ndset\n#> # A tibble: 1,239,696 × 9\n#> # Groups:   date, location, position [8,609]\n#>    type  id    location      position surveydate\n#>    <chr> <chr> <chr>         <chr>    <chr>     \n#>  1 DO    02    arikawagaramo 0m       170523    \n#>  2 DO    02    arikawagaramo 0m       170523    \n#>  3 DO    02    arikawagaramo 0m       170523    \n#>  4 DO    02    arikawagaramo 0m       170523    \n#>  5 DO    02    arikawagaramo 0m       170523    \n#>  6 DO    02    arikawagaramo 0m       170523    \n#>  7 DO    02    arikawagaramo 0m       170523    \n#>  8 DO    02    arikawagaramo 0m       170523    \n#>  9 DO    02    arikawagaramo 0m       170523    \n#> 10 DO    02    arikawagaramo 0m       170523    \n#> # … with 1,239,686 more rows, and 4 more variables:\n#> #   datetime <dttm>, mgl <dbl>, temperature <dbl>,\n#> #   date <date>\ndset = dset |> ungroup() |> \n  mutate(datetime = floor_date(datetime, \"10 mins\")) |> \n  group_nest(location, position, id) |> \n  mutate(data = map(data, contiguous, tau = datetime)) |> \n  unnest(data)\nnumber_of_cpu_cores = parallel::detectCores()\nplan(multisession, workers = 8)\nwtout = dset |> \n  arrange(datetime) |> \n  group_nest(group, location, id, position) |> \n  mutate(wtout = future_map(data, \\(df) {\n    df |> calc_wt(temperature, datetime)\n  }))\nwtout = dset |> \n  arrange(datetime) |> \n  group_nest(group, location, id, position) |> \n  mutate(wtout = map(data, \\(df) {\n    df |> calc_wt(temperature, datetime)\n  }))"},{"path":"wavelet.html","id":"wavelet-の図","chapter":"14 Wavelet 解析","heading":"14.5 Wavelet の図","text":"ウェーブレットの図も関数を設計して、作ります。\nwavelet_plot() は作図用の関数です。\nこれは、map() を通して作図します。ここで作図をします。そのままコードを実行すると RStudio には図のアウトプットはないです。\n図を見るためには、ファイルに保存したほうがいい。合計 324 のファイルを書き出すことになりそうなので、\nフォルダを作って保存します。ここでファイル名と保存するための関数をつくります。\nファイルはPDFとして保存するが、同時にPDFをPNGに変換します。\nPDFを変換すると、確実に選んだフォントが埋め込まれるので、\nPNGファイルのフォントも綺麗です。\nファイルの変換は magick パッケージをつかいます。\nFigure 14.2: \n有川湾鯨見山地先におけるガラモ場の水温に対するウェーブレット解析。\n観測期間は2018年5月17日から6月17日でしました。\n水温は海底で記録しました。\nパワーの強い所は明るい色（黄色）で示していて、パワーの弱い所は暗い色（青色）で示しています。\n白線はCOIを示しています。\n5月17日から25日、5月31日から6月2日、6月9日,　6月12日から17日の期間には強い24時間の周期があります。\nところどころ強い12時間の周期もあります。12時間の周期は潮汐と関係していると考えますね。\n12時間より短い周期でも比較的に強いパワーが示されています。\n","code":"\nwavelet_plot = function(wtout) {\n  ylabel = \"Period (hrs)\"\n  xlabel = \"Date\"\n  x = select(wtout, datetime, power) |> unnest(power)\n  s = select(wtout, datetime, signif) |> unnest(signif)\n  rng = x |> pull(period) |> range()\n  ggplot() +\n    geom_tile(aes(x = datetime,\n                  y = period, \n                  fill = power),\n              data = x) +\n    geom_contour(aes(x = datetime,\n                     y = period,\n                     z = signif),\n                 data = s,\n                 color = \"black\",\n                 size = 1, \n                 breaks = 1) +\n    geom_line(aes(x = datetime,\n                  y = coi),\n              data = wtout,\n              color = \"white\") +\n    geom_hline(yintercept = c(1, 6, 12, 24),\n               linetype = \"dashed\", color = \"grey50\") +\n    scale_y_continuous(ylabel,\n                       trans = scales::trans_new(\"log2reverse\",\n                                                 log2reverse,\n                                                 log2reverseinv,\n                                                 domain = c(0, Inf)),\n                       limits = rev(rng),\n                       breaks = 2^log2(c(1, 4, 6, 12, 16, 24, 64)),\n                       expand = expansion())  +\n    scale_x_datetime(xlabel, \n                     date_breaks = \"2 days\",\n                     labels = date_gnn)  +\n    guides(fill = \"none\") + \n    scale_fill_viridis_c()\n}\nwtplots = wtout |> \n  select(group, wtout, location, position, id) |> \n  mutate(ggplot = map(wtout, wavelet_plot))\nplots = wtplots$ggplot\nlength(plots)\n#> [1] 324\ndname = \"_wavelet_plots\"\nif(!dir.exists(dname)) {\n  dir.create(dname)\n}\nsave_wavelets = function(l,p,d,g,i,plot) {\n  pdfname = str_c(dname, \"/\", l, \"_\", p, \"_\", d, \"_\", g,\"_\", i, \".pdf\")\n  pngname = str_replace(pdfname, \"pdf\", \"png\")\n  ggsave(pdfname, plot = plot, width = 300, height = 200, units = \"mm\")\n  magick::image_read_pdf(pdfname, density = 300) |> \n    magick::image_trim() |> magick::image_write(pngname)\n}\n\nwtplots |> \n  mutate(period = map_chr(wtout, \\(x) {\n    z = x |> pull(datetime) |> as_date() |> range()\n    z = str_remove_all(z, \"-\")\n    str_c(z[1], \"_\", z[2])\n  })) |> \n  mutate(out = pmap(list(location, position, period, group, id, ggplot), save_wavelets))"},{"path":"wavelet.html","id":"snr-の求め方","chapter":"14 Wavelet 解析","heading":"14.6 SNR の求め方","text":"日周期 (diurnal) と高周期 (high) の比率を求めて、一日内の水温の安定性を調べてみましょう。\nウェーブレットで検出した周期情報を High (0 ~ 7 hr), Tidal (7 = 13 hrs), Diurnal (13 ~ 25 hrs), Low (> 25 hrs) に分類します。\n分離したら、一日あたりの平均パワー (power) と平均値の 95% 信頼区間を求めます。\nこの解析はまだ未熟ですが、もっといい方法に気づいたら紹介します。次に、wtout2 に隠れている tibble を縦長から横長に変換します。\nつづいて、SNRを求めますが、power はログスケールで求めたので、\nSNR比は snr = Diurnal - High です。SNR比を date, location, position ごとにまとめます。SNR比に2020年あたりに怪しい値はあるが、position ごとに傾向有るのかな？\nFigure 14.3: SNRは日周期と高周期の比率です。\nSNRの傾向を解析して、図に追加します。\nlocationとposition 間の比較に興味がないので、次のようなモデルを当てはめます。解析の結果一般線形モデルのF検定の結果、すべてのモデルのP値は 0.05 より大きいですね。モデル係数の結果は broom パッケージの tidy() 関数で確認できます。date はモデル傾きのパラメータです。\narikawaamamo 0m 以外の date 係数は正の値をとっていますが、\nWald’s 検定の結果、すべての P値は 0.05 より大きいです。帰無仮説検定論によると、0 との統計学的な有意差がなかったので、\n係数が 0 ではないという仮説は棄却できない。それにしても、とりあえずデータを図に追加してみよう。\nFigure 14.4: SNRの傾向に線形モデルを当てはめたが、線形モデルのF検定に統計学的に有意な結果はありませんでした。\n","code":"\ncalculate_mean_power = function(wt) {\n    wt |> \n      select(datetime, power) |> \n      unnest(power) |> \n      mutate(datetime = floor_date(datetime, \"day\")) |> \n      ungroup() |> \n      mutate(ftype = cut(period, breaks = c(-Inf, 7, 13, 25, Inf),\n                         labels = c(\"High\", \"Tidal\", \"Diurnal\", \"Low\"))) |> \n      group_by(datetime, ftype) |> \n      summarise(across(power, list(mean =mean, se = se)), .groups = \"drop\") |> \n      mutate(lower = power_mean - 1.96*power_se,\n             upper = power_mean + 1.96*power_se)\n}\n\nwtout = wtout |> mutate(wtout2 = map(wtout, calculate_mean_power))\ncalculate_snr = function(wt)  {\n  wt |> ungroup() |> \n    select(datetime, ftype, power_mean) |> \n    arrange(datetime) |> \n    pivot_wider(names_from =ftype, values_from = power_mean) |> \n    mutate(snr = Diurnal - High)\n}\nwtout = wtout |> \n  mutate(wtout3 = map(wtout2, calculate_snr))\nsnrdata = \n  wtout |> select(wtout3, location, position) |> unnest(wtout3) |> \n  mutate(date = floor_date(datetime, \"month\")) |> \n  group_by(date, location, position) |> \n  summarise(snr = mean(snr), .groups = \"drop\")\nylabel = \"Period (h)\"\nxlabel = \"Date\"\nggplot(snrdata) + \n  geom_point(aes(x = date, y = snr, color = position)) +\n  facet_wrap(vars(location))\nfit_model = function(df) {\n  lm(snr ~ date, data = df)\n}\nsnrdata |> \n  mutate(date = as_date(date)) |> \n  group_nest(location, position) |> \n  mutate(model = map(data, fit_model)) |> \n  mutate(summary = map(model, broom::glance)) |> \n  unnest(summary)\n#> # A tibble: 7 × 16\n#>   location   position     data model r.squared adj.r.squared\n#>   <chr>      <chr>    <list<t> <lis>     <dbl>         <dbl>\n#> 1 arikawaam… 0m       [46 × 2] <lm>    0.0131       -0.0128 \n#> 2 arikawaam… 1m       [46 × 2] <lm>    0.0210       -0.00623\n#> 3 arikawaam… 2m       [41 × 2] <lm>    0.0357        0.00731\n#> 4 arikawaam… surface  [46 × 2] <lm>    0.0196       -0.00271\n#> 5 arikawaga… 0m       [59 × 2] <lm>    0.00124      -0.0163 \n#> 6 arikawaga… 1m       [60 × 2] <lm>    0.00538      -0.0124 \n#> 7 arikawaga… surface  [60 × 2] <lm>    0.00406      -0.0151 \n#> # … with 10 more variables: sigma <dbl>, statistic <dbl>,\n#> #   p.value <dbl>, df <dbl>, logLik <dbl>, AIC <dbl>,\n#> #   BIC <dbl>, deviance <dbl>, df.residual <int>,\n#> #   nobs <int>\nsnrdata |> \n  mutate(date = as_date(date)) |> \n  group_nest(location, position) |> \n  mutate(model = map(data, fit_model)) |> \n  mutate(summary = map(model, broom::tidy)) |> \n  unnest(summary) \n#> # A tibble: 14 × 9\n#>    location position     data model term  estimate std.error\n#>    <chr>    <chr>    <list<t> <lis> <chr>    <dbl>     <dbl>\n#>  1 arikawa… 0m       [46 × 2] <lm>  (Int…  3.80e+1 20.8     \n#>  2 arikawa… 0m       [46 × 2] <lm>  date  -8.03e-4  0.00113 \n#>  3 arikawa… 1m       [46 × 2] <lm>  (Int… -1.19e+0 25.8     \n#>  4 arikawa… 1m       [46 × 2] <lm>  date   1.23e-3  0.00140 \n#>  5 arikawa… 2m       [41 × 2] <lm>  (Int… -4.24e-1 19.4     \n#>  6 arikawa… 2m       [41 × 2] <lm>  date   1.19e-3  0.00106 \n#>  7 arikawa… surface  [46 × 2] <lm>  (Int…  2.07e+0 21.4     \n#>  8 arikawa… surface  [46 × 2] <lm>  date   1.09e-3  0.00116 \n#>  9 arikawa… 0m       [59 × 2] <lm>  (Int…  1.53e+1 14.7     \n#> 10 arikawa… 0m       [59 × 2] <lm>  date   2.16e-4  0.000811\n#> 11 arikawa… 1m       [60 × 2] <lm>  (Int…  1.12e+1 14.5     \n#> 12 arikawa… 1m       [60 × 2] <lm>  date   4.40e-4  0.000799\n#> 13 arikawa… surface  [60 × 2] <lm>  (Int…  1.51e+1 13.6     \n#> 14 arikawa… surface  [60 × 2] <lm>  date   3.45e-4  0.000750\n#> # … with 2 more variables: statistic <dbl>, p.value <dbl>\nreturn_fit = function(data, model) {\n  nd = data |> expand(date)\n  bind_cols(nd, predict(model, newdata = nd, se.fit = TRUE) |> as_tibble()) |> \n    mutate(lower = fit - 1.96*se.fit,\n           upper = fit + 1.96*se.fit)\n}\n\nsnrdata = snrdata |> \n  mutate(date = as_date(date)) |> \n  group_nest(location, position) |> \n  mutate(model = map(data, fit_model)) |>\n  mutate(ndata = map2(data,model, return_fit))\nggplot() + \n  geom_line(aes(x = date, y = fit, color = position), data = snrdata |> unnest(ndata)) + \n  geom_point(aes(x = date, y = snr, color = position), data = snrdata |> unnest(data)) + \n  facet_wrap(vars(location))"}]
