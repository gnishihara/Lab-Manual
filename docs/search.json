[{"path":"index.html","id":"このマニュアルについて","chapter":"このマニュアルについて","heading":"このマニュアルについて","text":"解析の背景については講義1で紹介した内容を参考にしてください。\n章ごとに解析を紹介しています。\nR コードが記述されている部分はコードブロックまたはコードチャンクと呼びます。章ごとのコードは独立していないので、注意してください。\nコードをそのままコピー・ペーストすると、うまく動かないことがあります2。また、このマニュアルは R 環境における解析のコードを説明するためです3。\nこのマニュアルだけだと、統計解析はできるが、統計学の理解には不十分です。\n最後に、マニュアルは随時更新するので、タイトルにあるバージョン番号を時々確認してね。\nわからないことまたは間違いがあれば、メールで連絡ください。研究室用のデータは RStudio の ~/Lab_Data/ に入っています。","code":""},{"path":"index.html","id":"マニュアルの作り方","chapter":"このマニュアルについて","heading":"マニュアルの作り方","text":"HTMLバージョンのマニュアルは RStudio の Build パネルで組み立てます。Build パネルに移動するBuild Book のアイコンをクリックして、bookdown::bs4_book を選択するコンソールから組み立てる場合はつぎのコードを実行しましょう。作業しながら、マニュアルのプレビュー版はみれます。\n複数人でのプレビューはまだしたことないので、うまくいくかはわからない。","code":"\nbookdown::render_book()\nbookdown::serve_book()"},{"path":"data-input.html","id":"data-input","chapter":"1 データの読み込み","heading":"1 データの読み込み","text":"","code":""},{"path":"data-input.html","id":"必要なパッケージ","chapter":"1 データの読み込み","heading":"1.1 必要なパッケージ","text":"","code":"\nlibrary(tidyverse)\nlibrary(readxl)"},{"path":"data-input.html","id":"データの確認","chapter":"1 データの読み込み","heading":"1.2 データの確認","text":"データは環境省の「瀬戸内海における藻場・干潟分布状況調査（概要）」からまとめました。\nもとのファイルは環境省平成３０年９月スライドデッキ からダウンロードできます。\nXLSXファイルは readxl パッケージの read_xlsx() 関数で読み込みます。\nでは、XLSXファイルに存在するシートの確認をしましょう4。excel_sheets() を実行したら、ファイルから 2つのシート名が返ってきました。\n読み込む前に、それぞれのシートの構造を確認しましょう (Fig. 1.1 1.2)。\n確認はスプレッドシートソフト（MS Office、 Google Sheets、 Open Office、 Apple Numbers、 など）で行います。\nFigure 1.1: 瀬戸内海藻場データ.xlsx の FY1990 シートに入力されているデータは縦長の形式です。\nFY1990 のデータの構造は縦長なので、読み込みは比較的に楽です。\nそれぞれの変数は一つの列5に入力されているから、読み込みが簡単です。\nFigure 1.2: 瀬戸内海藻場データ.xlsx の FY2018 シートに入力されているデータは横長の形式です。\nFY2018 のデータの構造は横長です。\nデータは海藻と海草にわけられ、それぞれの変数じゃなくて、それぞれの場所の値を列に入力されています。\nこの用なデータの読み込みは手間がかかります6。","code":"\nrootdatafolder = rprojroot::find_rstudio_root_file(\"Data/\")\nfilename = '瀬戸内海藻場データ.xlsx'\npath = str_c(rootdatafolder, filename)\nexcel_sheets(path) # シート名を確認する\n#> [1] \"FY1990\" \"FY2018\""},{"path":"data-input.html","id":"データを読み込む","chapter":"1 データの読み込み","heading":"1.3 データを読み込む","text":"では、FY1990 シートのデータを読み込みます。\nここでシートから読み込むセルの範囲を指定します。データは tibble として読み込まれました。\nデータに大きな問題がなければ、各列の型・タイプ (type)7 は自動的に設定されます。調査海域 の列は <chr> : character, 文字列海藻 の列は <dbl>: double, ダブル・数値・実数海草 の列は <dbl>: double, ダブル・数値・実数変数名が日本語の場合、コードが書きづらくなったり、バグの原因になります。\n最初から英語表記にするのが合理的ですが、R環境内で名前を変換することは難しくないです。\nとりあえず d19 の内容をみましょう。FY2018 シートの読み込みは、海藻と海草ごとにする必要があります。\n読み込んだ後に、データを縦長に変換し、2 つの tibble を縦に結合します。最初のセル範囲を読み込んで ファイルのコンテンツを seaweed に書き込んだら、RNG を次のセル範囲に書き換えます。\nデータは同じシートにあるので、SHEET を変更したり、新たに定義する必要はありません。seaweed の内容は次のとおりです。seagrass の内容は次のとおりです。NA は Available の諸略です。\nRの場合、存在しないデータ (欠損値) は NA になります。","code":"\nRNG = \"A4:C27\"   # セルの範囲\nSHEET = \"FY1990\" # シート名\nd19 = read_xlsx(path, sheet = SHEET, range = RNG)\nd19 # FY1990 データの内容\n#> # A tibble: 23 × 3\n#>    調査海域  海藻  海草\n#>    <chr>    <dbl> <dbl>\n#>  1 東部        55    14\n#>  2 東部       128    62\n#>  3 東部        86     0\n#>  4 東部        87     8\n#>  5 東部       214    54\n#>  6 東部       140    57\n#>  7 中部        30    45\n#>  8 中部         5   623\n#>  9 中部       460   886\n#> 10 中部        51   180\n#> # … with 13 more rows\nRNG = \"A6:C15\"   # 海藻データのセル範囲\nSHEET = \"FY2018\" # シート名\nseaweed = read_xlsx(path, sheet = SHEET, range = RNG)\nRNG = \"E6:G15\"   # 海草データのセル範囲\nseagrass = read_xlsx(path, sheet = SHEET, range = RNG)\nseaweed\n#> # A tibble: 9 × 3\n#>    東部  中部  西部\n#>   <dbl> <dbl> <dbl>\n#> 1    14    62   108\n#> 2    93   838     0\n#> 3    12   933     0\n#> 4     8   193     0\n#> 5   444   235     0\n#> 6    85   150   126\n#> 7    NA   283     0\n#> 8    NA     3     0\n#> 9    NA    12    NA\nseagrass\n#> # A tibble: 9 × 3\n#>    東部  中部  西部\n#>   <dbl> <dbl> <dbl>\n#> 1    71    63   430\n#> 2   145     5   231\n#> 3    94   674   404\n#> 4    82    69  2005\n#> 5    49    21  1094\n#> 6   100   141    54\n#> 7    NA   635   221\n#> 8    NA    62   182\n#> 9    NA   440    NA"},{"path":"data-wrangling.html","id":"data-wrangling","chapter":"2 データの処理","heading":"2 データの処理","text":"","code":""},{"path":"data-wrangling.html","id":"必要なパッケージ-1","chapter":"2 データの処理","heading":"2.1 必要なパッケージ","text":"","code":"\nlibrary(tidyverse)\nlibrary(readxl)"},{"path":"data-wrangling.html","id":"データの読み込み","chapter":"2 データの処理","heading":"2.2 データの読み込み","text":"データの読み込みを参考に：1 。","code":"\nrootdatafolder = rprojroot::find_rstudio_root_file(\"Data/\")\nfilename = '瀬戸内海藻場データ.xlsx'\npath = str_c(rootdatafolder, filename)\n\n# fy1990 の処理\nRNG = \"A4:C27\"   # セルの範囲\nSHEET = \"FY1990\" # シート名\nd19 = read_xlsx(path, sheet = SHEET, range = RNG)\n\n# fy2018の処理\nRNG = \"A6:C15\"   # 海藻データのセル範囲\nSHEET = \"FY2018\" # シート名\nseaweed = read_xlsx(path, sheet = SHEET, range = RNG)\nRNG = \"E6:G15\"   # 海草データのセル範囲\nseagrass = read_xlsx(path, sheet = SHEET, range = RNG)"},{"path":"data-wrangling.html","id":"データの処理","chapter":"2 データの処理","heading":"2.3 データの処理","text":"データの形（横長から縦長）を変えたいとき、tidyverse の pivot_wider() と pivot_longer() を使うと楽です。では、FY2018シートの構造をFY1990シートと同じようにします。\n横長のデータを縦長に変換するには、pivot_longer() を使います。\nこれは MS Excel の ピボットテーブル (pivot table) の機能とにています。ここでの重要なポイントは、必ずピボットしたい列を指定することです。\nこのとき、すべての列をピボットしたいので、pivot_longer() には cols = everything() をわたします。\nピボットされた seaweed は次のとおりです。\n|> print(n = Inf) をすると、tibble 内容をすべて表示できます8。seagrass も同じように処理しました。では、次は seaweed と seagrass を縦に結合することです。\n複数の tibble を縦に結合するための関数は bind_rows() です。seaweed に seaweed、seagrass に seagrass を渡します。\nさらに、seaweed と seagrass を type 変数に書き込みます。実は、次のように bind_rows() を実行できますが、データの構造は不都合になります。\nどちらも 2つの tibble を縦に結合してくれますが、結果は全く違います。\nコードと結果の違いをよく確認して、その違いを理解しましょう。では、d20 の type ごとの value 変数を横にならべたら、d19 と全く同じ構造になります。このように処理したら、Warning message がでます。\nWarning (ウォーニング) は Error (エラー) ほどの問題ではないので、コードは実行されています。\nError の場合はコードは実行されません。\nこの Warning で values uniquely identified と返ってきました。\nつまり、各サンプルの値は、区別することができないと意味します。\nこのデータの場合は、区別しなくても問題ないので、このまま解析を続きます。\nそれにしても、seaweed と seagrass の変数 type は <list> です。\nそれぞれの変数の要素に <dbl [9]> と記述されています。\n各要素に 9つの値が入力されていると意味します。\n研究室では、seaweed と seagrass 変数は nested (ネスト) または、「たたまれている」といいます。\nでは、この２つの変数を unnest (アンネスト) します。さらに、name を site (調査海域) に変更します。最後に、d20 の NA データを外します。これで、d20 と d19 はほぼ同じ構造です。\n次のコードブロックで、d19 の変数名をrename() を用いて英語に変えます。\n日本語の変数名は使いづらくて、バグの原因になることが多いので名前を変更します。解析をするまえに、site を要因 (因子) として設定します。\nlevels = c('東部', '中部', '西部') は因子の順序を指定するためです。\n指定しなかった場合、アルファベット順やあいうえお順になります。","code":"\n# %>% と |> はパイプ演算子とよびます。\n# |> はR 4.1.0 から追加された、ネーティブのパイプ演算子です。\n# RStudio の設定を変えなければ、CTRL+SHIFT+M をしたら、%>% が入力されるとおもいます。\n# ネーティブパイプを使いたいなら、Tools -> Global Options -> Code に\n#   いって、Use native pipe operator のボックスにチェックを入れてください。\n# seaweed = seaweed %>% pivot_longer(cols = everything())\nseaweed = seaweed |> pivot_longer(cols = everything())\nseaweed |> print(n = Inf)\n#> # A tibble: 27 × 2\n#>    name  value\n#>    <chr> <dbl>\n#>  1 東部     14\n#>  2 中部     62\n#>  3 西部    108\n#>  4 東部     93\n#>  5 中部    838\n#>  6 西部      0\n#>  7 東部     12\n#>  8 中部    933\n#>  9 西部      0\n#> 10 東部      8\n#> 11 中部    193\n#> 12 西部      0\n#> 13 東部    444\n#> 14 中部    235\n#> 15 西部      0\n#> 16 東部     85\n#> 17 中部    150\n#> 18 西部    126\n#> 19 東部     NA\n#> 20 中部    283\n#> 21 西部      0\n#> 22 東部     NA\n#> 23 中部      3\n#> 24 西部      0\n#> 25 東部     NA\n#> 26 中部     12\n#> 27 西部     NA\nseagrass = seagrass |> pivot_longer(cols = everything())\nd20 = bind_rows(seaweed = seaweed, seagrass = seagrass, .id = \"type\")\nd20　# FY2018 データ\n#> # A tibble: 54 × 3\n#>    type    name  value\n#>    <chr>   <chr> <dbl>\n#>  1 seaweed 東部     14\n#>  2 seaweed 中部     62\n#>  3 seaweed 西部    108\n#>  4 seaweed 東部     93\n#>  5 seaweed 中部    838\n#>  6 seaweed 西部      0\n#>  7 seaweed 東部     12\n#>  8 seaweed 中部    933\n#>  9 seaweed 西部      0\n#> 10 seaweed 東部      8\n#> # … with 44 more rows\nbind_rows(seaweed, seagrass)\n#> # A tibble: 54 × 2\n#>    name  value\n#>    <chr> <dbl>\n#>  1 東部     14\n#>  2 中部     62\n#>  3 西部    108\n#>  4 東部     93\n#>  5 中部    838\n#>  6 西部      0\n#>  7 東部     12\n#>  8 中部    933\n#>  9 西部      0\n#> 10 東部      8\n#> # … with 44 more rows\nd20 = d20 |> pivot_wider(id_cols = name,\n                   names_from = type,\n                   values_from = value)\nd20 = d20 |> unnest(c(seaweed, seagrass))\nd20\n#> # A tibble: 27 × 3\n#>    name  seaweed seagrass\n#>    <chr>   <dbl>    <dbl>\n#>  1 東部       14       71\n#>  2 東部       93      145\n#>  3 東部       12       94\n#>  4 東部        8       82\n#>  5 東部      444       49\n#>  6 東部       85      100\n#>  7 東部       NA       NA\n#>  8 東部       NA       NA\n#>  9 東部       NA       NA\n#> 10 中部       62       63\n#> # … with 17 more rows\nd20 = d20 |> rename(site = name)\nd20\n#> # A tibble: 27 × 3\n#>    site  seaweed seagrass\n#>    <chr>   <dbl>    <dbl>\n#>  1 東部       14       71\n#>  2 東部       93      145\n#>  3 東部       12       94\n#>  4 東部        8       82\n#>  5 東部      444       49\n#>  6 東部       85      100\n#>  7 東部       NA       NA\n#>  8 東部       NA       NA\n#>  9 東部       NA       NA\n#> 10 中部       62       63\n#> # … with 17 more rows\nd20 = d20 |> drop_na() # NAを外す\nd20\n#> # A tibble: 23 × 3\n#>    site  seaweed seagrass\n#>    <chr>   <dbl>    <dbl>\n#>  1 東部       14       71\n#>  2 東部       93      145\n#>  3 東部       12       94\n#>  4 東部        8       82\n#>  5 東部      444       49\n#>  6 東部       85      100\n#>  7 中部       62       63\n#>  8 中部      838        5\n#>  9 中部      933      674\n#> 10 中部      193       69\n#> # … with 13 more rows\nd19 = d19 |> \n  rename(site = 調査海域, seaweed = 海藻, seagrass = 海草) |> \n  mutate(site = factor(site, levels = c('東部', '中部', '西部')))\nd20 = d20 |> \n  mutate(site = factor(site, levels = c('東部', '中部', '西部')))"},{"path":"data-logger-input.html","id":"data-logger-input","chapter":"3 ロガーからの読み込み","heading":"3 ロガーからの読み込み","text":"","code":""},{"path":"data-summary.html","id":"data-summary","chapter":"4 データの集計","heading":"4 データの集計","text":"","code":""},{"path":"data-summary.html","id":"必要なパッケージ-2","chapter":"4 データの集計","heading":"4.1 必要なパッケージ","text":"","code":"\nlibrary(tidyverse)\nlibrary(readxl)"},{"path":"data-summary.html","id":"データの準備","chapter":"4 データの集計","heading":"4.2 データの準備","text":"データの読み込みは章1 を参考にしてください。データの処理は章2 を参考にしてください。","code":"\nrootdatafolder = rprojroot::find_rstudio_root_file(\"Data/\")\nfilename = '瀬戸内海藻場データ.xlsx'\npath = str_c(rootdatafolder, filename)\n\n# fy1990 の処理\nRNG = \"A4:C27\"   # セルの範囲\nSHEET = \"FY1990\" # シート名\nd19 = read_xlsx(path, sheet = SHEET, range = RNG)\n\n# fy2018の処理\nRNG = \"A6:C15\"   # 海藻データのセル範囲\nSHEET = \"FY2018\" # シート名\nseaweed = read_xlsx(path, sheet = SHEET, range = RNG)\nRNG = \"E6:G15\"   # 海草データのセル範囲\nseagrass = read_xlsx(path, sheet = SHEET, range = RNG)\nseaweed = seaweed |> pivot_longer(cols = everything())\nseagrass = seagrass |> pivot_longer(cols = everything())\n\nd20 = bind_rows(seaweed = seaweed, seagrass = seagrass, .id = \"type\")\nd20 = d20 |> pivot_wider(id_cols = name,\n                   names_from = type, values_from = value, \n                   values_fn = \"list\")\nd20 = d20 |> unnest(c(seaweed, seagrass)) |> rename(site = name) |> drop_na()\n\n\nd19 = d19 |> \n  rename(site = 調査海域, seaweed = 海藻, seagrass = 海草) |> \n  mutate(site = factor(site, levels = c('東部', '中部', '西部')))\nd20 = d20 |> \n  mutate(site = factor(site, levels = c('東部', '中部', '西部')))"},{"path":"data-summary.html","id":"記述統計量","chapter":"4 データの集計","heading":"4.3 記述統計量","text":"一般的には、数値データは2つの値にまとめられます。Measures central tendency: 位置の尺度（平均値、中央値、最頻値）Measures dispersion: ばらつきの尺度（四分位数間範囲、平均絶対偏差、中央絶対偏差、範囲、標準偏差、分散）まず、サイコロの関数を定義してから、位置のの尺度とばらつきの尺度を求めましょう。2つのサイコロを10回投げます。サイコロの結果は次のとおりです。平均値, mean, average中央値, メディアン, median最頻値, モード, mode再頻値を求める。専用の関数がないので、ここで定義して使用します。分散, variance分散と標準偏差はもっとも使われるばらつきの尺度です。\\[\nVar(x) = \\frac{1}{N-1}\\sum_{n = 1}^N(x_n - \\overline{x})^2\n\\]標準偏差, standard deviation標準偏差は分散の平方根です。四分位数間範囲, inter-quantile range, IQR四分位数間範囲は第2四分位数と第3四分位数の距離です。\n箱ひげ図の箱の高さが四分位数間範囲です。範囲, range範囲はデータを最大値と最小値の距離です。平均絶対偏差, mean absolute deviation平均絶対偏差は、データと平均値との距離の平均値です。\\[\n\\text{MAD} = \\frac{1}{N} \\sum_{n = 1}^N |x_n - \\overline{x}|\n\\]\n\\(\\overline{x}\\) は平均値、\\(N\\)はデータ数です。\n専用の関数がないので、これも定義します。中央絶対偏差, median absolute deviation, MAD中央絶対偏差は、データと中央値との距離の中央値です。\\[\n\\text{MAD} = median(|x_i - \\tilde{x}|)\n\\]\n\\(\\tilde{x}\\) は \\(x\\) の中央値です。標準偏差として使う場合は、\\[\n\\hat{\\sigma}\\equiv s = k \\cdot \\text{MAD} = \\frac{1}{\\Phi^{-1}(3/4)}\\cdot \\text{MAD}\n\\]\nこの積は標準偏差のロバスト推定量 (robust estimator) といいます。\nロバスト推定量は外れ値に強く影響されないのが特徴です。tibbleデータの集計は次のようにします。\n全データの集計の場合は、tibble　を summarise 関数に渡します。ここではseaweed と seagrass の平均値を求めています。across() 関数を使えば、コードは諸略できます。\nacross() に渡したそれぞれのベクトル（列）に mean 関数を適応しています。mean() と sd() を同時に適応できます。ところが帰ってくる結果をみて、平均値と標準偏差の区別ができないので、次のようにコードをくみましょう。では、平均値、標準偏差、平均絶対偏差を求めています。site ごとに集計したいとき、group_by() 関数を使って、データのグループ化してから、それぞれの関数を適応します。","code":"\n# n: サイコロの数\n# s: サイコロの面の数\nroll_dice = function(n = 1, s = 6) {\n  face = 1:s\n  sum(sample(x = face, size = n, replace = TRUE))\n}\nset.seed(2022) #疑似乱数を固定することで、再現性のあるシミュレーションができる。\nx = replicate(10, roll_dice(n = 2, s = 6))\nx\n#>  [1]  7  9 10 10  4 11  8  6  4  4\nmean(x)\n#> [1] 7.3\nmedian(x)\n#> [1] 7.5\nmode = function(x) {\n  u = unique(x)\n  matched = tabulate(match(x,u))\n  u[near(matched, max(matched))]\n}\nmode(x)\n#> [1] 4\nvar(x)\n#> [1] 7.344444\nsd(x)\n#> [1] 2.710064\ndiff(quantile(x, c(0.25, 0.75)))\n#>  75% \n#> 5.25\ndiff(range(x))\n#> [1] 7\nmean_absolute_deviation = function(x) {\n  xbar = mean(x)\n  xout = abs(x - xbar)\n  mean(xout)\n}\nmean_absolute_deviation(x)\n#> [1] 2.3\nmad(x, constant = 1)\n#> [1] 2.5\nmad(x)\n#> [1] 3.7065\nd19 |> \n  summarise(seaweed = mean(seaweed),\n            seagrass = mean(seagrass))\n#> # A tibble: 1 × 2\n#>   seaweed seagrass\n#>     <dbl>    <dbl>\n#> 1    312.     127.\nd19 |> summarise(across(c(seaweed, seagrass), mean))\n#> # A tibble: 1 × 2\n#>   seaweed seagrass\n#>     <dbl>    <dbl>\n#> 1    312.     127.\nd19 |> summarise(across(c(seaweed, seagrass), list(mean, sd)))\n#> # A tibble: 1 × 4\n#>   seaweed_1 seaweed_2 seagrass_1 seagrass_2\n#>       <dbl>     <dbl>      <dbl>      <dbl>\n#> 1      312.      439.       127.       227.\nd19 |> summarise(across(c(seaweed, seagrass), list(mean = mean, sd = sd)))\n#> # A tibble: 1 × 4\n#>   seaweed_mean seaweed_sd seagrass_mean seagrass_sd\n#>          <dbl>      <dbl>         <dbl>       <dbl>\n#> 1         312.       439.          127.        227.\nd19 |> \n  summarise(across(c(seaweed, seagrass),\n                   list(mean = mean, sd = sd,\n                        mad = mean_absolute_deviation)))\n#> # A tibble: 1 × 6\n#>   seaweed_mean seaweed_sd seaweed_mad seagrass_mean\n#>          <dbl>      <dbl>       <dbl>         <dbl>\n#> 1         312.       439.        266.          127.\n#> # … with 2 more variables: seagrass_sd <dbl>,\n#> #   seagrass_mad <dbl>\nd19 |>\n  group_by(site) |> \n  summarise(across(c(seaweed, seagrass),\n                   list(mean = mean, sd = sd,\n                        mad = mean_absolute_deviation)))\n#> # A tibble: 3 × 7\n#>   site  seaweed_mean seaweed_sd seaweed_mad seagrass_mean\n#>   <fct>        <dbl>      <dbl>       <dbl>         <dbl>\n#> 1 東部          118.       56.1        42.3          32.5\n#> 2 中部          205.      195.        160.          276. \n#> 3 西部          578.      658.        486.           29.2\n#> # … with 2 more variables: seagrass_sd <dbl>,\n#> #   seagrass_mad <dbl>"},{"path":"map-function.html","id":"map-function","chapter":"5 map 関数ってすごい","heading":"5 map 関数ってすごい","text":"","code":""},{"path":"map-function.html","id":"必要なパッケージ-3","chapter":"5 map 関数ってすごい","heading":"5.1 必要なパッケージ","text":"","code":"\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(broom)"},{"path":"map-function.html","id":"データの準備-1","chapter":"5 map 関数ってすごい","heading":"5.2 データの準備","text":"Rの iris9 データで解析を紹介します。\niris は data.frame として定義されているので、as_tibble() を使って tibble のクラスを追加します。","code":"\niris = iris |> as_tibble()\niris\n#> # A tibble: 150 × 5\n#>    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#>           <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n#>  1          5.1         3.5          1.4         0.2 setosa \n#>  2          4.9         3            1.4         0.2 setosa \n#>  3          4.7         3.2          1.3         0.2 setosa \n#>  4          4.6         3.1          1.5         0.2 setosa \n#>  5          5           3.6          1.4         0.2 setosa \n#>  6          5.4         3.9          1.7         0.4 setosa \n#>  7          4.6         3.4          1.4         0.3 setosa \n#>  8          5           3.4          1.5         0.2 setosa \n#>  9          4.4         2.9          1.4         0.2 setosa \n#> 10          4.9         3.1          1.5         0.1 setosa \n#> # … with 140 more rows"},{"path":"map-function.html","id":"説明","chapter":"5 map 関数ってすごい","heading":"5.3 説明","text":"tidyverse の開発によって、Rでのデータ処理はすこぶる楽になりました。\n個人的には、Rでデータ処理するのはとても楽しいです。\nそこで、もっともデータ処理を楽にしてくれたのは map() 関数です。\n実は、数種類のmap() があります。map()map_lgl(), map_int(), map_dbl(), map_chr()そのほかにもありますが、研究室のコードでは上のものが多いです。\n他によく使う関数は pmap() と map2() です。\npmap() は3変数以上を関数に渡したいときに使います。\nmap2() は2変数のバージョンです。どの map() には .x と .f の引数を渡す必要があります。.x は list または vector のオブジェクトです。.f は list/vector のそれぞれの要素に適応したい関数です。例えば、つぎの list を定義します。それぞれの要素の平均値を出したいなら、次のように map() を使います。map()は必ず list として結果を返します。\nベース　(base) Rlapply() と同じですね。ベースRの sapply() のようにベクトルとして返してほしいなら、map_dbl() を使います。ベースRの sapply() の結果と同じです。ちなみに、-loop でもできますが、研究室では使用を禁じます。","code":"\nz = list(a = rnorm(10),\n         b = rnorm(10),\n         c = rnorm(5))\nz\n#> $a\n#>  [1]  0.68376422 -0.94980187 -1.17529152  1.97964189\n#>  [5]  0.20077142 -0.36419853  1.20120953 -0.09045133\n#>  [9] -0.10502512  1.35499167\n#> \n#> $b\n#>  [1] -0.24661541 -1.04568435  2.03721930  0.93663369\n#>  [5] -1.36365152 -0.40764046 -1.09065912 -0.08411995\n#>  [9] -0.32229773  1.54598291\n#> \n#> $c\n#> [1] -0.8991996  0.1591441 -2.8021409 -1.0197964 -1.9705745\nmap(z, mean)\n#> $a\n#> [1] 0.273561\n#> \n#> $b\n#> [1] -0.004083264\n#> \n#> $c\n#> [1] -1.306513\nlapply(z, mean)\n#> $a\n#> [1] 0.273561\n#> \n#> $b\n#> [1] -0.004083264\n#> \n#> $c\n#> [1] -1.306513\nmap_dbl(z, mean)\n#>            a            b            c \n#>  0.273561038 -0.004083264 -1.306513468\nsapply(z, mean)\n#>            a            b            c \n#>  0.273561038 -0.004083264 -1.306513468\n# 良い子はループ使わない。\nzout = vector(\"numeric\", 3)\nn = length(z)\nfor(i in 1:n) {\n  zout[i] = mean(z[[i]])\n}\nzout\n#> [1]  0.273561038 -0.004083264 -1.306513468"},{"path":"map-function.html","id":"map-の魅力","chapter":"5 map 関数ってすごい","heading":"5.4 map() の魅力","text":"map()の魅力は tidyverse のパイプラインに使えること、map() に複雑な関数を渡せること。\n結果は tibble として返せることかな。\n他にあるとおもいますが、使えるようになるとデータ処理は楽しいです。たとえば、次のようことができます。\niris のデータを tibble に変換し、pivot_longer() に渡して縦長に変えます。\npivot_longer() には Sepal と Petal を含む列を cols 引数に渡すようにしています。\n変換したあと、pivot_longer() が作った name の列は separate() によって part と measurement に分けます。ここでは関数を定義していますが、この関数は複数の t 検定を実施し、その結果を一つの tibble にまとめています。\nt.test() に渡すデータは filter() 関数に通しています。\nfilter() は str_detect() を使って、 Species 列から解析したいデータを抽出しています。\nstr_detect() で処理する列は Species、検索する文字列は pattern に渡しています。\nたとえば、pattern = \"set|ver\" は set または ver を意味しています。\nt 検定の結果を t12、t13、t23の入れます。\nこんど、それらを broom パッケージの tidy() に渡し、tibbleかします。\nbind_rows() を使って、縦に結合し、結合した要素の名前を comparison にします。上のコードチャンクで定義した関数は iris_long に適応しますが、\nmeasurement ごとに data の要素ごとに実施されます。つまり、data は map() を通して、 runmultttest() が適応されます。","code":"\niris_long = iris |> \n  as_tibble() |> \n  pivot_longer(cols = matches(\"Sepal|Petal\")) |> \n  separate(name, c(\"part\", \"measurement\"))\nrunmultttest = function(df) {\n  #t12: setosa - versicolor\n  #t13: setosa - virginica\n  #t23: versicolor - virginica\n   \n  t12 = t.test(value ~ part, data = filter(df, str_detect(string = Species, pattern = \"set|ver\")))\n  t13 = t.test(value ~ part, data = filter(df, str_detect(string = Species, pattern = \"set|vir\")))\n  t23 = t.test(value ~ part, data = filter(df, str_detect(string = Species, pattern = \"ver|vir\")))\n  bind_rows(\"setosa vs. versicolor\"    = tidy(t12), \n            \"setosa vs. virginica\"     = tidy(t13), \n            \"versicolor vs. virginica\" = tidy(t23), .id = \"comparison\")\n}\niris_long = iris_long |> group_nest(measurement) \niris_long\n#> # A tibble: 2 × 2\n#>   measurement               data\n#>   <chr>       <list<tibble[,3]>>\n#> 1 Length               [300 × 3]\n#> 2 Width                [300 × 3]\niris_long |> \n  mutate(tout = map(data, runmultttest)) |> \n  unnest(tout)\n#> # A tibble: 6 × 13\n#>   measurement             data comparison estimate estimate1\n#>   <chr>       <list<tibble[,3> <chr>         <dbl>     <dbl>\n#> 1 Length             [300 × 3] setosa vs…    -2.61     2.86 \n#> 2 Length             [300 × 3] setosa vs…    -2.29     3.51 \n#> 3 Length             [300 × 3] versicolo…    -1.36     4.91 \n#> 4 Width              [300 × 3] setosa vs…    -2.31     0.786\n#> 5 Width              [300 × 3] setosa vs…    -2.07     1.14 \n#> 6 Width              [300 × 3] versicolo…    -1.20     1.68 \n#> # … with 8 more variables: estimate2 <dbl>,\n#> #   statistic <dbl>, p.value <dbl>, parameter <dbl>,\n#> #   conf.low <dbl>, conf.high <dbl>, method <chr>,\n#> #   alternative <chr>"},{"path":"map-function.html","id":"map_dbl-の使い方","chapter":"5 map 関数ってすごい","heading":"5.5 map_dbl() の使い方","text":"map_lgl(), map_int(), map_dbl(), map_chr() シリーズの関数が返すものは N = 1 のベクトルです。\nよって、適応する関数はベクトルを返すようにくみましょう。\\(df) {...} は無名関数と呼びます。\n\\(df) {...} は function(df) {...} の諸略です。\nこのとき、関数は summarise() を通して、tibble()を返すので、エラーが発生します。次のコードは pull() を使って、mean だけ返すようにしたが、\nN > 1 のベクトルなので、エラーが発生した。Species, measurement, part ごとに map_dbl() で平均を求めたいので、\n一旦 iris_long の data のネスティングを作り直します。エラーがなくなりましたが、グループごとの平均値をもとめたいなら、summarise() のほうがいいですね。map2() を使えば、 2変数渡せます。\nここでは map2_dbl() を使っています。pmap() の場合、渡す変数は list にまとめてから渡しましょう。","code":"\niris_long |> \n  mutate(out = map_dbl(data, \\(df) {\n    df |> \n      group_by(Species, part) |> \n      summarise(value = mean(value))\n  }))\n#> Error in `mutate()`:\n#> ! Problem while computing `out = map_dbl(...)`.\n#> Caused by error in `stop_bad_type()`:\n#> ! Result 1 must be a single double, not a vector of class `grouped_df/tbl_df/tbl/data.frame` and of length 3\niris_long |> \n  mutate(out = map_dbl(data, \\(df) {\n    df |> \n      group_by(Species, part) |> \n      summarise(value = mean(value)) |> pull(mean)\n  }))\n#> Error in `mutate()`:\n#> ! Problem while computing `out = map_dbl(...)`.\n#> Caused by error:\n#> ! Must extract column with a single valid subscript.\n#> x Subscript `var` has the wrong type `function`.\n#> ℹ It must be numeric or character.\niris_long |> \n  unnest(data) |> \n  group_nest(Species, measurement, part) |> \n  mutate(out = map_dbl(data, \\(df) {\n    # mean(df$value) # でもOK\n    df |> summarise(value = mean(value)) |> pull(value)\n  }))\n#> # A tibble: 12 × 5\n#>    Species    measurement part                data   out\n#>    <fct>      <chr>       <chr> <list<tibble[,1]>> <dbl>\n#>  1 setosa     Length      Petal           [50 × 1] 1.46 \n#>  2 setosa     Length      Sepal           [50 × 1] 5.01 \n#>  3 setosa     Width       Petal           [50 × 1] 0.246\n#>  4 setosa     Width       Sepal           [50 × 1] 3.43 \n#>  5 versicolor Length      Petal           [50 × 1] 4.26 \n#>  6 versicolor Length      Sepal           [50 × 1] 5.94 \n#>  7 versicolor Width       Petal           [50 × 1] 1.33 \n#>  8 versicolor Width       Sepal           [50 × 1] 2.77 \n#>  9 virginica  Length      Petal           [50 × 1] 5.55 \n#> 10 virginica  Length      Sepal           [50 × 1] 6.59 \n#> 11 virginica  Width       Petal           [50 × 1] 2.03 \n#> 12 virginica  Width       Sepal           [50 × 1] 2.97\niris_long |> \n  unnest(data) |> \n  group_by(Species, measurement, part) |> \n  summarise(value = mean(value))\n#> # A tibble: 12 × 4\n#> # Groups:   Species, measurement [6]\n#>    Species    measurement part  value\n#>    <fct>      <chr>       <chr> <dbl>\n#>  1 setosa     Length      Petal 1.46 \n#>  2 setosa     Length      Sepal 5.01 \n#>  3 setosa     Width       Petal 0.246\n#>  4 setosa     Width       Sepal 3.43 \n#>  5 versicolor Length      Petal 4.26 \n#>  6 versicolor Length      Sepal 5.94 \n#>  7 versicolor Width       Petal 1.33 \n#>  8 versicolor Width       Sepal 2.77 \n#>  9 virginica  Length      Petal 5.55 \n#> 10 virginica  Length      Sepal 6.59 \n#> 11 virginica  Width       Petal 2.03 \n#> 12 virginica  Width       Sepal 2.97\niris |> \n  mutate(LW = map2_dbl(Petal.Length, Petal.Width, \\(l,w) {\n    (l * w)\n  })) \n#> # A tibble: 150 × 6\n#>    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#>           <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n#>  1          5.1         3.5          1.4         0.2 setosa \n#>  2          4.9         3            1.4         0.2 setosa \n#>  3          4.7         3.2          1.3         0.2 setosa \n#>  4          4.6         3.1          1.5         0.2 setosa \n#>  5          5           3.6          1.4         0.2 setosa \n#>  6          5.4         3.9          1.7         0.4 setosa \n#>  7          4.6         3.4          1.4         0.3 setosa \n#>  8          5           3.4          1.5         0.2 setosa \n#>  9          4.4         2.9          1.4         0.2 setosa \n#> 10          4.9         3.1          1.5         0.1 setosa \n#> # … with 140 more rows, and 1 more variable: LW <dbl>\niris |> \n  mutate(out = pmap_dbl(list(Petal.Length, Petal.Width, \n                             Sepal.Length, Sepal.Width), \\(pl,pw, sl, sw) {\n                               (pl * pw) / (sl * sw)\n                             })) \n#> # A tibble: 150 × 6\n#>    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#>           <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n#>  1          5.1         3.5          1.4         0.2 setosa \n#>  2          4.9         3            1.4         0.2 setosa \n#>  3          4.7         3.2          1.3         0.2 setosa \n#>  4          4.6         3.1          1.5         0.2 setosa \n#>  5          5           3.6          1.4         0.2 setosa \n#>  6          5.4         3.9          1.7         0.4 setosa \n#>  7          4.6         3.4          1.4         0.3 setosa \n#>  8          5           3.4          1.5         0.2 setosa \n#>  9          4.4         2.9          1.4         0.2 setosa \n#> 10          4.9         3.1          1.5         0.1 setosa \n#> # … with 140 more rows, and 1 more variable: out <dbl>"},{"path":"t-test.html","id":"t-test","chapter":"6 t 検定","heading":"6 t 検定","text":"","code":""},{"path":"t-test.html","id":"必要なパッケージ-4","chapter":"6 t 検定","heading":"6.1 必要なパッケージ","text":"","code":"\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(broom)"},{"path":"t-test.html","id":"データの準備-2","chapter":"6 t 検定","heading":"6.2 データの準備","text":"Rの iris10 データで解析を紹介します。\niris は data.frame として定義されているので、as_tibble() を使って tibble のクラスを追加します。irisには4つの数値データと1つの文字列データが入っています。このデータを可視化してから、解析します。","code":"\niris = iris |> as_tibble()\niris\n#> # A tibble: 150 × 5\n#>    Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#>           <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n#>  1          5.1         3.5          1.4         0.2 setosa \n#>  2          4.9         3            1.4         0.2 setosa \n#>  3          4.7         3.2          1.3         0.2 setosa \n#>  4          4.6         3.1          1.5         0.2 setosa \n#>  5          5           3.6          1.4         0.2 setosa \n#>  6          5.4         3.9          1.7         0.4 setosa \n#>  7          4.6         3.4          1.4         0.3 setosa \n#>  8          5           3.4          1.5         0.2 setosa \n#>  9          4.4         2.9          1.4         0.2 setosa \n#> 10          4.9         3.1          1.5         0.1 setosa \n#> # … with 140 more rows\niris |> pivot_longer(cols = matches(\"Sepal|Petal\")) |> \n  separate(name, c(\"part\", \"measurement\")) |> \n  ggplot() + \n  geom_point(aes(x = Species, y = value, color = Species),\n             position = position_jitter(0.2)) +\n  scale_color_viridis_d(end = 0.8) +\n  facet_grid(rows = vars(part),\n             cols = vars(measurement))"},{"path":"t-test.html","id":"petal-と-sepal-の長さの比較","chapter":"6 t 検定","heading":"6.3 Petal と Sepal の長さの比較","text":"種を無視して、アヤメの Petal （花びら）と Sepal (萼片)の長さを比較しています。花びらと萼片の平均値、標準偏差、標準誤差を先にもとめます。\n専用の標準誤差の関数はないので、ここで定義します。H0: 花びらと萼片の長さに違いがないHA: 花びらと萼片の長さに違いがあるt検定は t.test() で実施します。\n関数の出力には、使用したデータ、t値、 自由度、 P値、平均値の差の95%信頼区間、それぞれの平均値の推定量が返ってきました。\n結論から説明すると、P < 0.0001 なので、有意水準が 0.05 のとき、帰無仮説は棄却できます。t.test() の結果をオブジェクトに書き込んだら、t値 (t value)、p値 (p value)、自由度 (degrees freedom) を抽出できます。英語論文に記述する場合は、次の通りです。mean petal length samples 3.76 ± 0.14, whereas mean sepal length 5.84 ± 0.07.\nWelch’S t-test revealed statistical significance differences two sites (t(211.54) = -13.098; P < 0.0001) significance level \\(\\alpha=\\) 0.05.大事なポイントは、この 3 つの情報を報告することです。t(211.54): 検定に使用した自由度（サンプル数の目安）-13.098: t検定の統計量P = 0.0000: 結果のP値","code":"\nse = function(x) {\n  sd(x) / sqrt(length(x))\n}\nirisout = iris |> \n  summarise(across(matches(\"Length\"), list(mean = mean, sd = sd, se = se)))\nirisout\n#> # A tibble: 1 × 6\n#>   Sepal.Length_mean Sepal.Length_sd Sepal.Length_se\n#>               <dbl>           <dbl>           <dbl>\n#> 1              5.84           0.828          0.0676\n#> # … with 3 more variables: Petal.Length_mean <dbl>,\n#> #   Petal.Length_sd <dbl>, Petal.Length_se <dbl>\nt.test(iris$Petal.Length, iris$Sepal.Length)\n#> \n#>  Welch Two Sample t-test\n#> \n#> data:  iris$Petal.Length and iris$Sepal.Length\n#> t = -13.098, df = 211.54, p-value < 2.2e-16\n#> alternative hypothesis: true difference in means is not equal to 0\n#> 95 percent confidence interval:\n#>  -2.399166 -1.771500\n#> sample estimates:\n#> mean of x mean of y \n#>  3.758000  5.843333\ndset_test = t.test(iris$Petal.Length, iris$Sepal.Length)\ndset_test$statistic  # t value\n#>         t \n#> -13.09835\ndset_test$parameter  # degrees of freedom \n#>       df \n#> 211.5427\ndset_test$p.value 　 # p value\n#> [1] 4.262173e-29"},{"path":"t-test.html","id":"petal-と-sepal-の幅の比較","chapter":"6 t 検定","heading":"6.4 Petal と Sepal の幅の比較","text":"先程の説明は横長のでデータ解析でしたが、tidy データ（縦長）の場合、もっと簡易に検定のコードをくめます。\nまず、データを縦長に変換します。記述統計ととり方も変わります。\nグループ化してから平均値、標準偏差、標準誤差を求めます。H0: 花びらと萼片の幅に違いがないHA: 花びらと萼片の幅に違いがある長さと幅のデータが混ざっているので、幅のデータのフィルターを掛けてから解析します。幅の解析をしても、P < 0.0001 なので、帰無仮説を棄却できます。","code":"\niris_long = iris |> pivot_longer(cols = matches(\"Sepal|Petal\")) |> \n  separate(name, c(\"part\", \"measurement\"))\niris_long\n#> # A tibble: 600 × 4\n#>    Species part  measurement value\n#>    <fct>   <chr> <chr>       <dbl>\n#>  1 setosa  Sepal Length        5.1\n#>  2 setosa  Sepal Width         3.5\n#>  3 setosa  Petal Length        1.4\n#>  4 setosa  Petal Width         0.2\n#>  5 setosa  Sepal Length        4.9\n#>  6 setosa  Sepal Width         3  \n#>  7 setosa  Petal Length        1.4\n#>  8 setosa  Petal Width         0.2\n#>  9 setosa  Sepal Length        4.7\n#> 10 setosa  Sepal Width         3.2\n#> # … with 590 more rows\nirisout = iris_long |> \n  group_by(part, measurement) |> \n  summarise(across(value, list(mean = mean, sd = sd, se = se)))\nirisout\n#> # A tibble: 4 × 5\n#> # Groups:   part [2]\n#>   part  measurement value_mean value_sd value_se\n#>   <chr> <chr>            <dbl>    <dbl>    <dbl>\n#> 1 Petal Length            3.76    1.77    0.144 \n#> 2 Petal Width             1.20    0.762   0.0622\n#> 3 Sepal Length            5.84    0.828   0.0676\n#> 4 Sepal Width             3.06    0.436   0.0356\niris_width = iris_long |> filter(str_detect(measurement, \"Width\"))\niris_width\n#> # A tibble: 300 × 4\n#>    Species part  measurement value\n#>    <fct>   <chr> <chr>       <dbl>\n#>  1 setosa  Sepal Width         3.5\n#>  2 setosa  Petal Width         0.2\n#>  3 setosa  Sepal Width         3  \n#>  4 setosa  Petal Width         0.2\n#>  5 setosa  Sepal Width         3.2\n#>  6 setosa  Petal Width         0.2\n#>  7 setosa  Sepal Width         3.1\n#>  8 setosa  Petal Width         0.2\n#>  9 setosa  Sepal Width         3.6\n#> 10 setosa  Petal Width         0.2\n#> # … with 290 more rows\nt.test(value ~ part, data = iris_width)\n#> \n#>  Welch Two Sample t-test\n#> \n#> data:  value by part\n#> t = -25.916, df = 237.03, p-value < 2.2e-16\n#> alternative hypothesis: true difference in means between group Petal and group Sepal is not equal to 0\n#> 95 percent confidence interval:\n#>  -1.999237 -1.716763\n#> sample estimates:\n#> mean in group Petal mean in group Sepal \n#>            1.199333            3.057333"},{"path":"t-test.html","id":"map-関数を用いて解析する","chapter":"6 t 検定","heading":"6.5 map() 関数を用いて解析する","text":"上に紹介した解析は長さと幅に分けてから実施したが、tidyverse の機能を上手につかば最低限のコードでできます。map()関数の詳細は専用の章に説明しています (5)。group_nest() を使って、データのグループ化とネスティングをします。これで、iris_long の tibble は2つの 300 x 3 tibbleに分離しました。\n解析は map() を通して実施します。結果は tout に入っています。","code":"\niris_long = iris |> pivot_longer(cols = matches(\"Sepal|Petal\")) |> \n  separate(name, c(\"part\", \"measurement\"))\niris_long = iris_long |> group_nest(measurement)\niris_long\n#> # A tibble: 2 × 2\n#>   measurement               data\n#>   <chr>       <list<tibble[,3]>>\n#> 1 Length               [300 × 3]\n#> 2 Width                [300 × 3]\nrunttest = function(df) {\n  t.test(value ~ part, data = df)\n}\n\niris_long = iris_long |> mutate(tout = map(data, runttest))\niris_long\n#> # A tibble: 2 × 3\n#>   measurement               data tout   \n#>   <chr>       <list<tibble[,3]>> <list> \n#> 1 Length               [300 × 3] <htest>\n#> 2 Width                [300 × 3] <htest>\nlibrary(broom)\n\niris_long |> mutate(tout = map(tout, tidy)) |> unnest(tout)\n#> # A tibble: 2 × 12\n#>   measurement              data estimate estimate1 estimate2\n#>   <chr>       <list<tibble[,3]>    <dbl>     <dbl>     <dbl>\n#> 1 Length              [300 × 3]    -2.09      3.76      5.84\n#> 2 Width               [300 × 3]    -1.86      1.20      3.06\n#> # … with 7 more variables: statistic <dbl>, p.value <dbl>,\n#> #   parameter <dbl>, conf.low <dbl>, conf.high <dbl>,\n#> #   method <chr>, alternative <chr>"},{"path":"t-test.html","id":"種ごとの比較","chapter":"6 t 検定","heading":"6.6 種ごとの比較","text":"検証する帰無仮説はH0: 種間の花びらの長さに違いがないまたは、萼片の幅に違いがないHA: 種間の花びらの長さに違いがあるまたは、萼片の幅に違いがあるすべての t 検定に対して、P < 0.0001 でした。\nよって、帰無仮説を棄却できます。\nところが、このように複数の組み合わせで検定をすることで、第1種の誤りを起こすことがあります。第1種の誤差とは、帰無仮説が事実であるのに、棄却する誤りです。\n第1種の誤りを起こす確率は有意水準と同じです（\\(\\alpha\\)）。\nつまり、$= $ 0.05 にした場合、第1種の誤りを起こす確率は 0.5 です。第1種の誤りは検定の回数によって上昇します。\\[\n\\text{Type-error rate} = 1 - (1-\\alpha)^n\n\\]アヤメの解析の場合、種間の比較は 3 回実施したので、第1種の誤りは、\\[\n1 - (1-0.05)^3 = 0.142625\n\\]\nです。有意水準を厳しくすることで誤りの確率を抑えることができます。\n例えば、全体の第1種の誤りを 0.05 に抑えたい場合、ここの検定の有意水準を小さくします。\\[\n1 - (1 + 0.05)^{1/3} = 0.01695243\n\\]\nつまり、ここの検定の有意水準を 0.016 にすれば、第1種の誤りは 0.047 に抑えられます。\nそれにしても、このような 多重比較 は専用の解析手法があります。\n複数群の平均値が統計学的に違うかを検証したい場合は分散分析のような線形解析を実施します。\n最初から複数群のペアごとの違いを検証したいなら、多重比較の検定法を使用します。","code":"\niris_long = iris |> pivot_longer(cols = matches(\"Sepal|Petal\")) |> \n  separate(name, c(\"part\", \"measurement\"))\n\nrunmultttest = function(df) {\n  #t12: setosa - versicolor\n  #t13: setosa - virginica\n  #t23: versicolor - virginica\n   \n  t12 = t.test(value ~ part, data = filter(df, str_detect(Species, \"set|ver\"))) |> tidy()\n  t13 = t.test(value ~ part, data = filter(df, str_detect(Species, \"set|vir\"))) |> tidy()\n  t23 = t.test(value ~ part, data = filter(df, str_detect(Species, \"ver|vir\"))) |> tidy()\n  bind_rows(\"setosa vs. versicolor\" = t12, \n            \"setosa vs. virginica\" = t13, \n            \"versicolor vs. virginica\" = t23, .id = \"comparison\")\n}\n\niris_long |> group_nest(measurement) |> \n  mutate(tout = map(data, runmultttest)) |> \n  unnest(tout) \n#> # A tibble: 6 × 13\n#>   measurement             data comparison estimate estimate1\n#>   <chr>       <list<tibble[,3> <chr>         <dbl>     <dbl>\n#> 1 Length             [300 × 3] setosa vs…    -2.61     2.86 \n#> 2 Length             [300 × 3] setosa vs…    -2.29     3.51 \n#> 3 Length             [300 × 3] versicolo…    -1.36     4.91 \n#> 4 Width              [300 × 3] setosa vs…    -2.31     0.786\n#> 5 Width              [300 × 3] setosa vs…    -2.07     1.14 \n#> 6 Width              [300 × 3] versicolo…    -1.20     1.68 \n#> # … with 8 more variables: estimate2 <dbl>,\n#> #   statistic <dbl>, p.value <dbl>, parameter <dbl>,\n#> #   conf.low <dbl>, conf.high <dbl>, method <chr>,\n#> #   alternative <chr>"},{"path":"anova.html","id":"anova","chapter":"7 分散分析","heading":"7 分散分析","text":"","code":""},{"path":"anova.html","id":"必要なパッケージ-5","chapter":"7 分散分析","heading":"7.1 必要なパッケージ","text":"","code":"\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(car)\nlibrary(emmeans)"},{"path":"anova.html","id":"データの読み込み-1","chapter":"7 分散分析","heading":"7.2 データの読み込み","text":"","code":"\nrootdatafolder = rprojroot::find_rstudio_root_file(\"Data/\")\nfilename = '瀬戸内海藻場データ.xlsx'\npath = str_c(rootdatafolder, filename)\n\n# fy1990 の処理\nRNG = \"A4:C27\"   # セルの範囲\nSHEET = \"FY1990\" # シート名\nd19 = read_xlsx(path, sheet = SHEET, range = RNG)\nd19 = d19 |> \n  rename(site = 調査海域, seaweed = 海藻, seagrass = 海草) |> \n  mutate(site = factor(site, levels = c('東部', '中部', '西部')))\n\n# fy2018の処理\nRNG = \"A6:C15\"   # 海藻データのセル範囲\nSHEET = \"FY2018\" # シート名\nseaweed = read_xlsx(path, sheet = SHEET, range = RNG)\nRNG = \"E6:G15\"   # 海草データのセル範囲\n\nseagrass = read_xlsx(path, sheet = SHEET, range = RNG)\nseaweed = seaweed |> pivot_longer(cols = everything())\nseagrass = seagrass |> pivot_longer(cols = everything())\n\nd20 = bind_rows(seaweed = seaweed, seagrass = seagrass, .id = \"type\")\nd20 = d20 |> pivot_wider(id_cols = name,\n                   names_from = type, values_from = value, \n                   values_fn = \"list\")\nd20 = d20 |> unnest(c(seaweed, seagrass)) |> rename(site = name) |> drop_na()\nd20 = d20 |> \n  mutate(site = factor(site, levels = c('東部', '中部', '西部')))"},{"path":"anova.html","id":"一元配置分散分析","chapter":"7 分散分析","heading":"7.3 一元配置分散分析","text":"では、一元配置分散分析を実施します。\nまず、分散分析の平方和を正しく求めるためには、contr.sum を設定することです。\nその処理のあと、lm() 関数でモデルを当てはめます。\nlm() 関数に渡すモデルは、 〜 の右辺に説明変数、左辺に観測値を指定しましょう。FY1990 海藻藻場面積の一元配置分散分析の結果は次のとおりです。FY2019 海藻藻場面積の一元配置分散分析の結果は次のとおりです。FY1990 のP値は P = 0.0944、\nFY2018 のP値は P = 0.0750 でした。\nどちらも有意水準 (α = 0.05) より大きいので、帰無仮説（海域間の藻場面積は同じ）を棄却できません。等分散性と正規性の検定を無視したように、今回だけ分散分析の結果を無視して、多重比較をしてみます。","code":"\ncontrasts(d19$site) = contr.sum\ncontrasts(d20$site) = contr.sum\nm19 = lm(seaweed ~ site, data = d19)\nm20 = lm(seaweed ~ site, data = d20)\na19 = anova(m19)\na20 = anova(m20)\nanova(m19) # FY1990 の処理\n#> Analysis of Variance Table\n#> \n#> Response: seaweed\n#>           Df  Sum Sq Mean Sq F value  Pr(>F)  \n#> site       2  892963  446482  2.6621 0.09439 .\n#> Residuals 20 3354343  167717                  \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nanova(m20) # FY2018 の処理\n#> Analysis of Variance Table\n#> \n#> Response: seaweed\n#>           Df  Sum Sq Mean Sq F value  Pr(>F)  \n#> site       2  330811  165405  2.9569 0.07499 .\n#> Residuals 20 1118771   55939                  \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"anova.html","id":"多重比較","chapter":"7 分散分析","heading":"7.4 多重比較","text":"調査海域の全ペアの比較をしるので、Tukey HSDを用います。FY2019 の場合、全ペアを比較したら、有意な結果はありません。FY2020 も同じですね。この用な結果は予想していました。そもそも分散分析から有意な結果がでなかったので、多重比較しても有意な結果はでません。ちなみに Dunnet Method をつかって、西部と東部を中部と比較したら次の結果になります。Dunnet Method の場合でも有意な結果はありません。","code":"\ne19 = emmeans(m19, specs = pairwise ~ site, adjust = \"tukey\")\ne20 = emmeans(m20, specs = pairwise ~ site, adjust = \"tukey\")\ne19 # FY1990 の処理\n#> $emmeans\n#>  site emmean  SE df lower.CL upper.CL\n#>  東部    118 167 20     -230      467\n#>  中部    205 137 20      -80      490\n#>  西部    578 145 20      276      880\n#> \n#> Confidence level used: 0.95 \n#> \n#> $contrasts\n#>     contrast estimate  SE df t.ratio p.value\n#>  東部 - 中部    -86.4 216 20  -0.400  0.9158\n#>  東部 - 西部   -459.3 221 20  -2.077  0.1202\n#>  中部 - 西部   -372.8 199 20  -1.874  0.1723\n#> \n#> P value adjustment: tukey method for comparing a family of 3 estimates\ne20 # FY2018 の処理\n#> $emmeans\n#>  site emmean   SE df lower.CL upper.CL\n#>  東部  109.3 96.6 20    -92.1      311\n#>  中部  301.0 78.8 20    136.5      465\n#>  西部   29.2 83.6 20   -145.2      204\n#> \n#> Confidence level used: 0.95 \n#> \n#> $contrasts\n#>     contrast estimate  SE df t.ratio p.value\n#>  東部 - 中部   -191.7 125 20  -1.538  0.2953\n#>  東部 - 西部     80.1 128 20   0.627  0.8072\n#>  中部 - 西部    271.8 115 20   2.365  0.0696\n#> \n#> P value adjustment: tukey method for comparing a family of 3 estimates\ne19d = emmeans(m19, specs = trt.vs.ctrl ~ site, ref = 2)\ne20d = emmeans(m20, specs = trt.vs.ctrl ~ site, ref = 2)\ne19d # FY1990 の処理\n#> $emmeans\n#>  site emmean  SE df lower.CL upper.CL\n#>  東部    118 167 20     -230      467\n#>  中部    205 137 20      -80      490\n#>  西部    578 145 20      276      880\n#> \n#> Confidence level used: 0.95 \n#> \n#> $contrasts\n#>     contrast estimate  SE df t.ratio p.value\n#>  東部 - 中部    -86.4 216 20  -0.400  0.8781\n#>  西部 - 中部    372.8 199 20   1.874  0.1373\n#> \n#> P value adjustment: dunnettx method for 2 tests\ne20d # FY2018 の処理\n#> $emmeans\n#>  site emmean   SE df lower.CL upper.CL\n#>  東部  109.3 96.6 20    -92.1      311\n#>  中部  301.0 78.8 20    136.5      465\n#>  西部   29.2 83.6 20   -145.2      204\n#> \n#> Confidence level used: 0.95 \n#> \n#> $contrasts\n#>     contrast estimate  SE df t.ratio p.value\n#>  東部 - 中部     -192 125 20  -1.538  0.2441\n#>  西部 - 中部     -272 115 20  -2.365  0.0531\n#> \n#> P value adjustment: dunnettx method for 2 tests"},{"path":"anova.html","id":"二元配置分散分析","chapter":"7 分散分析","heading":"7.5 二元配置分散分析","text":"","code":""},{"path":"anova.html","id":"正規性と等分散性の確認","chapter":"7 分散分析","heading":"7.6 正規性と等分散性の確認","text":"分散分析を行う前に、Levene Test と Shapiro-Wilk Normality Test でデータの等分散性11 と正規性12 を確認します。\nルビーン検定とシャピロウィルク検定については、t 検定の資料を参考にしてください。\nここで紹介する解析は 海藻 に対してです。ルビーン検定FY1990とFY2018 データの等分散性検定結果は P = 0.0996 でしたので、\n帰無仮説は棄却できません。\nつまり、等分散性であると判断できます。シャピロウィルク検定FY1990とFY2018 データの等分散性について、P < 0.0001 だったので、\n帰無仮説を棄却できます。\nデータの母集団は正規分布に従わないかもしれないです。","code":"\ndall = bind_rows(fy1990 = d19,\n                 fy2018 = d20, \n                 .id = \"year\")\ndall = dall |> mutate(year = factor(year))\nleveneTest(seaweed ~ site*year, data = dall) \n#> Levene's Test for Homogeneity of Variance (center = median)\n#>       Df F value Pr(>F)  \n#> group  5  1.9994 0.0996 .\n#>       40                 \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nshapiro.test(x = dall$seaweed) # FY1990 の処理\n#> \n#>  Shapiro-Wilk normality test\n#> \n#> data:  dall$seaweed\n#> W = 0.63263, p-value = 1.731e-09"},{"path":"anova.html","id":"二元配置分散分析-1","chapter":"7 分散分析","heading":"7.7 二元配置分散分析","text":"では、二元配置分散分析を実施します。\nまず、分散分析の平方和を正しく求めるためには、contr.sum を設定することです。\nその処理のあと、lm() 関数でモデルを当てはめます。\nlm() 関数に渡すモデルは、 〜 の右辺に説明変数、左辺に観測値を指定しましょう。FY1990 海藻藻場面積の一元配置分散分析の結果は次のとおりです。site 効果のP値は P = 0.3274、\nyear 効果のP値は P = 0.1323、\n相互作用のP値は P = 0.0200 でした。\n相互作用のP値は有意水準 (α = 0.05) より大きいので、相互作用の帰無仮説は棄却できますが、主効果の帰無仮説は棄却できません。","code":"\ncontrasts(dall$site) = contr.sum\ncontrasts(dall$year) = contr.sum\nmall = lm(seaweed ~ site*year, data = dall)\nAnova(mall, type =3)\n#> Anova Table (Type III tests)\n#> \n#> Response: seaweed\n#>              Sum Sq Df F value    Pr(>F)    \n#> (Intercept) 2230084  1 19.9421 6.382e-05 ***\n#> site         256846  2  1.1484   0.32738    \n#> year         263994  1  2.3607   0.13230    \n#> site:year    966928  2  4.3233   0.01996 *  \n#> Residuals   4473114 40                      \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"anova.html","id":"多重比較-1","chapter":"7 分散分析","heading":"7.8 多重比較","text":"調査海域の全ペアの比較をしるので、Tukey HSDを用います。FY2019 の場合、全ペアを比較したら、有意な結果はありません。","code":"\neall = emmeans(mall, specs = pairwise ~ site:year, adjust = \"tukey\")\neall \n#> $emmeans\n#>  site year   emmean  SE df lower.CL upper.CL\n#>  東部 fy1990  118.3 137 40   -157.6      394\n#>  中部 fy1990  204.8 111 40    -20.5      430\n#>  西部 fy1990  577.6 118 40    338.7      817\n#>  東部 fy2018  109.3 137 40   -166.6      385\n#>  中部 fy2018  301.0 111 40     75.7      526\n#>  西部 fy2018   29.2 118 40   -209.7      268\n#> \n#> Confidence level used: 0.95 \n#> \n#> $contrasts\n#>      contrast              estimate  SE df t.ratio p.value\n#>  東部 fy1990 - 中部 fy1990    -86.4 176 40  -0.490  0.9962\n#>  東部 fy1990 - 西部 fy1990   -459.3 181 40  -2.543  0.1360\n#>  東部 fy1990 - 東部 fy2018      9.0 193 40   0.047  1.0000\n#>  東部 fy1990 - 中部 fy2018   -182.7 176 40  -1.036  0.9027\n#>  東部 fy1990 - 西部 fy2018     89.1 181 40   0.493  0.9961\n#>  中部 fy1990 - 西部 fy1990   -372.8 162 40  -2.295  0.2201\n#>  中部 fy1990 - 東部 fy2018     95.4 176 40   0.542  0.9940\n#>  中部 fy1990 - 中部 fy2018    -96.2 158 40  -0.610  0.9897\n#>  中部 fy1990 - 西部 fy2018    175.5 162 40   1.080  0.8863\n#>  西部 fy1990 - 東部 fy2018    468.3 181 40   2.593  0.1226\n#>  西部 fy1990 - 中部 fy2018    276.6 162 40   1.702  0.5383\n#>  西部 fy1990 - 西部 fy2018    548.4 167 40   3.280  0.0245\n#>  東部 fy2018 - 中部 fy2018   -191.7 176 40  -1.087  0.8835\n#>  東部 fy2018 - 西部 fy2018     80.1 181 40   0.443  0.9977\n#>  中部 fy2018 - 西部 fy2018    271.8 162 40   1.672  0.5573\n#> \n#> P value adjustment: tukey method for comparing a family of 6 estimates\nemmeans(mall, specs = pairwise ~ site|year, adjust = \"tukey\")\n#> $emmeans\n#> year = fy1990:\n#>  site emmean  SE df lower.CL upper.CL\n#>  東部  118.3 137 40   -157.6      394\n#>  中部  204.8 111 40    -20.5      430\n#>  西部  577.6 118 40    338.7      817\n#> \n#> year = fy2018:\n#>  site emmean  SE df lower.CL upper.CL\n#>  東部  109.3 137 40   -166.6      385\n#>  中部  301.0 111 40     75.7      526\n#>  西部   29.2 118 40   -209.7      268\n#> \n#> Confidence level used: 0.95 \n#> \n#> $contrasts\n#> year = fy1990:\n#>     contrast estimate  SE df t.ratio p.value\n#>  東部 - 中部    -86.4 176 40  -0.490  0.8762\n#>  東部 - 西部   -459.3 181 40  -2.543  0.0389\n#>  中部 - 西部   -372.8 162 40  -2.295  0.0681\n#> \n#> year = fy2018:\n#>     contrast estimate  SE df t.ratio p.value\n#>  東部 - 中部   -191.7 176 40  -1.087  0.5273\n#>  東部 - 西部     80.1 181 40   0.443  0.8976\n#>  中部 - 西部    271.8 162 40   1.672  0.2282\n#> \n#> P value adjustment: tukey method for comparing a family of 3 estimates\nemmeans(mall, specs = pairwise ~ year|site, adjust = \"tukey\")\n#> $emmeans\n#> site = 東部:\n#>  year   emmean  SE df lower.CL upper.CL\n#>  fy1990  118.3 137 40   -157.6      394\n#>  fy2018  109.3 137 40   -166.6      385\n#> \n#> site = 中部:\n#>  year   emmean  SE df lower.CL upper.CL\n#>  fy1990  204.8 111 40    -20.5      430\n#>  fy2018  301.0 111 40     75.7      526\n#> \n#> site = 西部:\n#>  year   emmean  SE df lower.CL upper.CL\n#>  fy1990  577.6 118 40    338.7      817\n#>  fy2018   29.2 118 40   -209.7      268\n#> \n#> Confidence level used: 0.95 \n#> \n#> $contrasts\n#> site = 東部:\n#>  contrast        estimate  SE df t.ratio p.value\n#>  fy1990 - fy2018      9.0 193 40   0.047  0.9631\n#> \n#> site = 中部:\n#>  contrast        estimate  SE df t.ratio p.value\n#>  fy1990 - fy2018    -96.2 158 40  -0.610  0.5451\n#> \n#> site = 西部:\n#>  contrast        estimate  SE df t.ratio p.value\n#>  fy1990 - fy2018    548.4 167 40   3.280  0.0022"},{"path":"anova.html","id":"等分散性と正規性の事後確認","chapter":"7 分散分析","heading":"7.9 等分散性と正規性の事後確認","text":"plot() に渡している mall は前章に当てはめた二元配置分散分析のモデルです。","code":""},{"path":"anova.html","id":"等分散性の確認に使うプロット","chapter":"7 分散分析","heading":"7.10 等分散性の確認に使うプロット","text":"\nFigure 7.1: 残渣 vs. 期待値\nFig. 7.1 は残渣13\nと期待値14 の関係を理解するてめに使います。\n等分散性に問題がない場合、残渣は y = 0 の周りを均一に、変動なくばらつきます。\nところが Fig. 7.1 の場合、期待値が高いとき、残渣のばらつきが大きい。\nFigure 7.2: スケール・位置プロット\nFig. 7.2 はスケール・ロケーションプロットといいます。\nスケール15 は確率密度分布のばらつきのパラメータです。\n位置（ロケーション）16 は確率分布の中心のパラメータです。\nたとえば、正規分布のスケールパラメータは分散、位置パラメータは平均値です。\nFig. 7.2 の横軸は位置、縦長はスケールパラメータで標準化した残渣の平方根です。\n示されている標準化残渣のばらつきが均一で、期待値17 と無関係であれば、ばらつきは均一であると考えられます。\nFig. 7.2 の場合、標準化残渣は期待値と正の関係があるので、ばらつきは均一であると考えられません。","code":"\nplot(mall, which = 1)\nplot(mall, which = 3)"},{"path":"anova.html","id":"正規性の確認に使うプロット","chapter":"7 分散分析","heading":"7.11 正規性の確認に使うプロット","text":"\nFigure 7.3: QQプロット\n","code":"\nplot(mall, which = 2)"},{"path":"anova.html","id":"飛び値異常値の確認プロット","chapter":"7 分散分析","heading":"7.12 飛び値・異常値の確認プロット","text":"\nFigure 7.4: クックの距離とてこ比\n","code":"\nplot(mall, which = 5)"},{"path":"glm.html","id":"glm","chapter":"8 一般化線形モデル","heading":"8 一般化線形モデル","text":"","code":""},{"path":"gam.html","id":"gam","chapter":"9 一般化加法モデル","heading":"9 一般化加法モデル","text":"","code":""},{"path":"ggplot.html","id":"ggplot","chapter":"10 ggplot","heading":"10 ggplot","text":"","code":"\nlibrary(tidyverse)\nlibrary(ggpubr)\nlibrary(lemon)\nlibrary(scales)\nlibrary(ggrepel)\nlibrary(patchwork)\niris = as_tibble(iris)"},{"path":"maps.html","id":"maps","chapter":"11 地図の作り方","heading":"11 地図の作り方","text":"","code":""},{"path":"maps.html","id":"必要なパッケージ-6","chapter":"11 地図の作り方","heading":"11.1 必要なパッケージ","text":"次の2つは地図専用のパッケージです。Noto Sans のフォントが好きなので、ここで Google Fonts からアクセスします。ggplot のデフォルトテーマも設定し、フォント埋め込みも可能にします。\nここでデフォルトを設定すると、毎回 theme_pubr() を ggplotのチェインにたさなくていい。","code":"\nlibrary(tidyverse)　# Essential package\nlibrary(ggpubr)     # Publication-oriented figures\nlibrary(kableExtra) # Tables\nlibrary(magick)     # Imagemagick R API\nlibrary(patchwork)  # Simplified figure tiling\nlibrary(showtext)   # I want to use google fonts in the figures\nlibrary(ggspatial)  # Essential for map-making with ggplot\nlibrary(sf)         # Essential for map data manipulation\nfont_add_google(\"Noto Sans\",\"notosans\")\ntheme_pubr(base_size = 10, base_family = \"notosans\") |> theme_set()\nshowtext_auto() # Automatically embed the Noto Sans fonts into the ggplots."},{"path":"maps.html","id":"シェープファイルの読み込み","chapter":"11 地図の作り方","heading":"11.2 シェープファイルの読み込み","text":"シェープファイル (shapefile) は地図データのことです。\n基本的の拡張子は shp, shx, dbf　ですが、その他に prj と xml もあります。研究室用にダウンロードした 国土交通省・国土数値情報ダウンロードサービス のシェープファイルは ~/Lab_Data/Japan_map_data/Japan に入っています。mlit に読み込んだシェープファイルはここへ。シェープファイルの 座標参照系 (CRS: Coordinate Reference System) を確認しましょう。CRSには 地理座標系 と 投影座標系 の2種類があります。\n座標系にはEPSGコードもつけられています。このデータは政策区域のデータなので、とても重いです。\nまずは、都道府県ごとにまとめた RDS ファイルを作って保存します。\n都道府県ごとに st_union() を使って polgyon データを結合します。\n結合したデータを unnest して、simple feature に戻してかた保存します。\n121158 features もあるので、数時間もかります。沿岸のデータだけなら軽いですので、C23 シリーズのファイルを読み込みます。では、ここで地図の確認をします。mlit のデータは細かい政策区域まで分けられているので、全国スケールの図には向いていません。\nst_union() をつかって、都道府県ごとに polygon を結合したファイルは、~/Lab_Data/Japan_map_data/Japan/todofuken.rds に保存しています。\n次のコードで、都道府県ごとにまとめましたが、並列処理でも５時間以上もかかったので、RDS ファイルを使いましょう。","code":"\nmlit = read_sf(\"~/Lab_Data/Japan_map_data/Japan/N03-20210101_GML/\")\nst_crs(mlit)\n#> Coordinate Reference System:\n#>   User input: JGD2011 \n#>   wkt:\n#> GEOGCRS[\"JGD2011\",\n#>     DATUM[\"Japanese Geodetic Datum 2011\",\n#>         ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n#>             LENGTHUNIT[\"metre\",1]]],\n#>     PRIMEM[\"Greenwich\",0,\n#>         ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>     CS[ellipsoidal,2],\n#>         AXIS[\"geodetic latitude (Lat)\",north,\n#>             ORDER[1],\n#>             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>         AXIS[\"geodetic longitude (Lon)\",east,\n#>             ORDER[2],\n#>             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#>     USAGE[\n#>         SCOPE[\"Horizontal component of 3D system.\"],\n#>         AREA[\"Japan - onshore and offshore.\"],\n#>         BBOX[17.09,122.38,46.05,157.65]],\n#>     ID[\"EPSG\",6668]]\n# HTML 用テーブル\ntibble(`EPSG Code` = c(4326,6668,6677),\n       `CRS` = c(\"WGS84\", \"JGD2011\", \"JGD2011 / Japan Plane Rectangular CS IX\"),\n       `Units` = c(\"degrees\", \"degrees\", \"meters\")) |> \n  kbl() |> \n  kable_styling(bootstrap_options = c(\"hover\"))\nmlit = tibble(folder = dir(\"~/Lab_Data/Japan_map_data/Coastline/\", full = TRUE)) |> \n  mutate(data = map(folder, read_sf)) |> select(data) |> \n  unnest(data) |> \n  st_as_sf(crs = st_crs(6668))\nmlit |> ggplot() + geom_sf()\n# Takes 5.5 hours to complete with 30 cores!\nlibrary(furrr)\nplan(multisession, workers = 30)\n# Group by prefecture\nmlit1 = mlit |> group_nest(N03_001) |> \n  mutate(data = future_map(data, st_union)) |> \n  unnest(data) |> st_as_sf() \nmlit1 |> write_rds(\"~/Lab_Data/Japan_map_data/Japan/todofuken.rds\")\nmlit1 = read_rds(\"~/Lab_Data/Japan_map_data/Japan/todofuken.rds\")\nmlit1 |> ggplot() + geom_sf()"},{"path":"maps.html","id":"調査地点のデータを準備する","chapter":"11 地図の作り方","heading":"11.3 調査地点のデータを準備する","text":"形上湾アマモ場調査のステーションの GPS tibble を準備する。zostera に緯度経度を設定する。\nCRS は mlit と同じにします。","code":"\nzostera = read_csv(\"~/Lab_Data/matsumuro/Katagami_Bay/longlat_info.csv\")\nzostera |> print(n = 3)\n#> # A tibble: 105 × 6\n#>    Name   lat  long datetime            eelgrass\n#>   <dbl> <dbl> <dbl> <dttm>              <chr>   \n#> 1     1  33.0  130. 2021-05-25 09:14:48 absent  \n#> 2     2  33.0  130. 2021-05-25 09:30:32 absent  \n#> 3     3  33.0  130. 2021-05-25 09:37:16 present \n#> # … with 102 more rows, and 1 more variable:\n#> #   `coverage(%)` <dbl>\nzostera = zostera |> st_as_sf(coords = c(\"long\", \"lat\"), crs = st_crs(mlit))\nzostera |> print(n = 3)\n#> Simple feature collection with 105 features and 4 fields\n#> Geometry type: POINT\n#> Dimension:     XY\n#> Bounding box:  xmin: 129.7845 ymin: 32.90032 xmax: 129.806 ymax: 32.95375\n#> Geodetic CRS:  JGD2011\n#> # A tibble: 105 × 5\n#>    Name datetime            eelgrass `coverage(%)`\n#> * <dbl> <dttm>              <chr>            <dbl>\n#> 1     1 2021-05-25 09:14:48 absent               0\n#> 2     2 2021-05-25 09:30:32 absent               0\n#> 3     3 2021-05-25 09:37:16 present              5\n#> # … with 102 more rows, and 1 more variable:\n#> #   geometry <POINT [°]>"},{"path":"maps.html","id":"九州データの抽出","chapter":"11 地図の作り方","heading":"11.4 九州データの抽出","text":"九州のデータと長崎のデータを抽出します。\n重要：長崎の名前が誤っています。Nagasaki のはずが、Naoasaki として記録されています。海岸線のデータ (mlit) から長崎の情報を抽出したいが、このデータの位置情報はコードで記述されています。長崎の海岸線は次のようになります。九州は mlit1 から抽出したので、都道府県政策区域として作図されます。長崎をハイライトしましょう。この図には、違和感を感じるので、山口、島根、愛媛、広島と高知も追加します。\nそしれ、最初に作った kyushu の範囲を抽出しておきます。長崎、九州、その他の色分けをして、 kyushu をクロップします。\nクロップ範囲は kbbox です。この地図は次のようになりました。","code":"\ntoget = \"長崎|福岡|大分|佐賀|熊本|鹿児島|宮崎\"\nkyushu = mlit1 |> filter(str_detect(N03_001, toget))\nadmincode = readxl::read_xlsx(\"~/Lab_Data/Japan_map_data/AdminiBoundary_CD.xlsx\", skip = 2)\nadmincode = admincode |> select(code = matches(\"行政\"), N03_001 = matches(\"都道府県*.*漢字\"))\ncodes = admincode |> filter(str_detect(N03_001, \"長崎\")) |> pull(code)\nnagasaki = mlit |> filter(str_detect(C23_001, str_c(codes, collapse = \"|\"))) \nggplot() + geom_sf(data = nagasaki)\nggplot() + geom_sf(data = kyushu)\nkyushu |> \n  mutate(fillme = str_detect(N03_001, \"長崎\")) |> \n  ggplot() + geom_sf(aes(fill = fillme), color = NA) +\n  guides(fill = \"none\") +\n  scale_fill_viridis_d() +\n  theme(panel.background = element_rect(fill = \"lightblue\", color = \"black\"),\n        axis.line = element_blank())\nkbbox = kyushu |> st_bbox()\ntoget = \"長崎|福岡|大分|佐賀|熊本|鹿児島|宮崎|山口|島根|愛媛|高知|広島\"\nkyushu = mlit1 |> filter(str_detect(N03_001, toget))\nkyushu = kyushu |>\n  mutate(fillme = case_when(str_detect(N03_001, \"長崎\") ~ \"Nagasaki\",\n                            str_detect(N03_001, \"福岡|大分|佐賀|熊本|鹿児島|宮崎\") ~ \"Kyushu\",\n                            TRUE ~ \"Honshu\")) |> \n  st_crop(kbbox)\nggplot(kyushu) + \n  geom_sf(aes(fill = fillme), color = NA) +\n  guides(fill = \"none\") +\n  coord_sf(expand = FALSE) +\n  scale_fill_viridis_d() +\n  theme(panel.background = element_rect(fill = \"lightblue\", color = \"black\"),\n        axis.line = element_blank())"},{"path":"maps.html","id":"調査地点の図","chapter":"11 地図の作り方","heading":"11.5 調査地点の図","text":"形上湾と大村湾の図を作ります。\n形上湾の方には、調査地点と結果ものせます。\nまずは形上湾と大村湾の範囲を決めます。\n範囲は Google Map で選びました。ここで、それぞれの湾のデータを kyushu からぬきます。アマモの被度データの simple features データを準備します。九州の図を先につくります。大村湾と形上湾の図を次に作りますが、先にラベルの tibble を準備します。\ntibble の long と lat のデータは試行錯誤で来ました。\nもっといい方法はあるはずです。では、大村湾と形上湾の地図をつくります。patchwork のパッケージをつかって、図を組み立てます。\n図は PDF に保存したら、magick を使って、PNGにも変換します。","code":"\nkatagami = rbind(rev(c(32.95809069048365, 129.7669185309373)),\n                 rev(c(32.89802000729197, 129.82832411747583))) |>\n  as_tibble(.name_repair = \\(x) c(\"long\", \"lat\")) |>\n  st_as_sf(coords = c(\"long\", \"lat\"), crs = st_crs(kyushu))\n\nomurabay = rbind(rev(c(33.103196388120104, 129.67183787501082)),\n                 rev(c(32.817013859622804, 130.03298144413574))) |> \n  as_tibble(.name_repair = \\(x) c(\"long\", \"lat\")) |>\n  st_as_sf(coords = c(\"long\", \"lat\"), crs = st_crs(kyushu))\nomurabay_area = kyushu |> filter(str_detect(N03_001, \"長崎\")) |> st_crop(st_bbox(omurabay)) \nkatagami_area = kyushu |> filter(str_detect(N03_001, \"長崎\")) |> st_crop(st_bbox(katagami)) \nzostera = zostera |>\n  st_as_sf(coords = c(\"long\", \"lat\"), crs = st_crs(kyushu)) |> \n  rename(coverage = matches(\"cover\")) |> \n  mutate(rank = cut(coverage, \n                    c(-Inf, 1, 10, 40, 70, Inf),\n                    labels = c(\"E\", \"D\", \"C\", \"B\", \"A\"))) |> \n  mutate(rank = factor(rank, \n                       levels = LETTERS[1:5],\n                       labels = LETTERS[1:5]))\n# The main plot of kyushu\npmain = ggplot(kyushu) + \n  geom_sf(aes(fill = fillme), color = NA) +\n  guides(fill = \"none\") +\n  coord_sf(expand = FALSE) +\n  scale_fill_viridis_d() +\n  theme(panel.grid = element_blank(),\n        panel.background = element_rect(fill = \"lightblue\", color =\"black\"),\n        panel.border  = element_rect(fill = NA, color =\"black\"),\n        plot.background =  element_rect(fill = NA, color =NA),\n        axis.title = element_blank(),\n        axis.line = element_blank())\n# Build plots for Omura Bay and Katagami Bay.\ntmp1 = omurabay_area |> st_transform(crs = st_crs(6677)) |> st_bbox()\ntmp2 = katagami_area |> st_transform(crs = st_crs(6677)) |> st_bbox()\n# tibble for labeling figures. The long and lat are by trial-and-error.\n# Need to find a better method.\nlabel1 = tibble(long = tmp1[3] -2500,\n                lat = tmp1[2] +1700,\n                label = \"Omura Bay, Nagasaki, Japan\") |> \n  st_as_sf(coords = c(\"long\", \"lat\"), crs = st_crs(6677), agr = \"constant\") |> \n  st_transform(crs = st_crs(omurabay_area))\n\nlabel2 = tibble(long = tmp2[1] +800,\n                lat = tmp2[4] -150,\n                label = \"Katagami Bay, Nagasaki, Japan\") |> \n  st_as_sf(coords = c(\"long\", \"lat\"), crs = st_crs(6677), agr = \"constant\") |> \n  st_transform(crs = st_crs(omurabay_area))\npomura = ggplot() +\n  geom_sf(fill = \"grey50\", data = omurabay_area, size = 0) +\n  geom_sf_text(aes(label = label), \n               data = label1,\n               color = \"white\",\n               family = \"notosans\", \n               fontface = \"bold\",\n               vjust = 1, hjust = 1,\n               size = 5)  + \n  coord_sf(expand = FALSE) +\n  annotation_north_arrow(style = north_arrow_minimal(text_family = \"notosans\", \n                                                     text_face = \"bold\",\n                                                     line_width = 2,\n                                                     text_size = 20),\n                         pad_y = unit(0.3, \"npc\")) + \n  theme(panel.background = element_rect(fill = \"lightblue\", color =\"black\"),\n        panel.border  = element_rect(fill = NA, color =\"black\"),\n        plot.background =  element_rect(fill = \"white\", color =NA),\n        axis.title = element_blank(),\n        axis.line = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank())\n\npkatagami = ggplot() +\n  geom_sf(fill = \"grey50\", data = katagami_area, size = 0) +\n  geom_sf(aes(fill = rank), data = zostera,\n          pch = 21, size = 3,\n          color = \"white\", stroke = 1) +\n  geom_sf_text(aes(label = label), \n               data = label2,\n               color = \"white\",\n               family = \"notosans\", \n               fontface = \"bold\",\n               vjust = 1.0, hjust = 0.0,\n               size = 5)  + \n  annotation_north_arrow(style = north_arrow_minimal(text_family = \"notosans\", \n                                                     text_face = \"bold\",\n                                                     line_width = 2,\n                                                     text_size = 20)) + \n  coord_sf(expand = FALSE, crs = st_crs(katagami_area)) +\n  scale_fill_viridis_d(end = 0.8) +\n  theme(panel.grid = element_blank(),\n        panel.background = element_rect(fill = \"lightblue\", color =\"black\"),\n        panel.border  = element_rect(fill = NA, color =\"black\"),\n        plot.background =  element_rect(fill = \"white\", color =NA),\n        axis.title = element_blank(),\n        axis.line = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank())\npout = pmain + (pomura / pkatagami)\npdfname = \"katagami-map-v1.pdf\"\npngname = str_replace(pdfname, \"pdf\", \"png\")\nggsave(pdfname, plot= pout, width = 300, height = 300, units = \"mm\")\nimage_read_pdf(pdfname, density = 600) |> image_write(pngname)\nknitr::include_graphics(str_c(\"./\", pngname))"},{"path":"maps.html","id":"sesssion-information","chapter":"11 地図の作り方","heading":"11.6 Sesssion information","text":"","code":"\nsessionInfo()\n#> R version 4.1.3 (2022-03-10)\n#> Platform: x86_64-pc-linux-gnu (64-bit)\n#> Running under: Debian GNU/Linux 11 (bullseye)\n#> \n#> Matrix products: default\n#> BLAS:   /usr/lib/x86_64-linux-gnu/atlas/libblas.so.3.10.3\n#> LAPACK: /usr/lib/x86_64-linux-gnu/atlas/liblapack.so.3.10.3\n#> \n#> locale:\n#>  [1] LC_CTYPE=en_US.utf8        LC_NUMERIC=C              \n#>  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.utf8     \n#>  [5] LC_MONETARY=ja_JP.UTF-8    LC_MESSAGES=en_US.utf8    \n#>  [7] LC_PAPER=ja_JP.UTF-8       LC_NAME=C                 \n#>  [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n#> [11] LC_MEASUREMENT=ja_JP.UTF-8 LC_IDENTIFICATION=C       \n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets \n#> [6] methods   base     \n#> \n#> other attached packages:\n#>  [1] sf_1.0-7         ggspatial_1.1.5  kableExtra_1.3.4\n#>  [4] patchwork_1.1.1  magick_2.7.3     ggpubr_0.4.0    \n#>  [7] showtext_0.9-5   showtextdb_3.0   sysfonts_0.8.5  \n#> [10] forcats_0.5.1    stringr_1.4.0    dplyr_1.0.8     \n#> [13] purrr_0.3.4      readr_2.1.2      tidyr_1.2.0     \n#> [16] tibble_3.1.6     ggplot2_3.3.5    tidyverse_1.3.1 \n#> \n#> loaded via a namespace (and not attached):\n#>  [1] fs_1.5.2           bit64_4.0.5       \n#>  [3] lubridate_1.8.0    webshot_0.5.2     \n#>  [5] httr_1.4.2         tools_4.1.3       \n#>  [7] backports_1.4.1    bslib_0.3.1       \n#>  [9] utf8_1.2.2         R6_2.5.1          \n#> [11] KernSmooth_2.23-20 DBI_1.1.2         \n#> [13] colorspace_2.0-3   withr_2.5.0       \n#> [15] tidyselect_1.1.2   downlit_0.4.0     \n#> [17] bit_4.0.4          curl_4.3.2        \n#> [19] compiler_4.1.3     textshaping_0.3.6 \n#> [21] cli_3.2.0          rvest_1.0.2       \n#> [23] xml2_1.3.3         bookdown_0.24     \n#> [25] sass_0.4.0         scales_1.1.1      \n#> [27] classInt_0.4-3     askpass_1.1       \n#> [29] proxy_0.4-26       systemfonts_1.0.4 \n#> [31] digest_0.6.29      rmarkdown_2.12    \n#> [33] svglite_2.1.0      pkgconfig_2.0.3   \n#> [35] htmltools_0.5.2    highr_0.9         \n#> [37] dbplyr_2.1.1       fastmap_1.1.0     \n#> [39] rlang_1.0.2        readxl_1.3.1      \n#> [41] rstudioapi_0.13    farver_2.1.0      \n#> [43] jquerylib_0.1.4    generics_0.1.2    \n#> [45] jsonlite_1.8.0     vroom_1.5.7       \n#> [47] car_3.0-12         magrittr_2.0.2    \n#> [49] s2_1.0.7           Rcpp_1.0.8        \n#> [51] munsell_0.5.0      fansi_1.0.2       \n#> [53] abind_1.4-5        lifecycle_1.0.1   \n#> [55] stringi_1.7.6      yaml_2.3.5        \n#> [57] carData_3.0-5      grid_4.1.3        \n#> [59] parallel_4.1.3     crayon_1.5.0      \n#> [61] haven_2.4.3        hms_1.1.1         \n#> [63] knitr_1.37         pillar_1.7.0      \n#> [65] ggsignif_0.6.3     codetools_0.2-18  \n#> [67] wk_0.6.0           reprex_2.0.1      \n#> [69] glue_1.6.2         evaluate_0.15     \n#> [71] pdftools_3.1.1     qpdf_1.1          \n#> [73] modelr_0.1.8       vctrs_0.3.8       \n#> [75] tzdb_0.2.0         cellranger_1.1.0  \n#> [77] gtable_0.3.0       assertthat_0.2.1  \n#> [79] cachem_1.0.6       xfun_0.30         \n#> [81] broom_0.7.12       e1071_1.7-9       \n#> [83] rstatix_0.7.0      ragg_1.2.2        \n#> [85] class_7.3-20       viridisLite_0.4.0 \n#> [87] memoise_2.0.1      units_0.8-0       \n#> [89] ellipsis_0.3.2"}]
