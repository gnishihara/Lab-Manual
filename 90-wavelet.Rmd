# Wavlet 解析 {#wavelet}

## 必要なパッケージ

```{r}
library(tidyverse)
library(lubridate)
library(gnnlab)
library(furrr)
library(biwavelet)
```


```{r, echo = FALSE}
fn1 = function(t, scale = 1, k = 6) {
  # t: time (seconds)
  # scale: wavelet scale
  
  eta = (t - mean(t)) / scale
  z1 = pi^(-0.25) * exp(-0.5 * eta^2)
  z2 = exp(as.complex(1i) * eta * k)
  z1 * z2 |> Re()
}
```

```{r, echo = FALSE}
p1 = expand_grid(t = seq(-4,4, length = 501),
            k = c(3,6,12)) |> 
  mutate(y = fn1(t, 1, k)) |> 
  mutate(k = factor(k)) |> 
  ggplot() + 
  geom_line(aes(t,y, color = k), show.legend = FALSE) +
  scale_x_continuous(parse(text = "eta == t/s")) +
  scale_y_continuous(parse(text = "Re(psi)"), limits = c(-1,1))+
  scale_color_viridis_d(end = 0.8) +
  ggtitle("Morlet wavelet for k = {3, 6, 12} when s = 1")+
  facet_grid(rows = vars(k), labeller = as_labeller(\(x) str_c("k = ", x)))
p2 = expand_grid(t = seq(-6,6, length = 501),
            s = c(0.5, 1, 2)) |> 
  mutate(y = fn1(t, s, k = 12)) |> 
  mutate(s = factor(s)) |> 
  ggplot() + 
  geom_line(aes(t,y, color = s), show.legend = FALSE) +
  scale_x_continuous(parse(text = "eta == t/s")) +
  scale_y_continuous(parse(text = "Re(psi)"), limits = c(-1,1))+
  scale_color_viridis_d(end = 0.8) +
  ggtitle("Morlet wavelet for s = {0.5, 1, 1} when k = 12")+
  facet_grid(rows = vars(s), labeller = as_labeller(\(x) str_c("s = ", x)))
p1 + p2
```


ウェーブレット変換は時系列データから時間的変化の特徴と周波数成分を調べるために使います。
**細かい説明はいつか追加します。**
本章を完成するまでは、次の論文を参考にしてください。


Cazelles B., Chavez M., Berteaux D., Menard F., Vik J.O., Jenouvrier S., and Stenseth N.C. 2008. Wavelet analysis of ecological time series. Oecologia 156: 287-304.

Grinsted A., Moore J.C., and Jevrejeva S. 2004. Application of the cross wavelet transform and wavelet coherence to geophysical time series. Nonlinear Processes in Geophysics 11: 561-566.

Torrence C. and Compo G.P. 1998. A practical guide to wavelet analysis. Bulletin of the American Meteorological Society 79: 61-78.

Torrence C and Webster P.J. 1998. The annual cycle of persistence in the El Nino/Southern Oscillation. Quarterly Journal of the Royal Meteorological Society 124: 1985-2004.



$$
\psi(\eta) = \frac{1}{\sqrt[\leftroot{-2}\uproot{3}4]{\pi}}\;e^{\eta^2/2}\;e^{ik\eta}
$$
パラメータ $\eta$ は時間 $(t)$ と ウェーブレットスケール $(s)$ の比率です。
ウェーブレットスケールを下げると時間軸方向の分解能が上がります。
ウェーブレットパラメータ $k$ はモレー・ウェーブレットの振動数（山の数）を制御しています。
よって、$k$ を上げると、周波数分解能が上がります。

では、`biwavelet` パッケージで次の波形を解析してみましょう。

$$
\begin{aligned}
y &= \sin(2 \pi \omega) \\
\omega &= 50 t^2 + 10
\end{aligned}
$$
この波形の角周波数 $(\omega)$ は徐々に高くなります。

```{r}
ft = function(t) {
  f = 50 * t^2 + 10
  sin(2 * pi * f )
}
z = tibble(t = seq(0, 1, length = 3000)) |> mutate(y = ft(t))
```

```{r}
ggplot(z) + geom_line(aes(x = t, y = y)) +
  scale_x_continuous("t") +
  scale_y_continuous("y")
```

`biwavelet` の `wt()` 関数でウェーブレット解析をします。
`wt()` に渡すデータは行列としてわたしましょう。
行列の1列目には時間情報、2列名には解析したい値です。

```{r}
wtout = wt(as.matrix(z), mother = "morlet")
```


```{r, echo = FALSE}
CAP = "
これはウェーブレット解析のパワーを示す図です。
パワーの低いところは青色、パワーの高いところは赤色です。
黒線は統計学的に優位な領域を示しています。
赤色の部分が時間につれ、周期が上昇しています。
ウェーブレット解析で角周波数の傾向を十分抽出できたとおもいます。
白くなっているところは cone of influence (COI) です。
COIは解析アルゴリズムの精度が落ちているところを示しているので、
示されたパワーは両端から $e^{-2}$ に従って下がります。
"
```

```{r, fig.cap = CAP}
plot(wtout)
```

## 解析に使う関数

Wavelet を求めるための関数です。
コアは `biwavelet` パッケージの `wt()` 関数です。
マザー・ウェーブレット (mother wavelet) はモレーウェーブレット (Morlet wavelet) に固定しています。
`wt()` には DoG (derivative of gaussian) と Paul ウェーブレットも使えます。

```{r}
calculate_wavelet = function(df, obs, tau = NULL, fs = 6, dB = TRUE) {
  pad_cname = function(x, w) {
    x = str_pad(x, width = w, pad = "0")
    str_c("V",x)
  }
  
  # 
  N = nrow(df)
  datetime  = df %>% pull({{tau}})
  if(!near(day(datetime[N]), day(datetime[N-1]))) {
    df = df |> slice(1:(N-1))
    datetime  = df %>% pull({{tau}})
  }
  
  hours = as.double(datetime - datetime[1], units = "hours")
  observation = df %>% pull({{obs}})
  wtout = wt(cbind(hours, observation), mother = "morlet")
  
  xval = wtout$t        # Vector of times
  yval = wtout$period   # Vector of periods
  sigma2 = wtout$sigma2 # Vector of variance of time series
  coi = wtout$coi       # Vector of cone of influence
  signif = wtout$signif # Matrix of significance levels
  # Matrix of bias-corrected power
  if(dB) {
    Z = 10 * log10(abs(wtout$power.corr / sigma2))
  } else {
    Z = log2(abs(wtout$power.corr / sigma2))
  }
  
  zlim = range(c(-1, 1) * max(Z))
  Z[Z < zlim[1]] = zlim[1]
  n = dim(Z)
  tmp1 = signif |> as_tibble(.name_repair = ~pad_cname(1:n[2], nchar(n[2]) + 1)) |> 
    mutate(period = yval) |> pivot_longer(starts_with("V"), values_to = "signif") |> 
    group_nest(name, .key = "signif") |> 
    arrange(name) |> 
    mutate(datetime)
  tmp2 = Z |> as_tibble(.name_repair =  ~pad_cname(1:n[2], nchar(n[2]) + 1)) |> 
    mutate(period = yval) |> pivot_longer(starts_with("V"), values_to = "power") |> 
    group_nest(name, .key = "power") |> 
    arrange(name) |> 
    mutate(datetime)
  
  full_join(tmp1, tmp2, by = c("name", "datetime")) |> mutate(coi, hours = xval) |> 
    select(name, datetime, hours, coi, power, signif)
}
```

関数を適応した場合、エラーがでたらスクリプトはエラーが発生したところで止まります。
関数を `possibly()`のラッパーにとおせば、エラーがでたとき、`NULL` を返すようにする。
これで、スクリプトは止まりません。

```{r}
calc_wt = possibly(calculate_wavelet, NULL)
```

## 補足関数

```{r}
se = function(x, na.rm=FALSE) {
  # 標準誤差
  N = sum(!is.na(x))
  sd(x, na.rm) / sqrt(N - 1)
} 

date_gnn = function(x) {
  # ggplot の時間軸のlabel 関数
  tmp0 = year(x)
  tmp1 = month.abb[month(x)]
  tmp2 = day(x)
  tmp0[duplicated(tmp0)] = ""
  
  str_c(tmp2, "\n" ,tmp1, "\n",tmp0) 
}

date_gnn2 = function(x) {
  # ggplot の時間軸のlabel 関数
  tmp0 = year(x)
  tmp1 = month.abb[month(x)]
  tmp2 = day(x)
  tmp0[duplicated(tmp0)] = ""
  
  str_c(tmp1, "\n",tmp0) 
}
# ggplot 用の関数
log2reverse = function(x) {-log2(x)}
log2reverseinv = function(x) {2^(-x)}

contiguous = function(df, tau, deltaT = 10) {
  # 隣接データを確認するための関数
  x = df |> pull({{tau}})
  xout = as.double(x - lag(x), units = "mins")
  df |> mutate(contig = xout) |> 
    mutate(group = as.numeric(!near(contig, deltaT))) |> 
    mutate(group = replace_na(group, 0)) |> 
    mutate(group = cumsum(group)) |> 
    mutate(group = factor(group))
}

```


## データの前処理

データの読み込みは並列で行うので、`map()` じゃなくて `future_map()` を使います。
`read_onset()` は 研究室の `gnnlab` パッケージの関数です。
```{r}
labdatafolder = "~/Lab_Data/kawatea/Oxygen"
dset = tibble(fnames = dir(labdatafolder, pattern = "DO.*arikawa.*csv", full = TRUE)) |> 
  filter(str_detect(fnames, "calib", negate = TRUE))
dset = dset |>
  mutate(data = future_map(fnames, read_onset)) 
dset
```

次はファイル名の処理をします。

```{r}
dset = dset |> filter(str_detect(fnames, "edge|sand", negate = T)) |> 
  mutate(fnames = basename(fnames)) |> 
  mutate(fnames = str_remove(fnames, ".csv")) |> 
  separate(fnames, into = c("type", "id", "location", "position", "surveydate"))
dset = dset |> unnest(data)
```

観測期間のデータを読み込んで、観測インターバルを設定します。

```{r}
surveyperiod = read_csv("~/Lab_Data/kawatea/period_info_220422.csv")
surveyperiod
```

```{r}
surveyperiod = surveyperiod |> mutate(interval = interval(start = start_date, end = end_date))
surveyperiod
```

観測インストールをつかて、データをフィルタにかけます。
インストール以外のデータはここで外します。

```{r}
ints = surveyperiod |> pull(interval)
dset = dset |> filter(datetime %within% as.list(ints))
```

さらに、データの数を求めてフィルタにかけます。
10分間隔で測定しているので、一日あたりに 144 のデータがあります。
条件に合わないデータは外します。

```{r}
dset = dset |> 
  mutate(date = as_date(datetime)) |> 
  group_by(date, location, position) |> 
  filter(near(n(), 144))
dset
```

Wavelet 解析を実施するときは、データが隣接しているかを確認しましょう。
隣接しているデータをグループ化してから解析をします。
ここでは、`contiguous()` 関数に `datetime` をわたして、必要な情報を加えます。

```{r}
dset = dset |> ungroup() |> 
  mutate(datetime = floor_date(datetime, "10 mins")) |> 
  group_nest(location, position, id) |> 
  mutate(data = map(data, contiguous, tau = datetime)) |> 
  unnest(data)
```

グループ化が完了したら、wavelet 解析をします。

ウェーブレット解析はとても重いので、研究室のサーバなら並列処理で解析します。

```{r}
number_of_cpu_cores = parallel::detectCores()
plan(multisession, workers = 8)
```

では、`future_map()` を使って並列処理で解散をします。


```{r}
wtout = dset |> 
  arrange(datetime) |> 
  group_nest(group, location, id, position) |> 
  mutate(wtout = future_map(data, \(df) {
    df |> calc_wt(temperature, datetime)
  }))
```

普通に逐次処理のときは、`map()` で実行しましょう。

```{r, eval = FALSE}
wtout = dset |> 
  arrange(datetime) |> 
  group_nest(group, location, id, position) |> 
  mutate(wtout = map(data, \(df) {
    df |> calc_wt(temperature, datetime)
  }))
```

## Wavelet の図

ウェーブレットの図も関数を設計して、作ります。
`wavelet_plot()` は作図用の関数です。
これは、`map()` を通して作図します。


```{r}
wavelet_plot = function(wtout) {
  ylabel = "Period (hrs)"
  xlabel = "Date"
  x = select(wtout, datetime, power) |> unnest(power)
  s = select(wtout, datetime, signif) |> unnest(signif)
  rng = x |> pull(period) |> range()
  ggplot() +
    geom_tile(aes(x = datetime,
                  y = period, 
                  fill = power),
              data = x) +
    geom_contour(aes(x = datetime,
                     y = period,
                     z = signif),
                 data = s,
                 color = "black",
                 size = 1, 
                 breaks = 1) +
    geom_line(aes(x = datetime,
                  y = coi),
              data = wtout,
              color = "white") +
    geom_hline(yintercept = c(1, 6, 12, 24),
               linetype = "dashed", color = "grey50") +
    scale_y_continuous(ylabel,
                       trans = scales::trans_new("log2reverse",
                                                 log2reverse,
                                                 log2reverseinv,
                                                 domain = c(0, Inf)),
                       limits = rev(rng),
                       breaks = 2^log2(c(1, 4, 6, 12, 16, 24, 64)),
                       expand = expansion())  +
    scale_x_datetime(xlabel, 
                     date_breaks = "2 days",
                     labels = date_gnn)  +
    guides(fill = "none") + 
    scale_fill_viridis_c()
}
```

ここで作図をします。

```{r}
wtplots = wtout |> 
  select(group, wtout, location, position, id) |> 
  mutate(ggplot = map(wtout, wavelet_plot))
```

そのままコードを実行すると RStudio には図のアウトプットはないです。
図を見るためには、ファイルに保存したほうがいい。

```{r}
plots = wtplots$ggplot
length(plots)
```

合計 `r length(plots)` のファイルを書き出すことになりそうなので、
フォルダを作って保存します。

```{r}
dname = "_wavelet_plots"
if(!dir.exists(dname)) {
  dir.create(dname)
}
```

ここでファイル名と保存するための関数をつくります。
ファイルはPDFとして保存するが、同時にPDFをPNGに変換します。
PDFを変換すると、確実に選んだフォントが埋め込まれるので、
PNGファイルのフォントも綺麗です。
ファイルの変換は `magick` パッケージをつかいます。

```{r, include = FALSE}
library(showtext)
library(ggpubr)
library(magick)

# やっぱり Noto Sans がきれい。
if(!any(str_detect(font_families(), "notosans"))) {
  font_add_google("Noto Sans JP","notosans")
}
# 図のフォントがからだったので、ここで修正した
# １）theme_set() をつかってデフォルトのフォントをかえる
# ２）ggplot() の theme() からとんとの指定をはずす。

theme_pubr(base_size = 10, base_family = "notosans") |> theme_set()
showtext_auto()
```

```{r, eval = FALSE}
save_wavelets = function(l,p,d,g,i,plot) {
  pdfname = str_c(dname, "/", l, "_", p, "_", d, "_", g,"_", i, ".pdf")
  pngname = str_replace(pdfname, "pdf", "png")
  ggsave(pdfname, plot = plot, width = 300, height = 200, units = "mm")
  magick::image_read_pdf(pdfname, density = 300) |> 
    magick::image_trim() |> magick::image_write(pngname)
}

wtplots |> 
  mutate(period = map_chr(wtout, \(x) {
    z = x |> pull(datetime) |> as_date() |> range()
    z = str_remove_all(z, "-")
    str_c(z[1], "_", z[2])
  })) |> 
  mutate(out = pmap(list(location, position, period, group, id, ggplot), save_wavelets))
```

```{r, fig.cap = CAP, echo=FALSE}
CAP = "
有川湾鯨見山地先におけるガラモ場の水温に対するウェーブレット解析。
観測期間は2018年5月17日から6月17日でしました。
水温は海底で記録しました。
パワーの強い所は明るい色（黄色）で示していて、パワーの弱い所は暗い色（青色）で示しています。
白線はCOIを示しています。
5月17日から25日、5月31日から6月2日、6月9日,　6月12日から17日の期間には強い24時間の周期があります。
ところどころ強い12時間の周期もあります。12時間の周期は潮汐と関係していると考えますね。
12時間より短い周期でも比較的に強いパワーが示されています。
"
fn = rprojroot::find_rstudio_root_file("_wavelet_plots") |> 
  dir(full = TRUE, pattern = "png")
str_subset(fn, "arikawagaramo_0m_7_03") |> magick::image_read()
```

## SNR の求め方

日周期 (diurnal) と高周期 (high) の比率を求めて、一日内の水温の安定性を調べてみましょう。
ウェーブレットで検出した周期情報を High (0 ~ 7 hr), Tidal (7 = 13 hrs), Diurnal (13 ~ 25 hrs), Low (> 25 hrs) に分類します。
分離したら、一日あたりの平均パワー (power) と平均値の 95% 信頼区間を求めます。
**この解析はまだ未熟ですが、もっといい方法に気づいたら紹介します。**

```{r}
calculate_mean_power = function(wt) {
    wt |> 
      select(datetime, power) |> 
      unnest(power) |> 
      mutate(datetime = floor_date(datetime, "day")) |> 
      ungroup() |> 
      mutate(ftype = cut(period, breaks = c(-Inf, 7, 13, 25, Inf),
                         labels = c("High", "Tidal", "Diurnal", "Low"))) |> 
      group_by(datetime, ftype) |> 
      summarise(across(power, list(mean =mean, se = se)), .groups = "drop") |> 
      mutate(lower = power_mean - 1.96*power_se,
             upper = power_mean + 1.96*power_se)
}

wtout = wtout |> mutate(wtout2 = map(wtout, calculate_mean_power))
```

次に、`wtout2` に隠れている `tibble` を縦長から横長に変換します。
つづいて、SNRを求めますが、power はログスケールで求めたので、
SNR比は `snr = Diurnal - High` です。

```{r}
calculate_snr = function(wt)  {
  wt |> ungroup() |> 
    select(datetime, ftype, power_mean) |> 
    arrange(datetime) |> 
    pivot_wider(names_from =ftype, values_from = power_mean) |> 
    mutate(snr = Diurnal - High)
}
wtout = wtout |> 
  mutate(wtout3 = map(wtout2, calculate_snr))
```

SNR比を `date`, `location`, `position` ごとにまとめます。

```{r}
snrdata = 
  wtout |> select(wtout3, location, position) |> unnest(wtout3) |> 
  mutate(date = floor_date(datetime, "month")) |> 
  group_by(date, location, position) |> 
  summarise(snr = mean(snr), .groups = "drop")
```

SNR比に2020年あたりに怪しい値はあるが、`position` ごとに傾向有るのかな？

```{r, fig.cap = "SNRは日周期と高周期の比率です。"}
ylabel = "Period (h)"
xlabel = "Date"
ggplot(snrdata) + 
  geom_point(aes(x = date, y = snr, color = position)) +
  facet_wrap(vars(location))
```

SNRの傾向を解析して、図に追加します。
`location`と`position` 間の比較に興味がないので、次のようなモデルを当てはめます。

```{r}
fit_model = function(df) {
  lm(snr ~ date, data = df)
}
```

**解析の結果**

一般線形モデルのF検定の結果、すべてのモデルのP値は 0.05 より大きいですね。

```{r}
snrdata |> 
  mutate(date = as_date(date)) |> 
  group_nest(location, position) |> 
  mutate(model = map(data, fit_model)) |> 
  mutate(summary = map(model, broom::glance)) |> 
  unnest(summary)
```

モデル係数の結果は `broom` パッケージの `tidy()` 関数で確認できます。

```{r}
snrdata |> 
  mutate(date = as_date(date)) |> 
  group_nest(location, position) |> 
  mutate(model = map(data, fit_model)) |> 
  mutate(summary = map(model, broom::tidy)) |> 
  unnest(summary) 
```

`date` はモデル傾きのパラメータです。
`arikawaamamo` `0m` 以外の `date` 係数は正の値をとっていますが、
Wald's 検定の結果、すべての P値は 0.05 より大きいです。

帰無仮説検定論によると、0 との統計学的な有意差がなかったので、
係数が 0 ではないという仮説は棄却できない。

それにしても、とりあえずデータを図に追加してみよう。

```{r}
return_fit = function(data, model) {
  nd = data |> expand(date)
  bind_cols(nd, predict(model, newdata = nd, se.fit = TRUE) |> as_tibble()) |> 
    mutate(lower = fit - 1.96*se.fit,
           upper = fit + 1.96*se.fit)
}

snrdata = snrdata |> 
  mutate(date = as_date(date)) |> 
  group_nest(location, position) |> 
  mutate(model = map(data, fit_model)) |>
  mutate(ndata = map2(data,model, return_fit))
```


```{r, fig.cap = "SNRの傾向に線形モデルを当てはめたが、線形モデルのF検定に統計学的に有意な結果はありませんでした。"}
ggplot() + 
  geom_line(aes(x = date, y = fit, color = position), data = snrdata |> unnest(ndata)) + 
  geom_point(aes(x = date, y = snr, color = position), data = snrdata |> unnest(data)) + 
  facet_wrap(vars(location))
```






















